{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to DevSolutions Hub","text":""},{"location":"#about-me","title":"About Me","text":"<p>Hello! I'm Yan Naing Htwe, a passionate Software Engineer with over 4 years of experience building user-friendly and scalable web and mobile applications. Based in Tokyo, I specialize in creating complete end-to-end solutions that solve real-world problems.</p>"},{"location":"#my-expertise","title":"My Expertise","text":"<ul> <li>Frontend Development: React, Next.js, React Native, Vue.js</li> <li>Backend Technologies: Node.js, Laravel, Spring Boot</li> <li>Mobile Development: React Native, cross-platform solutions</li> <li>Full-Stack Development: API design, database management, system integration</li> </ul>"},{"location":"#why-i-created-this-resource-hub","title":"Why I Created This Resource Hub","text":"<p>Throughout my journey as a software engineer, I've encountered countless challenges - from complex Docker configurations to intricate Git workflows, from debugging production issues to managing enterprise-level projects. Each obstacle taught me valuable lessons that I wish I had documented from the beginning.</p>"},{"location":"#the-problem-i-noticed","title":"The Problem I Noticed","text":"<p>In the fast-paced world of software development, developers often: - Reinvent the wheel by searching for the same solutions repeatedly - Lose time looking through scattered documentation and Stack Overflow posts - Struggle with consistency across different projects and teams - Face challenges when transitioning between different tools and workflows</p>"},{"location":"#my-solution","title":"My Solution","text":"<p>This documentation hub serves as a centralized knowledge base containing:</p> <p>\u2728 Practical guides that I've tested in real production environments \u2728 Step-by-step tutorials with clear explanations and context \u2728 Industry best practices learned from working in enterprise settings \u2728 Quick reference materials for daily development tasks \u2728 Troubleshooting guides for common issues developers face  </p>"},{"location":"#what-youll-find-here","title":"What You'll Find Here","text":""},{"location":"#docker-containerization","title":"\ud83d\udc33 Docker &amp; Containerization","text":"<p>Comprehensive guides covering everything from basic Docker commands to complex multi-container applications, including database setup and networking configurations.</p>"},{"location":"#git-version-control","title":"\ud83d\udd27 Git &amp; Version Control","text":"<p>Industry-standard Git workflows, branching strategies, collaborative development practices, and emergency procedures for production environments.</p>"},{"location":"#development-workflows","title":"\ud83d\ude80 Development Workflows","text":"<p>Best practices for project setup, deployment strategies, CI/CD pipelines, and maintaining code quality in team environments.</p>"},{"location":"#tools-technologies","title":"\ud83d\udee0\ufe0f Tools &amp; Technologies","text":"<p>Detailed documentation for various development tools, frameworks, and technologies commonly used in modern software development.</p>"},{"location":"#my-philosophy","title":"My Philosophy","text":"<p>\"Documentation is not just about recording what you did - it's about enabling others to build upon your work and avoid the same pitfalls.\"</p> <p>I believe in: - Learning by doing - Every guide here comes from hands-on experience - Sharing knowledge - The developer community thrives when we help each other - Continuous improvement - Documentation should evolve with our understanding - Practical solutions - Focus on real-world scenarios over theoretical concepts</p>"},{"location":"#how-this-site-helps-you","title":"How This Site Helps You","text":""},{"location":"#save-time","title":"\ud83c\udfaf Save Time","text":"<p>Instead of searching through multiple resources, find consolidated, tested solutions in one place.</p>"},{"location":"#improve-skills","title":"\ud83d\udcc8 Improve Skills","text":"<p>Learn not just the \"how\" but also the \"why\" behind different approaches and best practices.</p>"},{"location":"#work-better-in-teams","title":"\ud83e\udd1d Work Better in Teams","text":"<p>Understand industry-standard workflows and practices that make collaboration smoother.</p>"},{"location":"#handle-emergencies","title":"\ud83d\udea8 Handle Emergencies","text":"<p>Quick reference guides for when production issues need immediate attention.</p>"},{"location":"#my-background","title":"My Background","text":"<p>As a software engineer who has worked on diverse projects - from mobile applications to full-stack web solutions - I've experienced the challenges of:</p> <ul> <li>Leading frontend and mobile projects while ensuring pixel-perfect UIs</li> <li>Collaborating with cross-functional teams including designers and backend developers  </li> <li>Managing complex deployments and maintaining system reliability</li> <li>Working in enterprise environments with strict quality and security requirements</li> </ul> <p>This experience has given me insights into what developers really need in their daily work, which I'm excited to share through this platform.</p>"},{"location":"#connect-with-me","title":"Connect With Me","text":"<p>Want to learn more about my work or discuss these topics further?</p> <ul> <li>Portfolio: yannainghtwe-portfolio.vercel.app</li> <li>Location: Tokyo, Japan</li> <li>Expertise: Full-Stack Development, Mobile Applications, System Architecture</li> </ul>"},{"location":"#how-to-use-this-documentation","title":"How to Use This Documentation","text":"<p>Each section in this hub is designed to be: - Self-contained - You can jump to any topic without reading others - Practical - Real commands and examples you can use immediately - Contextual - Explanations of when and why to use different approaches - Progressive - From basic concepts to advanced techniques</p> <p>Whether you're a junior developer looking to learn industry practices or a senior engineer seeking quick references, this documentation hub is built to support your daily development needs.</p> <p>Happy coding! \ud83d\ude80</p> <p>Last updated: September 2025</p>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/","title":"Spring Boot CI/CD Setup Guide","text":""},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#automated-deployment-to-ec2-ubuntu-with-github-actions-caddy","title":"Automated Deployment to EC2 Ubuntu with GitHub Actions &amp; Caddy","text":"<p>This guide walks you through setting up a complete CI/CD pipeline for Spring Boot applications using GitHub Actions, EC2 Ubuntu, and Caddy reverse proxy with automatic HTTPS.</p>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Prerequisites</li> <li>EC2 Server Setup</li> <li>Secure Application Properties</li> <li>GitHub Actions Workflow</li> <li>Caddy Reverse Proxy Setup</li> <li>GitHub Secrets Configuration</li> <li>Testing &amp; Verification</li> <li>Troubleshooting</li> </ol>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#prerequisites","title":"Prerequisites","text":""},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#local-machine","title":"Local Machine","text":"<ul> <li>Git installed</li> <li>SSH key pair (<code>.pem</code> file) for EC2 access</li> <li>Maven project with Spring Boot</li> </ul>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#aws-account","title":"AWS Account","text":"<ul> <li>EC2 Ubuntu instance (t2.micro or higher)</li> <li>Security groups configured (see below)</li> <li>RDS PostgreSQL database (optional)</li> </ul>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#ec2-server-setup","title":"EC2 Server Setup","text":""},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#1-launch-ec2-instance","title":"1. Launch EC2 Instance","text":"<ul> <li>AMI: Ubuntu 24.04 LTS</li> <li>Instance Type: t2.micro (or higher)</li> <li>Key Pair: Create or use existing <code>.pem</code> file</li> </ul>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#2-configure-security-groups","title":"2. Configure Security Groups","text":""},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#ec2-security-group-inbound-rules","title":"EC2 Security Group Inbound Rules:","text":"Type Protocol Port Source Description SSH TCP 22 Your IP or GitHub Actions IPs SSH access HTTP TCP 80 0.0.0.0/0 HTTP traffic HTTPS TCP 443 0.0.0.0/0 HTTPS traffic Custom TCP TCP 8080 0.0.0.0/0 Spring Boot (optional if using Caddy)"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#rds-security-group-inbound-rules-if-using-rds","title":"RDS Security Group Inbound Rules (if using RDS):","text":"Type Protocol Port Source Description PostgreSQL TCP 5432 EC2 Security Group ID Database access from EC2"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#3-install-java-17-on-ec2","title":"3. Install Java 17 on EC2","text":"<pre><code>ssh -i your-key.pem ubuntu@YOUR_EC2_IP\n\nsudo apt update\nsudo apt install -y openjdk-17-jre-headless\njava -version\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#4-create-application-directory","title":"4. Create Application Directory","text":"<pre><code>sudo mkdir -p /opt/YOUR_APP_NAME\nsudo chown ubuntu:ubuntu /opt/YOUR_APP_NAME\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#5-install-caddy-optional-for-https","title":"5. Install Caddy (Optional - for HTTPS)","text":"<pre><code>sudo apt install -y debian-keyring debian-archive-keyring apt-transport-https curl\ncurl -1sLf 'https://dl.cloudsmith.io/public/caddy/stable/gpg.key' | sudo gpg --dearmor -o /usr/share/keyrings/caddy-stable-archive-keyring.gpg\ncurl -1sLf 'https://dl.cloudsmith.io/public/caddy/stable/debian.deb.txt' | sudo tee /etc/apt/sources.list.d/caddy-stable.list\nsudo apt update\nsudo apt install caddy\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#secure-application-properties","title":"Secure Application Properties","text":""},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#1-update-applicationproperties","title":"1. Update <code>application.properties</code>","text":"<p>Replace hardcoded credentials with environment variables:</p> <pre><code># Database\nspring.datasource.url=${DB_URL:jdbc:postgresql://your-db-host:5432/your_db}\nspring.datasource.username=${DB_USERNAME:postgres}\nspring.datasource.password=${DB_PASSWORD}\n\n# Email (if using)\nspring.mail.username=${MAIL_USERNAME:your-email@gmail.com}\nspring.mail.password=${MAIL_PASSWORD}\n\n# AWS Credentials (if using)\ncloud.aws.credentials.access-key=${AWS_ACCESS_KEY}\ncloud.aws.credentials.secret-key=${AWS_SECRET_KEY}\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#2-create-envexample","title":"2. Create <code>.env.example</code>","text":"<pre><code># Database credentials\nDB_PASSWORD=your_database_password\n\n# Email credentials\nMAIL_PASSWORD=your_gmail_app_password\n\n# AWS credentials\nAWS_ACCESS_KEY=your_aws_access_key\nAWS_SECRET_KEY=your_aws_secret_key\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#3-update-gitignore","title":"3. Update <code>.gitignore</code>","text":"<pre><code># Environment variables\n.env\n\n# Maven build output\ntarget/\n\n# IDE files\n.idea/\n*.iml\n.vscode/\n.DS_Store\n\n# Log files\n*.log\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#github-actions-workflow","title":"GitHub Actions Workflow","text":""},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#create-githubworkflowsdeploy-devyml","title":"Create <code>.github/workflows/deploy-dev.yml</code>","text":"<pre><code>name: Deploy to Dev\n\non:\n  push:\n    branches:\n      - dev\n\njobs:\n  build-and-deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout code\n        uses: actions/checkout@v4\n\n      - name: Set up JDK 17\n        uses: actions/setup-java@v4\n        with:\n          java-version: '17'\n          distribution: 'temurin'\n          cache: 'maven'\n\n      - name: Build with Maven\n        run: mvn clean package -DskipTests -T 1C\n\n      - name: Deploy to EC2 via SSH\n        uses: appleboy/scp-action@v0.1.7\n        with:\n          host: ${{ secrets.EC2_HOST }}\n          username: ${{ secrets.EC2_USERNAME }}\n          key: ${{ secrets.EC2_SSH_KEY }}\n          port: ${{ secrets.EC2_PORT }}\n          source: \"target/YOUR_APP_NAME-0.0.1-SNAPSHOT.jar\"\n          target: \"~/YOUR_APP_NAME/\"\n\n      - name: Restart application on EC2\n        uses: appleboy/ssh-action@v1.0.3\n        env:\n          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}\n          MAIL_PASSWORD: ${{ secrets.MAIL_PASSWORD }}\n          AWS_ACCESS_KEY: ${{ secrets.AWS_ACCESS_KEY }}\n          AWS_SECRET_KEY: ${{ secrets.AWS_SECRET_KEY }}\n        with:\n          host: ${{ secrets.EC2_HOST }}\n          username: ${{ secrets.EC2_USERNAME }}\n          key: ${{ secrets.EC2_SSH_KEY }}\n          port: ${{ secrets.EC2_PORT }}\n          envs: DB_PASSWORD,MAIL_PASSWORD,AWS_ACCESS_KEY,AWS_SECRET_KEY\n          script: |\n            # Copy new JAR to app directory\n            sudo mkdir -p /opt/YOUR_APP_NAME\n            sudo cp ~/YOUR_APP_NAME/target/YOUR_APP_NAME-0.0.1-SNAPSHOT.jar /opt/YOUR_APP_NAME/app.jar\n\n            # Update systemd service with environment variables\n            sudo tee /etc/systemd/system/YOUR_APP_NAME.service &gt; /dev/null &lt;&lt;EOF\n            [Unit]\n            Description=Your Spring Boot Application\n            After=network.target\n\n            [Service]\n            Type=simple\n            User=ubuntu\n            WorkingDirectory=/opt/YOUR_APP_NAME\n            ExecStart=/usr/bin/java -jar /opt/YOUR_APP_NAME/app.jar\n            Restart=on-failure\n            RestartSec=10\n            StandardOutput=append:/opt/YOUR_APP_NAME/app.log\n            StandardError=append:/opt/YOUR_APP_NAME/app.log\n            Environment=\"DB_PASSWORD=$DB_PASSWORD\"\n            Environment=\"MAIL_PASSWORD=$MAIL_PASSWORD\"\n            Environment=\"AWS_ACCESS_KEY=$AWS_ACCESS_KEY\"\n            Environment=\"AWS_SECRET_KEY=$AWS_SECRET_KEY\"\n\n            [Install]\n            WantedBy=multi-user.target\n            EOF\n\n            # Reload systemd and restart service\n            sudo systemctl daemon-reload\n            sudo systemctl enable YOUR_APP_NAME\n            sudo systemctl restart YOUR_APP_NAME\n\n            # Wait and verify\n            sleep 5\n            if sudo systemctl is-active --quiet YOUR_APP_NAME; then\n              echo \"Application started successfully\"\n            else\n              echo \"Failed to start application\"\n              sudo journalctl -u YOUR_APP_NAME -n 50\n              exit 1\n            fi\n\n            # Reload Caddy (if running)\n            if sudo systemctl is-active --quiet caddy; then\n              echo \"Reloading Caddy...\"\n              sudo systemctl reload caddy\n            fi\n\n      - name: Upload artifact\n        uses: actions/upload-artifact@v4\n        with:\n          name: YOUR_APP_NAME-jar\n          path: target/YOUR_APP_NAME-0.0.1-SNAPSHOT.jar\n</code></pre> <p>Important: Replace <code>YOUR_APP_NAME</code> with your actual application name throughout the file.</p>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#caddy-reverse-proxy-setup","title":"Caddy Reverse Proxy Setup","text":""},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#1-create-caddyfile","title":"1. Create Caddyfile","text":"<pre><code>ssh -i your-key.pem ubuntu@YOUR_EC2_IP\nsudo nano /etc/caddy/Caddyfile\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#2-add-configuration","title":"2. Add Configuration","text":"<pre><code>YOUR_EC2_IP.nip.io {\n    # Let's Encrypt will automatically provision SSL certificate\n\n    reverse_proxy localhost:8080 {\n        header_up Host {http.request.host}\n        header_up X-Real-IP {remote_host}\n        header_up X-Forwarded-For {remote_host}\n        header_up X-Forwarded-Proto {scheme}\n    }\n\n    log {\n        output file /var/log/caddy/access.log\n    }\n}\n</code></pre> <p>Replace <code>YOUR_EC2_IP</code> with your actual EC2 public IP address.</p>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#3-reload-caddy","title":"3. Reload Caddy","text":"<pre><code>sudo systemctl reload caddy\nsudo systemctl status caddy\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#github-secrets-configuration","title":"GitHub Secrets Configuration","text":"<p>Go to your GitHub repository: Settings \u2192 Secrets and variables \u2192 Actions \u2192 New repository secret</p>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#required-secrets","title":"Required Secrets:","text":"Secret Name Description Example Value <code>EC2_HOST</code> EC2 public IP or domain <code>54.123.45.67</code> <code>EC2_USERNAME</code> SSH username <code>ubuntu</code> <code>EC2_SSH_KEY</code> Private key content Content of your <code>.pem</code> file <code>EC2_PORT</code> SSH port <code>22</code> <code>DB_PASSWORD</code> Database password Your actual DB password <code>MAIL_PASSWORD</code> Email password Gmail app password <code>AWS_ACCESS_KEY</code> AWS access key <code>AKIA...</code> <code>AWS_SECRET_KEY</code> AWS secret key Your AWS secret key"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#how-to-get-ssh-key-content","title":"How to Get SSH Key Content:","text":"<pre><code>cat ~/path/to/your-key.pem\n</code></pre> <p>Copy the entire output including:</p> <pre><code>-----BEGIN RSA PRIVATE KEY-----\n[your key content]\n-----END RSA PRIVATE KEY-----\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#testing-verification","title":"Testing &amp; Verification","text":""},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#1-test-locally","title":"1. Test Locally","text":"<pre><code># Set environment variables\nexport DB_PASSWORD=\"your_password\"\nexport MAIL_PASSWORD=\"your_mail_password\"\nexport AWS_ACCESS_KEY=\"your_access_key\"\nexport AWS_SECRET_KEY=\"your_secret_key\"\n\n# Run application\nmvn spring-boot:run\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#2-test-cicd-pipeline","title":"2. Test CI/CD Pipeline","text":"<pre><code># Commit and push to dev branch\ngit checkout dev\ngit add .\ngit commit -m \"Test CI/CD deployment\"\ngit push origin dev\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#3-monitor-deployment","title":"3. Monitor Deployment","text":"<ol> <li>Go to GitHub Actions tab in your repository</li> <li>Watch the workflow progress</li> <li>Check for any errors</li> </ol>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#4-verify-on-ec2","title":"4. Verify on EC2","text":"<pre><code># SSH into EC2\nssh -i your-key.pem ubuntu@YOUR_EC2_IP\n\n# Check service status\nsudo systemctl status YOUR_APP_NAME\n\n# Check logs\ntail -f /opt/YOUR_APP_NAME/app.log\n\n# Or check with journalctl\nsudo journalctl -u YOUR_APP_NAME -f\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#5-test-application-endpoint","title":"5. Test Application Endpoint","text":"<pre><code># Test with HTTPS (if using Caddy)\ncurl -I https://YOUR_EC2_IP.nip.io\n\n# Test with HTTP\ncurl -I http://YOUR_EC2_IP:8080\n\n# Expected response: HTTP 401 (if Spring Security is enabled)\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#application-wont-start","title":"Application Won't Start","text":"<p>Check logs:</p> <pre><code>sudo journalctl -u YOUR_APP_NAME -n 100\ntail -100 /opt/YOUR_APP_NAME/app.log\n</code></pre> <p>Common issues: - Missing environment variables - Database connection timeout - Port already in use - Insufficient memory</p>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#database-connection-failed","title":"Database Connection Failed","text":"<p>Test connectivity:</p> <pre><code>nc -zv your-db-host 5432\n</code></pre> <p>Solution: Update RDS security group to allow EC2 access</p>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#ssl-certificate-error","title":"SSL Certificate Error","text":"<p>Using self-signed certificate:</p> <pre><code>curl -Ik https://YOUR_EC2_IP.nip.io\n</code></pre> <p>Solution: Update Caddyfile to use Let's Encrypt (remove <code>tls internal</code>)</p>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#workflow-failed","title":"Workflow Failed","text":"<p>Check GitHub Actions logs: 1. Go to Actions tab 2. Click on failed workflow 3. Review error messages</p> <p>Common issues: - Missing GitHub secrets - Wrong SSH key format - Network connectivity issues</p>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#check-service-status","title":"Check Service Status","text":"<pre><code># Application status\nsudo systemctl status YOUR_APP_NAME\n\n# Caddy status\nsudo systemctl status caddy\n\n# Restart services\nsudo systemctl restart YOUR_APP_NAME\nsudo systemctl reload caddy\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#check-environment-variables","title":"Check Environment Variables","text":"<pre><code>sudo systemctl show YOUR_APP_NAME | grep Environment\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#check-disk-space","title":"Check Disk Space","text":"<pre><code>df -h\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#check-memory","title":"Check Memory","text":"<pre><code>free -h\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#view-caddy-logs","title":"View Caddy Logs","text":"<pre><code>tail -f /var/log/caddy/access.log\nsudo journalctl -u caddy -f\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#directory-structure","title":"Directory Structure","text":"<pre><code>your-spring-boot-app/\n\u251c\u2500\u2500 .github/\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u2514\u2500\u2500 deploy-dev.yml\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 main/\n\u2502       \u2514\u2500\u2500 resources/\n\u2502           \u2514\u2500\u2500 application.properties\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 .env.example\n\u251c\u2500\u2500 pom.xml\n\u2514\u2500\u2500 CI_CD_SETUP_GUIDE.md\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#quick-reference-commands","title":"Quick Reference Commands","text":""},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#on-ec2-server","title":"On EC2 Server:","text":"<pre><code># View application logs\ntail -f /opt/YOUR_APP_NAME/app.log\n\n# Restart application\nsudo systemctl restart YOUR_APP_NAME\n\n# Check application status\nsudo systemctl status YOUR_APP_NAME\n\n# Reload Caddy\nsudo systemctl reload caddy\n\n# Test database connection\nnc -zv your-db-host 5432\n\n# Test application\ncurl http://localhost:8080\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#on-local-machine","title":"On Local Machine:","text":"<pre><code># Build and test locally\nmvn clean package\nmvn spring-boot:run\n\n# Deploy to dev\ngit push origin dev\n\n# SSH to EC2\nssh -i your-key.pem ubuntu@YOUR_EC2_IP\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#best-practices","title":"Best Practices","text":"<ol> <li>Always use environment variables for secrets - Never commit credentials</li> <li>Test locally first - Ensure app runs before deploying</li> <li>Monitor logs - Check logs after each deployment</li> <li>Use branches - Deploy <code>dev</code> branch to dev server, <code>main</code> to production</li> <li>Backup database - Regular backups before deployments</li> <li>Keep dependencies updated - Regular security updates</li> <li>Use HTTPS - Always enable SSL in production</li> <li>Set up monitoring - Use CloudWatch or similar tools</li> </ol>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#additional-features-to-add","title":"Additional Features to Add","text":""},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#health-check-endpoint","title":"Health Check Endpoint","text":"<p>Add to <code>application.properties</code>:</p> <pre><code>management.endpoints.web.exposure.include=health,info\nmanagement.endpoint.health.show-details=always\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#multiple-environments","title":"Multiple Environments","text":"<p>Create separate workflows: - <code>deploy-dev.yml</code> - Deploys to dev server - <code>deploy-staging.yml</code> - Deploys to staging server - <code>deploy-prod.yml</code> - Deploys to production server</p>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#automated-rollback","title":"Automated Rollback","text":"<p>Add rollback step in workflow:</p> <pre><code>- name: Rollback on failure\n  if: failure()\n  run: |\n    # Copy previous version and restart\n</code></pre>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#database-migrations","title":"Database Migrations","text":"<p>Add Flyway or Liquibase to <code>pom.xml</code> for automated migrations.</p>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#support-resources","title":"Support &amp; Resources","text":"<ul> <li>GitHub Actions Documentation: https://docs.github.com/en/actions</li> <li>Caddy Documentation: https://caddyserver.com/docs/</li> <li>Spring Boot Documentation: https://spring.io/projects/spring-boot</li> <li>AWS EC2 Documentation: https://docs.aws.amazon.com/ec2/</li> </ul>"},{"location":"CI_CD/spring_boot_ci_cd_setup_guide/#license","title":"License","text":"<p>This guide is provided as-is for educational and development purposes.</p> <p>Created: November 2025 Last Updated: November 2025</p>"},{"location":"aws/aws-apache-flink/","title":"AWS Managed Service for Apache Flink Developer Guide","text":"<p>This guide explains Amazon Managed Service for Apache Flink concepts in a practical, developer-focused way, including stream processing, SQL queries, windowing, state management, and integration patterns, using real application examples.</p>"},{"location":"aws/aws-apache-flink/#table-of-contents","title":"Table of Contents","text":"<ol> <li>What is AWS Managed Service for Apache Flink</li> <li>Where Flink Fits in an Application</li> <li>Flink vs Kinesis Data Analytics vs Lambda</li> <li>Core Concepts: Applications and State</li> <li>Data Sources and Destinations</li> <li>Flink SQL for Stream Processing</li> <li>Windowing and Aggregations</li> <li>Stateful Processing</li> <li>Scaling and Parallelism</li> <li>Checkpoints and Fault Tolerance</li> <li>Real Application Example: Fraud Detection</li> <li>Best Practices</li> <li>Common Exam &amp; Interview Notes</li> </ol>"},{"location":"aws/aws-apache-flink/#what-is-aws-managed-service-for-apache-flink","title":"What is AWS Managed Service for Apache Flink","text":"<p>Amazon Managed Service for Apache Flink (formerly Kinesis Data Analytics) is a fully managed service for processing and analyzing streaming data in real-time using Apache Flink.</p> <p>Core idea:</p> <p>Flink transforms streaming data using SQL or Java/Scala code without managing infrastructure.</p>"},{"location":"aws/aws-apache-flink/#key-features","title":"Key Features","text":"<ul> <li>Real-time stream processing</li> <li>SQL and programming APIs (Java, Scala, Python)</li> <li>Stateful computations</li> <li>Exactly-once processing</li> <li>Automatic scaling</li> <li>Built-in fault tolerance</li> </ul>"},{"location":"aws/aws-apache-flink/#where-flink-fits-in-an-application","title":"Where Flink Fits in an Application","text":""},{"location":"aws/aws-apache-flink/#typical-architecture","title":"Typical Architecture","text":"<pre><code>Data Sources\n    \u2193\nKinesis / Kafka / MSK\n    \u2193\nManaged Flink Application\n    \u2193\nKinesis / S3 / Redshift / OpenSearch\n    \u2193\nDashboards / Alerts\n</code></pre> <p>Use Flink when:</p> <ul> <li>Complex stream transformations needed</li> <li>Real-time aggregations required</li> <li>Stateful processing is essential</li> <li>SQL queries on streaming data</li> <li>Low-latency analytics (sub-second)</li> </ul>"},{"location":"aws/aws-apache-flink/#flink-vs-kinesis-data-analytics-vs-lambda","title":"Flink vs Kinesis Data Analytics vs Lambda","text":"Feature Flink Kinesis Data Analytics Lambda Processing Model Stream processing framework SQL on streams Event-driven functions Language Java, Scala, Python, SQL SQL only Multiple languages State Management Advanced stateful Limited Stateless (default) Complexity Complex transformations Simple queries Simple logic Windowing Advanced Basic Manual Exactly-Once Yes Yes At-least-once Scaling Automatic parallelism Automatic Concurrent executions Use Case Complex analytics Simple SQL queries Event processing"},{"location":"aws/aws-apache-flink/#when-to-use-what","title":"When to Use What","text":"<p>Flink:</p> <ul> <li>Complex event processing</li> <li>Stateful computations</li> <li>Advanced windowing</li> <li>Pattern detection</li> <li>Machine learning on streams</li> </ul> <p>Kinesis Data Analytics (SQL):</p> <ul> <li>Simple SQL queries</li> <li>Quick prototyping</li> <li>Basic aggregations</li> <li>Low code requirements</li> </ul> <p>Lambda:</p> <ul> <li>Simple transformations</li> <li>Event-driven logic</li> <li>Serverless architecture</li> <li>Integration with AWS services</li> </ul>"},{"location":"aws/aws-apache-flink/#core-concepts-applications-and-state","title":"Core Concepts: Applications and State","text":""},{"location":"aws/aws-apache-flink/#flink-application","title":"Flink Application","text":"<p>A Flink application is a program that processes streaming data.</p> <p>Components:</p> <ul> <li>Source: Where data comes from (Kinesis, Kafka)</li> <li>Processing: Transformations, aggregations, joins</li> <li>Sink: Where data goes to (S3, Kinesis, Redshift)</li> </ul>"},{"location":"aws/aws-apache-flink/#state","title":"State","text":"<p>State is data that Flink maintains across events for stateful processing.</p> <p>Types of State:</p> <p>Keyed State:</p> <ul> <li>Associated with a specific key</li> <li>Example: User session data, account balance</li> </ul> <p>Operator State:</p> <ul> <li>Associated with a processing operator</li> <li>Example: Buffered records, counters</li> </ul>"},{"location":"aws/aws-apache-flink/#why-state-matters","title":"Why State Matters","text":"<ul> <li>Calculate running totals</li> <li>Detect patterns across events</li> <li>Maintain session information</li> <li>Track user behavior</li> </ul>"},{"location":"aws/aws-apache-flink/#data-sources-and-destinations","title":"Data Sources and Destinations","text":""},{"location":"aws/aws-apache-flink/#supported-sources","title":"Supported Sources","text":"<p>Amazon Kinesis Data Streams:</p> <ul> <li>Real-time data ingestion</li> <li>Most common source</li> </ul> <p>Amazon MSK (Managed Kafka):</p> <ul> <li>High-throughput streaming</li> <li>Kafka compatibility</li> </ul> <p>Custom Sources:</p> <ul> <li>REST APIs</li> <li>Databases</li> <li>File systems</li> </ul>"},{"location":"aws/aws-apache-flink/#supported-destinations-sinks","title":"Supported Destinations (Sinks)","text":"<p>Amazon Kinesis Data Streams:</p> <ul> <li>Send processed data downstream</li> </ul> <p>Amazon S3:</p> <ul> <li>Store results in data lake</li> <li>Parquet, JSON, CSV formats</li> </ul> <p>Amazon Redshift:</p> <ul> <li>Load analytics results</li> <li>Business intelligence</li> </ul> <p>Amazon OpenSearch:</p> <ul> <li>Search and visualization</li> <li>Real-time dashboards</li> </ul> <p>Custom Sinks:</p> <ul> <li>DynamoDB (via Lambda)</li> <li>RDS (via Lambda)</li> <li>External APIs</li> </ul>"},{"location":"aws/aws-apache-flink/#flink-sql-for-stream-processing","title":"Flink SQL for Stream Processing","text":""},{"location":"aws/aws-apache-flink/#basic-query-structure","title":"Basic Query Structure","text":"<pre><code>CREATE TABLE source_table (\n    user_id VARCHAR,\n    event_type VARCHAR,\n    amount DECIMAL,\n    event_time TIMESTAMP(3),\n    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND\n)\nWITH (\n    'connector' = 'kinesis',\n    'stream' = 'input-stream',\n    'aws.region' = 'us-east-1'\n);\n\nCREATE TABLE destination_table (\n    user_id VARCHAR,\n    total_amount DECIMAL,\n    event_count BIGINT\n)\nWITH (\n    'connector' = 'kinesis',\n    'stream' = 'output-stream',\n    'aws.region' = 'us-east-1'\n);\n\nINSERT INTO destination_table\nSELECT\n    user_id,\n    SUM(amount) as total_amount,\n    COUNT(*) as event_count\nFROM source_table\nWHERE event_type = 'purchase'\nGROUP BY user_id;\n</code></pre>"},{"location":"aws/aws-apache-flink/#key-sql-features","title":"Key SQL Features","text":"<p>Filtering:</p> <pre><code>WHERE event_type = 'purchase' AND amount &gt; 100\n</code></pre> <p>Aggregations:</p> <pre><code>SUM(amount), COUNT(*), AVG(price), MAX(quantity)\n</code></pre> <p>Joins (Stream-to-Stream):</p> <pre><code>SELECT a.user_id, a.order_id, b.product_name\nFROM orders a\nJOIN products b\nON a.product_id = b.product_id\nWHERE a.event_time BETWEEN b.event_time - INTERVAL '1' HOUR\n                       AND b.event_time + INTERVAL '1' HOUR\n</code></pre>"},{"location":"aws/aws-apache-flink/#windowing-and-aggregations","title":"Windowing and Aggregations","text":""},{"location":"aws/aws-apache-flink/#what-is-windowing","title":"What is Windowing?","text":"<p>Windowing groups streaming data into finite chunks for aggregation.</p>"},{"location":"aws/aws-apache-flink/#window-types","title":"Window Types","text":""},{"location":"aws/aws-apache-flink/#1-tumbling-window","title":"1. Tumbling Window","text":"<p>Fixed-size, non-overlapping windows</p> <pre><code>SELECT\n    user_id,\n    TUMBLE_START(event_time, INTERVAL '5' MINUTE) as window_start,\n    SUM(amount) as total\nFROM transactions\nGROUP BY\n    user_id,\n    TUMBLE(event_time, INTERVAL '5' MINUTE)\n</code></pre> <p>Visualization:</p> <pre><code>[0-5min] [5-10min] [10-15min]\n</code></pre> <p>Use cases: Hourly metrics, daily summaries</p>"},{"location":"aws/aws-apache-flink/#2-hopping-window","title":"2. Hopping Window","text":"<p>Fixed-size, overlapping windows</p> <pre><code>SELECT\n    user_id,\n    HOP_START(event_time, INTERVAL '1' MINUTE, INTERVAL '5' MINUTE) as window_start,\n    COUNT(*) as event_count\nFROM events\nGROUP BY\n    user_id,\n    HOP(event_time, INTERVAL '1' MINUTE, INTERVAL '5' MINUTE)\n</code></pre> <p>Visualization:</p> <pre><code>[0-5min]\n  [1-6min]\n    [2-7min]\n</code></pre> <p>Use cases: Moving averages, sliding metrics</p>"},{"location":"aws/aws-apache-flink/#3-session-window","title":"3. Session Window","text":"<p>Dynamic windows based on inactivity gaps</p> <pre><code>SELECT\n    user_id,\n    SESSION_START(event_time, INTERVAL '15' MINUTE) as session_start,\n    COUNT(*) as clicks_in_session\nFROM clicks\nGROUP BY\n    user_id,\n    SESSION(event_time, INTERVAL '15' MINUTE)\n</code></pre> <p>Visualization:</p> <pre><code>Activity \u2192 Gap (15min) \u2192 New Session\n[Session 1]  idle  [Session 2]\n</code></pre> <p>Use cases: User sessions, engagement tracking</p>"},{"location":"aws/aws-apache-flink/#stateful-processing","title":"Stateful Processing","text":""},{"location":"aws/aws-apache-flink/#why-stateful-processing","title":"Why Stateful Processing?","text":"<ul> <li>Maintain context across events</li> <li>Detect patterns</li> <li>Calculate running totals</li> <li>Track user journeys</li> </ul>"},{"location":"aws/aws-apache-flink/#example-running-sum","title":"Example: Running Sum","text":"<pre><code>-- Calculate running total per user\nSELECT\n    user_id,\n    event_time,\n    amount,\n    SUM(amount) OVER (\n        PARTITION BY user_id\n        ORDER BY event_time\n        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n    ) as running_total\nFROM transactions\n</code></pre>"},{"location":"aws/aws-apache-flink/#example-pattern-detection","title":"Example: Pattern Detection","text":"<p>Detect 3 failed login attempts within 5 minutes:</p> <pre><code>SELECT *\nFROM login_events\nMATCH_RECOGNIZE (\n    PARTITION BY user_id\n    ORDER BY event_time\n    MEASURES\n        FIRST(A.event_time) as first_attempt,\n        LAST(C.event_time) as last_attempt\n    PATTERN (A B C)\n    DEFINE\n        A AS A.status = 'failed',\n        B AS B.status = 'failed',\n        C AS C.status = 'failed'\n)\nWHERE last_attempt - first_attempt &lt; INTERVAL '5' MINUTE\n</code></pre>"},{"location":"aws/aws-apache-flink/#state-backend","title":"State Backend","text":"<p>Flink stores state in:</p> <ul> <li>RocksDB (default): Disk-based, large state</li> <li>Heap: Memory-based, fast but limited</li> </ul>"},{"location":"aws/aws-apache-flink/#scaling-and-parallelism","title":"Scaling and Parallelism","text":""},{"location":"aws/aws-apache-flink/#parallelism-units-kpus","title":"Parallelism Units (KPUs)","text":"<p>Kinesis Processing Unit (KPU):</p> <ul> <li>1 KPU = 1 vCPU + 4 GB memory</li> <li>Minimum: 1 KPU</li> <li>Auto-scaling available</li> </ul>"},{"location":"aws/aws-apache-flink/#how-scaling-works","title":"How Scaling Works","text":"<pre><code>Input Stream (multiple shards)\n    \u2193\nFlink Application (multiple parallel tasks)\n    \u2193\nOutput Stream (multiple shards)\n</code></pre>"},{"location":"aws/aws-apache-flink/#parallelism-example","title":"Parallelism Example","text":"<pre><code>3 Kinesis shards \u2192 6 KPUs (2 per shard)\n    \u2193\nProcess 3x more data\n</code></pre>"},{"location":"aws/aws-apache-flink/#auto-scaling","title":"Auto-Scaling","text":"<ul> <li>Monitors CPU and backpressure</li> <li>Adds/removes KPUs automatically</li> <li>Scales between min and max settings</li> </ul>"},{"location":"aws/aws-apache-flink/#checkpoints-and-fault-tolerance","title":"Checkpoints and Fault Tolerance","text":""},{"location":"aws/aws-apache-flink/#checkpoints","title":"Checkpoints","text":"<p>Checkpoints are periodic snapshots of application state.</p> <p>Purpose:</p> <ul> <li>Enable fault recovery</li> <li>Guarantee exactly-once processing</li> <li>Allow application updates</li> </ul>"},{"location":"aws/aws-apache-flink/#how-checkpoints-work","title":"How Checkpoints Work","text":"<pre><code>1. Flink triggers checkpoint\n2. State is saved to S3\n3. Processing continues\n4. If failure occurs \u2192 restore from last checkpoint\n</code></pre>"},{"location":"aws/aws-apache-flink/#configuration","title":"Configuration","text":"<pre><code>SET 'execution.checkpointing.interval' = '60s';\nSET 'execution.checkpointing.mode' = 'EXACTLY_ONCE';\n</code></pre>"},{"location":"aws/aws-apache-flink/#exactly-once-processing","title":"Exactly-Once Processing","text":"<p>Flink guarantees exactly-once semantics:</p> <ul> <li>Each record processed exactly once</li> <li>No duplicates</li> <li>No data loss</li> </ul>"},{"location":"aws/aws-apache-flink/#recovery-flow","title":"Recovery Flow","text":"<pre><code>Application Running \u2192 Checkpoint Saved\n    \u2193\nFailure Occurs\n    \u2193\nRestore from Last Checkpoint\n    \u2193\nResume Processing\n</code></pre>"},{"location":"aws/aws-apache-flink/#real-application-example-fraud-detection","title":"Real Application Example: Fraud Detection","text":""},{"location":"aws/aws-apache-flink/#scenario","title":"Scenario","text":"<p>E-commerce platform detecting fraudulent transactions in real-time.</p>"},{"location":"aws/aws-apache-flink/#architecture","title":"Architecture","text":"<pre><code>Payment Events (Kinesis)\n    \u2193\nManaged Flink Application\n    \u2193\n\u251c\u2500\u2500 Real-time Alerts (Kinesis \u2192 Lambda \u2192 SNS)\n\u2514\u2500\u2500 Analytics Storage (S3)\n</code></pre>"},{"location":"aws/aws-apache-flink/#fraud-detection-rules","title":"Fraud Detection Rules","text":"<p>Rule 1: Multiple high-value transactions in short time</p> <pre><code>-- Detect 3+ transactions over $500 within 10 minutes\nSELECT\n    user_id,\n    TUMBLE_START(event_time, INTERVAL '10' MINUTE) as window_start,\n    COUNT(*) as transaction_count,\n    SUM(amount) as total_amount\nFROM transactions\nWHERE amount &gt; 500\nGROUP BY\n    user_id,\n    TUMBLE(event_time, INTERVAL '10' MINUTE)\nHAVING COUNT(*) &gt;= 3\n</code></pre> <p>Rule 2: Transactions from multiple countries</p> <pre><code>-- Detect purchases from 2+ countries within 1 hour\nSELECT\n    user_id,\n    HOP_START(event_time, INTERVAL '5' MINUTE, INTERVAL '1' HOUR) as window_start,\n    COUNT(DISTINCT country) as country_count,\n    COLLECT(country) as countries\nFROM transactions\nGROUP BY\n    user_id,\n    HOP(event_time, INTERVAL '5' MINUTE, INTERVAL '1' HOUR)\nHAVING COUNT(DISTINCT country) &gt;= 2\n</code></pre> <p>Rule 3: Failed \u2192 Successful pattern</p> <pre><code>-- Detect failed attempt followed by successful transaction\nSELECT *\nFROM transactions\nMATCH_RECOGNIZE (\n    PARTITION BY user_id, card_number\n    ORDER BY event_time\n    MEASURES\n        A.event_time as failed_time,\n        B.event_time as success_time,\n        B.amount as amount\n    PATTERN (A+ B)\n    WITHIN INTERVAL '30' MINUTE\n    DEFINE\n        A AS A.status = 'declined',\n        B AS B.status = 'approved' AND B.amount &gt; 1000\n)\n</code></pre>"},{"location":"aws/aws-apache-flink/#output-flow","title":"Output Flow","text":"<p>Suspicious Transaction Detected:</p> <ol> <li>Flink identifies pattern</li> <li>Sends alert to output Kinesis stream</li> <li>Lambda consumes alert</li> <li>SNS notifies fraud team</li> <li>Transaction flagged in DynamoDB</li> </ol> <p>Benefits:</p> <ul> <li>Real-time: Detect fraud in milliseconds</li> <li>Stateful: Track patterns across events</li> <li>Scalable: Handle millions of transactions</li> <li>Accurate: Complex rules with low false positives</li> </ul>"},{"location":"aws/aws-apache-flink/#best-practices","title":"Best Practices","text":"<ol> <li>Choose appropriate windowing: Match business requirements</li> <li>Configure checkpointing: Balance frequency vs performance</li> <li>Use event time over processing time: More accurate results</li> <li>Set watermarks correctly: Handle late data</li> <li>Optimize parallelism: Match source partitions</li> <li>Monitor metrics: Backpressure, checkpoint duration</li> <li>Use Flink SQL for simple cases: Faster development</li> <li>Use Java/Scala for complex logic: More flexibility</li> <li>Test with production-like data: Avoid surprises</li> <li>Enable auto-scaling: Handle variable workloads</li> <li>Set appropriate state backend: RocksDB for large state</li> <li>Handle late data: Use allowed lateness settings</li> </ol>"},{"location":"aws/aws-apache-flink/#common-exam-interview-notes","title":"Common Exam &amp; Interview Notes","text":"<ul> <li>Flink provides exactly-once processing semantics</li> <li>Stateful processing maintains context across events</li> <li>Windowing groups unbounded streams into finite chunks</li> <li>Tumbling windows: Non-overlapping, fixed-size</li> <li>Hopping windows: Overlapping, fixed-size</li> <li>Session windows: Dynamic, based on inactivity gaps</li> <li>Checkpoints enable fault tolerance and recovery</li> <li>KPU = 1 vCPU + 4 GB memory</li> <li>Supports Flink SQL and programming APIs (Java, Scala, Python)</li> <li>Integrates with Kinesis, MSK, S3, Redshift</li> <li>Event time vs processing time for windowing</li> <li>Watermarks handle out-of-order data</li> <li>Better than Lambda for complex stream processing</li> <li>Better than Kinesis Data Analytics for advanced use cases</li> <li>Auto-scaling based on CPU and backpressure</li> </ul>"},{"location":"aws/aws-apache-flink/#summary","title":"Summary","text":"<ul> <li>Managed Flink processes streaming data with SQL or code</li> <li>Stateful processing enables complex event patterns</li> <li>Windowing aggregates unbounded streams</li> <li>Exactly-once processing guarantees data accuracy</li> <li>Checkpoints provide fault tolerance</li> <li>Auto-scaling handles variable workloads</li> <li>Essential for real-time analytics and complex stream processing</li> </ul>"},{"location":"aws/aws-data-firehose/","title":"AWS Data Firehose Developer Guide","text":"<p>This guide explains Amazon Data Firehose concepts in a practical, developer-focused way, including data ingestion, transformation, buffering, destinations, and delivery patterns, using real application examples.</p>"},{"location":"aws/aws-data-firehose/#table-of-contents","title":"Table of Contents","text":"<ol> <li>What is AWS Data Firehose</li> <li>Where Firehose Fits in an Application</li> <li>Firehose vs Kinesis Data Streams vs Lambda</li> <li>Core Concepts: Delivery Streams</li> <li>Data Sources</li> <li>Data Transformation</li> <li>Buffering and Batching</li> <li>Supported Destinations</li> <li>Data Format Conversion</li> <li>Error Handling and Monitoring</li> <li>Real Application Example: Log Analytics Pipeline</li> <li>Best Practices</li> <li>Common Exam &amp; Interview Notes</li> </ol>"},{"location":"aws/aws-data-firehose/#what-is-aws-data-firehose","title":"What is AWS Data Firehose","text":"<p>Amazon Data Firehose (formerly Kinesis Data Firehose) is a fully managed service for reliably loading streaming data into data lakes, data stores, and analytics services.</p> <p>Core idea:</p> <p>Firehose is the easiest way to capture, transform, and load streaming data into AWS destinations.</p>"},{"location":"aws/aws-data-firehose/#key-features","title":"Key Features","text":"<ul> <li>Zero administration: No servers to manage</li> <li>Automatic scaling: Handles any throughput</li> <li>Built-in transformations: Lambda integration</li> <li>Format conversion: JSON to Parquet/ORC</li> <li>Near real-time delivery: 60 seconds minimum</li> <li>Pay per use: No upfront costs</li> </ul>"},{"location":"aws/aws-data-firehose/#where-firehose-fits-in-an-application","title":"Where Firehose Fits in an Application","text":""},{"location":"aws/aws-data-firehose/#typical-architecture","title":"Typical Architecture","text":"<pre><code>Data Sources (Apps, Logs, IoT)\n        \u2193\n    Firehose\n        \u2193\n    [Optional: Lambda Transform]\n        \u2193\n    [Optional: Format Conversion]\n        \u2193\nDestinations (S3, Redshift, OpenSearch, etc.)\n        \u2193\nAnalytics / Dashboards\n</code></pre> <p>Use Firehose when:</p> <ul> <li>Loading data into S3, Redshift, or OpenSearch</li> <li>No need for real-time processing (near real-time is OK)</li> <li>Simple transformations sufficient</li> <li>You want zero infrastructure management</li> </ul>"},{"location":"aws/aws-data-firehose/#firehose-vs-kinesis-data-streams-vs-lambda","title":"Firehose vs Kinesis Data Streams vs Lambda","text":"Feature Firehose Kinesis Streams Lambda Purpose Load data to destinations Stream processing Event processing Management Fully managed Manage shards Serverless Latency Near real-time (60s+) Real-time (200ms) Real-time Scaling Automatic Manual/auto sharding Automatic Storage No storage 1-365 days No storage Replay No Yes No Destinations Built-in (S3, Redshift) Any Any Transformation Lambda integration Requires consumer Native Cost Model Pay per GB Pay per shard-hour Pay per invocation Use Case ETL to data lake Stream processing Event-driven logic"},{"location":"aws/aws-data-firehose/#when-to-use-what","title":"When to Use What","text":"<p>Firehose:</p> <ul> <li>Loading data to S3/Redshift/OpenSearch</li> <li>Simple ETL pipelines</li> <li>Log aggregation and archival</li> <li>Near real-time analytics</li> </ul> <p>Kinesis Streams:</p> <ul> <li>Custom stream processing</li> <li>Multiple consumers</li> <li>Data replay needed</li> <li>Real-time requirements</li> </ul> <p>Lambda:</p> <ul> <li>Event-driven processing</li> <li>Complex transformations</li> <li>Integration with AWS services</li> <li>Custom destinations</li> </ul>"},{"location":"aws/aws-data-firehose/#core-concepts-delivery-streams","title":"Core Concepts: Delivery Streams","text":""},{"location":"aws/aws-data-firehose/#delivery-stream","title":"Delivery Stream","text":"<p>A delivery stream is the underlying entity of Firehose that you use to deliver data.</p> <p>Components:</p> <ul> <li>Source: Where data comes from</li> <li>Transformation: Optional Lambda processing</li> <li>Conversion: Optional format change (JSON \u2192 Parquet)</li> <li>Destination: Where data is delivered</li> <li>Backup: Optional S3 backup for all data</li> </ul>"},{"location":"aws/aws-data-firehose/#delivery-stream-types","title":"Delivery Stream Types","text":"<p>Direct PUT:</p> <pre><code>Application \u2192 Firehose \u2192 Destination\n</code></pre> <p>Kinesis Data Stream as Source:</p> <pre><code>Application \u2192 Kinesis Stream \u2192 Firehose \u2192 Destination\n</code></pre> <p>MSK as Source:</p> <pre><code>Kafka Producers \u2192 MSK \u2192 Firehose \u2192 Destination\n</code></pre>"},{"location":"aws/aws-data-firehose/#data-sources","title":"Data Sources","text":""},{"location":"aws/aws-data-firehose/#1-direct-put-putrecordputrecordbatch","title":"1. Direct PUT (PutRecord/PutRecordBatch)","text":"<p>Most common method:</p> <pre><code>PutRecordRequest request = PutRecordRequest.builder()\n    .deliveryStreamName(\"my-delivery-stream\")\n    .record(Record.builder()\n        .data(SdkBytes.fromUtf8String(jsonData))\n        .build())\n    .build();\n\nfirehose.putRecord(request);\n</code></pre> <p>Characteristics:</p> <ul> <li>Direct API calls from your application</li> <li>Synchronous writes</li> <li>Automatic batching available</li> </ul>"},{"location":"aws/aws-data-firehose/#2-kinesis-data-streams","title":"2. Kinesis Data Streams","text":"<p>For existing Kinesis streams:</p> <pre><code>Kinesis Stream \u2192 Firehose \u2192 S3\n</code></pre> <p>Benefits:</p> <ul> <li>Leverage existing Kinesis infrastructure</li> <li>Multiple consumers (Kinesis + Firehose)</li> <li>Data replay capability</li> </ul>"},{"location":"aws/aws-data-firehose/#3-amazon-msk","title":"3. Amazon MSK","text":"<p>For Kafka workloads:</p> <pre><code>MSK Cluster \u2192 Firehose \u2192 S3/Redshift\n</code></pre> <p>Benefits:</p> <ul> <li>Kafka compatibility</li> <li>No code changes needed</li> <li>Automatic offset management</li> </ul>"},{"location":"aws/aws-data-firehose/#4-aws-services","title":"4. AWS Services","text":"<p>Integrated sources:</p> <ul> <li>CloudWatch Logs</li> <li>IoT Core</li> <li>EventBridge</li> <li>API Gateway</li> </ul>"},{"location":"aws/aws-data-firehose/#data-transformation","title":"Data Transformation","text":""},{"location":"aws/aws-data-firehose/#lambda-transformation","title":"Lambda Transformation","text":"<p>Firehose can invoke Lambda to transform data before delivery.</p>"},{"location":"aws/aws-data-firehose/#transformation-flow","title":"Transformation Flow","text":"<pre><code>Source Data \u2192 Firehose \u2192 Lambda \u2192 Transformed Data \u2192 Destination\n</code></pre>"},{"location":"aws/aws-data-firehose/#example-enrich-and-filter-logs","title":"Example: Enrich and Filter Logs","text":"<pre><code>import json\nimport base64\n\ndef lambda_handler(event, context):\n    output = []\n\n    for record in event['records']:\n        # Decode input\n        payload = base64.b64decode(record['data']).decode('utf-8')\n        data = json.loads(payload)\n\n        # Transform: Add timestamp, filter errors\n        if data.get('level') == 'ERROR':\n            data['processed_at'] = context.invoked_function_arn\n            data['enriched'] = True\n\n            # Encode output\n            output_data = json.dumps(data) + '\\n'\n            output_record = {\n                'recordId': record['recordId'],\n                'result': 'Ok',\n                'data': base64.b64encode(output_data.encode('utf-8')).decode('utf-8')\n            }\n        else:\n            # Drop non-error logs\n            output_record = {\n                'recordId': record['recordId'],\n                'result': 'Dropped'\n            }\n\n        output.append(output_record)\n\n    return {'records': output}\n</code></pre>"},{"location":"aws/aws-data-firehose/#transformation-results","title":"Transformation Results","text":"<ul> <li>Ok: Record transformed successfully</li> <li>Dropped: Record filtered out</li> <li>ProcessingFailed: Retry or backup to S3</li> </ul>"},{"location":"aws/aws-data-firehose/#use-cases","title":"Use Cases","text":"<ul> <li>Data enrichment (add metadata)</li> <li>Filtering (remove unwanted records)</li> <li>Format conversion (CSV to JSON)</li> <li>Data masking (PII redaction)</li> <li>Schema validation</li> </ul>"},{"location":"aws/aws-data-firehose/#buffering-and-batching","title":"Buffering and Batching","text":""},{"location":"aws/aws-data-firehose/#buffer-conditions","title":"Buffer Conditions","text":"<p>Firehose buffers data before delivering based on size OR time (whichever comes first).</p>"},{"location":"aws/aws-data-firehose/#default-settings","title":"Default Settings","text":"<p>S3:</p> <ul> <li>Buffer size: 5 MB</li> <li>Buffer interval: 300 seconds (5 minutes)</li> </ul> <p>Redshift:</p> <ul> <li>Buffer size: 5 MB</li> <li>Buffer interval: 300 seconds</li> </ul> <p>OpenSearch:</p> <ul> <li>Buffer size: 5 MB</li> <li>Buffer interval: 300 seconds</li> </ul>"},{"location":"aws/aws-data-firehose/#configuration-example","title":"Configuration Example","text":"<pre><code>Buffer Size: 1 MB\nBuffer Interval: 60 seconds\n\nScenario 1: 1 MB reached in 30 seconds \u2192 Deliver\nScenario 2: 50 seconds passed, only 0.5 MB \u2192 Deliver at 60s\n</code></pre>"},{"location":"aws/aws-data-firehose/#important-notes","title":"Important Notes","text":"<ul> <li>Minimum interval: 60 seconds (for S3, Redshift)</li> <li>Lower values: More frequent, smaller files</li> <li>Higher values: Fewer, larger files</li> <li>Cost tradeoff: More frequent = more API calls = higher cost</li> </ul>"},{"location":"aws/aws-data-firehose/#supported-destinations","title":"Supported Destinations","text":""},{"location":"aws/aws-data-firehose/#1-amazon-s3","title":"1. Amazon S3","text":"<p>Most popular destination:</p> <p>Features:</p> <ul> <li>Automatic partitioning</li> <li>Compression (GZIP, Snappy, ZIP)</li> <li>Encryption (SSE-S3, SSE-KMS)</li> <li>Prefix customization</li> </ul> <p>Output Structure:</p> <pre><code>s3://my-bucket/year/month/day/hour/\n    data-timestamp-uuid.json\n</code></pre> <p>Use Cases:</p> <ul> <li>Data lake</li> <li>Long-term storage</li> <li>Analytics with Athena</li> <li>Archive and compliance</li> </ul>"},{"location":"aws/aws-data-firehose/#2-amazon-redshift","title":"2. Amazon Redshift","text":"<p>Data warehouse loading:</p> <p>How it works:</p> <pre><code>Firehose \u2192 S3 (staging) \u2192 Redshift COPY command\n</code></pre> <p>Features:</p> <ul> <li>Automatic COPY commands</li> <li>Column-based compression</li> <li>Retry logic</li> <li>Error logging</li> </ul> <p>Use Cases:</p> <ul> <li>Business intelligence</li> <li>SQL analytics</li> <li>Data warehousing</li> </ul>"},{"location":"aws/aws-data-firehose/#3-amazon-opensearch","title":"3. Amazon OpenSearch","text":"<p>Search and analytics:</p> <p>Features:</p> <ul> <li>Index rotation (daily, weekly, monthly)</li> <li>Automatic retry</li> <li>Backup to S3</li> <li>VPC support</li> </ul> <p>Use Cases:</p> <ul> <li>Log analytics</li> <li>Full-text search</li> <li>Real-time dashboards</li> <li>Security monitoring</li> </ul>"},{"location":"aws/aws-data-firehose/#4-http-endpoints","title":"4. HTTP Endpoints","text":"<p>Custom destinations:</p> <p>Supported:</p> <ul> <li>Datadog</li> <li>Splunk</li> <li>New Relic</li> <li>Custom endpoints</li> </ul> <p>Features:</p> <ul> <li>Authentication headers</li> <li>Retry configuration</li> <li>Backup to S3</li> </ul>"},{"location":"aws/aws-data-firehose/#5-third-party-services","title":"5. Third-Party Services","text":"<p>Direct integrations:</p> <ul> <li>Datadog</li> <li>MongoDB</li> <li>Splunk</li> <li>Coralogix</li> </ul>"},{"location":"aws/aws-data-firehose/#data-format-conversion","title":"Data Format Conversion","text":""},{"location":"aws/aws-data-firehose/#json-to-parquetorc","title":"JSON to Parquet/ORC","text":"<p>Firehose can automatically convert JSON to columnar formats.</p>"},{"location":"aws/aws-data-firehose/#why-convert","title":"Why Convert?","text":"<p>Benefits of Parquet/ORC:</p> <ul> <li>90% smaller than JSON</li> <li>10x faster queries</li> <li>Lower costs for storage and queries</li> <li>Optimized for analytics</li> </ul>"},{"location":"aws/aws-data-firehose/#configuration","title":"Configuration","text":"<p>Requirements:</p> <ul> <li>Input: JSON (one record per line)</li> <li>Schema: AWS Glue Data Catalog table</li> <li>Output: Parquet or ORC</li> </ul>"},{"location":"aws/aws-data-firehose/#example-flow","title":"Example Flow","text":"<pre><code>JSON Logs \u2192 Firehose \u2192 Parquet \u2192 S3 \u2192 Athena Queries\n</code></pre> <p>Input (JSON):</p> <pre><code>{\"user_id\":\"123\",\"event\":\"click\",\"timestamp\":\"2025-01-15T10:30:00Z\"}\n{\"user_id\":\"456\",\"event\":\"view\",\"timestamp\":\"2025-01-15T10:31:00Z\"}\n</code></pre> <p>Output (Parquet):</p> <pre><code>Columnar binary format (90% smaller)\n</code></pre>"},{"location":"aws/aws-data-firehose/#glue-integration","title":"Glue Integration","text":"<pre><code>1. Create Glue Crawler\n2. Crawler scans sample data\n3. Glue creates table schema\n4. Firehose uses schema for conversion\n</code></pre>"},{"location":"aws/aws-data-firehose/#error-handling-and-monitoring","title":"Error Handling and Monitoring","text":""},{"location":"aws/aws-data-firehose/#failed-records","title":"Failed Records","text":"<p>Delivery Failures:</p> <ul> <li>Lambda transformation errors</li> <li>Destination unavailable</li> <li>Format conversion errors</li> </ul> <p>Handling:</p> <pre><code>Failed Record \u2192 S3 Backup Bucket\n    \u2193\nCloudWatch Alarm\n    \u2193\nManual Review/Retry\n</code></pre>"},{"location":"aws/aws-data-firehose/#backup-configuration","title":"Backup Configuration","text":"<p>Options:</p> <ul> <li>All data: Backup everything to S3</li> <li>Failed only: Backup only failed records</li> </ul> <p>Backup Structure:</p> <pre><code>s3://backup-bucket/\n    processing-failed/\n    elasticsearch-failed/\n</code></pre>"},{"location":"aws/aws-data-firehose/#monitoring-metrics","title":"Monitoring Metrics","text":"<p>Key CloudWatch Metrics:</p> <ul> <li><code>DeliveryToS3.Success</code></li> <li><code>DeliveryToS3.DataFreshness</code> (latency)</li> <li><code>IncomingBytes</code></li> <li><code>IncomingRecords</code></li> <li><code>DataReadFromKinesisStream.Bytes</code></li> <li><code>ExecuteProcessing.Duration</code> (Lambda)</li> </ul>"},{"location":"aws/aws-data-firehose/#cloudwatch-alarms","title":"CloudWatch Alarms","text":"<pre><code>Alarm: DeliveryToS3.DataFreshness &gt; 15 minutes\nAction: SNS notification to ops team\n</code></pre>"},{"location":"aws/aws-data-firehose/#real-application-example-log-analytics-pipeline","title":"Real Application Example: Log Analytics Pipeline","text":""},{"location":"aws/aws-data-firehose/#scenario","title":"Scenario","text":"<p>SaaS application with microservices sending logs to centralized analytics.</p>"},{"location":"aws/aws-data-firehose/#architecture","title":"Architecture","text":"<pre><code>10 Microservices (EC2/ECS)\n    \u2193\nCloudWatch Logs\n    \u2193\nFirehose Delivery Stream\n    \u2193\n[Lambda Transform: Filter &amp; Enrich]\n    \u2193\n[Convert JSON \u2192 Parquet]\n    \u2193\nS3 Data Lake\n    \u2193\n\u251c\u2500\u2500 Athena (SQL Queries)\n\u251c\u2500\u2500 QuickSight (Dashboards)\n\u2514\u2500\u2500 Redshift (Data Warehouse)\n</code></pre>"},{"location":"aws/aws-data-firehose/#implementation-steps","title":"Implementation Steps","text":"<p>1. Configure Firehose:</p> <pre><code>Source: CloudWatch Logs\nBuffer: 1 MB or 60 seconds\nTransformation: Lambda (enabled)\nConversion: JSON to Parquet\nDestination: S3\nBackup: Failed records to S3\n</code></pre> <p>2. Lambda Transformation:</p> <pre><code>def lambda_handler(event, context):\n    output = []\n\n    for record in event['records']:\n        payload = json.loads(base64.b64decode(record['data']))\n\n        # Filter: Only ERROR and WARN levels\n        if payload.get('level') in ['ERROR', 'WARN']:\n\n            # Enrich: Add service name, environment\n            payload['service'] = extract_service(payload)\n            payload['environment'] = 'production'\n            payload['processed_timestamp'] = int(time.time())\n\n            output.append({\n                'recordId': record['recordId'],\n                'result': 'Ok',\n                'data': base64.b64encode(\n                    json.dumps(payload).encode('utf-8')\n                )\n            })\n        else:\n            # Drop INFO and DEBUG logs\n            output.append({\n                'recordId': record['recordId'],\n                'result': 'Dropped'\n            })\n\n    return {'records': output}\n</code></pre> <p>3. Glue Schema:</p> <pre><code>CREATE EXTERNAL TABLE logs (\n    timestamp string,\n    level string,\n    message string,\n    service string,\n    environment string,\n    processed_timestamp bigint\n)\nSTORED AS PARQUET\nLOCATION 's3://my-logs-bucket/processed/'\n</code></pre> <p>4. S3 Structure:</p> <pre><code>s3://my-logs-bucket/\n    processed/\n        year=2025/\n            month=01/\n                day=15/\n                    hour=10/\n                        data.parquet\n    backup/\n        processing-failed/\n</code></pre> <p>5. Query with Athena:</p> <pre><code>SELECT\n    service,\n    level,\n    COUNT(*) as error_count\nFROM logs\nWHERE year = '2025'\n  AND month = '01'\n  AND level = 'ERROR'\nGROUP BY service, level\nORDER BY error_count DESC\n</code></pre>"},{"location":"aws/aws-data-firehose/#benefits","title":"Benefits","text":"<ul> <li>90% cost reduction: Parquet compression</li> <li>Near real-time: 1-minute delivery</li> <li>Filtered data: Only errors and warnings</li> <li>Enriched logs: Service and environment tags</li> <li>Queryable: SQL analytics with Athena</li> <li>Zero management: Fully managed pipeline</li> </ul>"},{"location":"aws/aws-data-firehose/#best-practices","title":"Best Practices","text":"<ol> <li>Choose buffer settings wisely: Balance latency vs cost</li> <li>Enable compression: GZIP for S3 (saves 70-90%)</li> <li>Use Parquet conversion: For analytics workloads</li> <li>Configure S3 backup: Capture failed records</li> <li>Monitor DataFreshness: Alert on delivery delays</li> <li>Keep Lambda transforms simple: &lt; 3 minutes execution</li> <li>Use Kinesis as source: When replay is needed</li> <li>Partition S3 data: Use custom prefixes</li> <li>Set up CloudWatch alarms: Monitor failures</li> <li>Test Lambda transforms: Handle edge cases</li> <li>Use VPC for OpenSearch: Security best practice</li> <li>Enable encryption: SSE-S3 or SSE-KMS</li> </ol>"},{"location":"aws/aws-data-firehose/#common-exam-interview-notes","title":"Common Exam &amp; Interview Notes","text":"<ul> <li>Firehose is near real-time (60+ seconds minimum)</li> <li>Automatic scaling: No capacity planning needed</li> <li>No data storage: Data is delivered, not stored</li> <li>Buffer conditions: Size OR time (whichever first)</li> <li>Lambda transformation: 3 minutes max execution time</li> <li>Format conversion: JSON to Parquet/ORC via Glue schema</li> <li>Destinations: S3, Redshift, OpenSearch, HTTP endpoints</li> <li>Backup to S3: All data or failed only</li> <li>No replay capability: Use Kinesis Streams if needed</li> <li>Pay per GB: No upfront costs or provisioning</li> <li>Maximum record size: 1 MB before base64 encoding</li> <li>Batch operations: PutRecordBatch up to 500 records</li> <li>Firehose automatically retries failed deliveries</li> <li>Works with CloudWatch Logs for centralized logging</li> </ul>"},{"location":"aws/aws-data-firehose/#summary","title":"Summary","text":"<ul> <li>Firehose delivers streaming data to AWS destinations</li> <li>Fully managed with automatic scaling</li> <li>Built-in Lambda transformation support</li> <li>JSON to Parquet conversion for analytics</li> <li>Near real-time delivery (60+ seconds)</li> <li>Perfect for ETL, log aggregation, and data lake ingestion</li> <li>Zero infrastructure management required</li> </ul>"},{"location":"aws/aws-kinesis-data-streams/","title":"AWS Kinesis Data Streams Developer Guide","text":"<p>This guide explains Amazon Kinesis Data Streams concepts in a practical, developer-focused way, including shards, partition keys, consumers, scaling, and real-time processing, using real application examples.</p>"},{"location":"aws/aws-kinesis-data-streams/#table-of-contents","title":"Table of Contents","text":"<ol> <li>What is AWS Kinesis Data Streams</li> <li>Where Kinesis Fits in an Application</li> <li>Kinesis vs SQS vs SNS</li> <li>Core Concepts: Streams and Shards</li> <li>Partition Keys and Data Distribution</li> <li>Producers: Writing Data</li> <li>Consumers: Reading Data</li> <li>Data Retention</li> <li>Scaling and Resharding</li> <li>Enhanced Fan-Out</li> <li>Real Application Example: Clickstream Analytics</li> <li>Best Practices</li> <li>Common Exam &amp; Interview Notes</li> </ol>"},{"location":"aws/aws-kinesis-data-streams/#what-is-aws-kinesis-data-streams","title":"What is AWS Kinesis Data Streams","text":"<p>Amazon Kinesis Data Streams is a real-time data streaming service that enables you to collect, process, and analyze streaming data at scale.</p> <p>Core idea:</p> <p>Kinesis captures high-volume, real-time data and allows multiple consumers to process it independently.</p>"},{"location":"aws/aws-kinesis-data-streams/#where-kinesis-fits-in-an-application","title":"Where Kinesis Fits in an Application","text":""},{"location":"aws/aws-kinesis-data-streams/#typical-architecture","title":"Typical Architecture","text":"<pre><code>IoT Devices / Apps / Logs\n        \u2193\n   Kinesis Stream\n        \u2193\n    \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2193        \u2193        \u2193         \u2193\nLambda   Lambda   Firehose   Analytics\n    \u2193        \u2193        \u2193         \u2193\n  DynamoDB  S3   Data Lake  Dashboard\n</code></pre> <p>Use Kinesis when:</p> <ul> <li>Processing real-time data streams</li> <li>Multiple consumers need the same data</li> <li>Data needs to be replayed</li> <li>High throughput required (MB/sec)</li> </ul>"},{"location":"aws/aws-kinesis-data-streams/#kinesis-vs-sqs-vs-sns","title":"Kinesis vs SQS vs SNS","text":"Feature Kinesis SQS SNS Pattern Streaming Queue Pub/Sub Data Flow Pull Pull Push Consumers Multiple Single Multiple Ordering Per shard FIFO only No Retention 1-365 days Up to 14 days None Replay Yes No No Throughput MB/sec Messages/sec Messages/sec Use Case Real-time analytics Background jobs Notifications"},{"location":"aws/aws-kinesis-data-streams/#when-to-use-what","title":"When to Use What","text":"<p>Kinesis:</p> <ul> <li>Real-time analytics</li> <li>Log aggregation</li> <li>Clickstream data</li> <li>IoT telemetry</li> </ul> <p>SQS:</p> <ul> <li>Task queues</li> <li>Decoupling services</li> <li>Batch processing</li> </ul> <p>SNS:</p> <ul> <li>Notifications</li> <li>Event broadcasting</li> <li>Alerts</li> </ul>"},{"location":"aws/aws-kinesis-data-streams/#core-concepts-streams-and-shards","title":"Core Concepts: Streams and Shards","text":""},{"location":"aws/aws-kinesis-data-streams/#stream","title":"Stream","text":"<p>A stream is a logical grouping of shards that holds your data records.</p>"},{"location":"aws/aws-kinesis-data-streams/#shard","title":"Shard","text":"<p>A shard is a unit of capacity in a stream.</p> <p>Each shard provides:</p> <ul> <li>Write capacity: 1 MB/sec or 1,000 records/sec</li> <li>Read capacity: 2 MB/sec (standard) or 2 MB/sec per consumer (enhanced)</li> </ul>"},{"location":"aws/aws-kinesis-data-streams/#example","title":"Example","text":"<pre><code>Stream: \"ClickstreamData\"\n    \u251c\u2500\u2500 Shard 1 (1 MB/sec write, 2 MB/sec read)\n    \u251c\u2500\u2500 Shard 2 (1 MB/sec write, 2 MB/sec read)\n    \u2514\u2500\u2500 Shard 3 (1 MB/sec write, 2 MB/sec read)\n\nTotal: 3 MB/sec write, 6 MB/sec read\n</code></pre>"},{"location":"aws/aws-kinesis-data-streams/#shard-count","title":"Shard Count","text":"<ul> <li>Minimum: 1 shard</li> <li>Maximum: No limit (request increase)</li> <li>Cost: Per shard-hour</li> </ul>"},{"location":"aws/aws-kinesis-data-streams/#partition-keys-and-data-distribution","title":"Partition Keys and Data Distribution","text":""},{"location":"aws/aws-kinesis-data-streams/#partition-key","title":"Partition Key","text":"<p>A partition key determines which shard receives the data record.</p>"},{"location":"aws/aws-kinesis-data-streams/#how-it-works","title":"How It Works","text":"<pre><code>Data Record + Partition Key\n        \u2193\n    MD5 Hash\n        \u2193\n  Shard Selection (based on hash range)\n</code></pre>"},{"location":"aws/aws-kinesis-data-streams/#example_1","title":"Example","text":"<pre><code>Record 1: userId = \"user123\" \u2192 Shard 1\nRecord 2: userId = \"user456\" \u2192 Shard 2\nRecord 3: userId = \"user123\" \u2192 Shard 1 (same user, same shard)\n</code></pre>"},{"location":"aws/aws-kinesis-data-streams/#ordering-guarantee","title":"Ordering Guarantee","text":"<p>Records with the same partition key go to the same shard and maintain order.</p>"},{"location":"aws/aws-kinesis-data-streams/#important-rules","title":"Important Rules","text":"<ul> <li>Choose partition keys with high cardinality</li> <li>Avoid hot shards (uneven distribution)</li> <li>Same partition key = same shard = ordered</li> </ul>"},{"location":"aws/aws-kinesis-data-streams/#producers-writing-data","title":"Producers: Writing Data","text":""},{"location":"aws/aws-kinesis-data-streams/#producer-types","title":"Producer Types","text":"<p>AWS SDK:</p> <ul> <li>PutRecord (single record)</li> <li>PutRecords (batch up to 500 records)</li> </ul> <p>Kinesis Producer Library (KPL):</p> <ul> <li>Automatic batching</li> <li>Compression</li> <li>Retries</li> <li>Higher throughput</li> </ul> <p>Kinesis Agent:</p> <ul> <li>For log files</li> <li>Pre-built monitoring</li> </ul>"},{"location":"aws/aws-kinesis-data-streams/#writing-data","title":"Writing Data","text":"<pre><code>PutRecordRequest request = PutRecordRequest.builder()\n    .streamName(\"ClickstreamData\")\n    .partitionKey(\"user123\")  // Determines shard\n    .data(SdkBytes.fromUtf8String(jsonData))\n    .build();\n\nkinesis.putRecord(request);\n</code></pre>"},{"location":"aws/aws-kinesis-data-streams/#limits","title":"Limits","text":"<ul> <li>Maximum record size: 1 MB</li> <li>Maximum throughput per shard: 1 MB/sec or 1,000 records/sec</li> </ul>"},{"location":"aws/aws-kinesis-data-streams/#consumers-reading-data","title":"Consumers: Reading Data","text":""},{"location":"aws/aws-kinesis-data-streams/#consumer-types","title":"Consumer Types","text":""},{"location":"aws/aws-kinesis-data-streams/#1-kinesis-client-library-kcl","title":"1. Kinesis Client Library (KCL)","text":"<p>Characteristics:</p> <ul> <li>Manages distributed consumption</li> <li>Checkpointing (DynamoDB)</li> <li>Load balancing across consumers</li> <li>2 MB/sec per shard (shared)</li> </ul> <p>Use when: Multiple application instances reading</p>"},{"location":"aws/aws-kinesis-data-streams/#2-aws-sdk-getrecords","title":"2. AWS SDK (GetRecords)","text":"<p>Characteristics:</p> <ul> <li>Direct API calls</li> <li>Manual shard management</li> <li>2 MB/sec per shard (shared)</li> <li>5 GetRecords calls/sec per shard</li> </ul> <p>Use when: Simple, single consumer</p>"},{"location":"aws/aws-kinesis-data-streams/#3-lambda","title":"3. Lambda","text":"<p>Characteristics:</p> <ul> <li>Event-driven processing</li> <li>Automatic scaling</li> <li>Built-in checkpointing</li> <li>Batch processing</li> </ul> <p>Use when: Serverless processing needed</p>"},{"location":"aws/aws-kinesis-data-streams/#4-kinesis-data-analytics","title":"4. Kinesis Data Analytics","text":"<p>Characteristics:</p> <ul> <li>SQL queries on streams</li> <li>Real-time analytics</li> <li>No code required</li> </ul> <p>Use when: SQL-based analysis</p>"},{"location":"aws/aws-kinesis-data-streams/#data-retention","title":"Data Retention","text":""},{"location":"aws/aws-kinesis-data-streams/#default-retention","title":"Default Retention","text":"<ul> <li>24 hours (default)</li> </ul>"},{"location":"aws/aws-kinesis-data-streams/#extended-retention","title":"Extended Retention","text":"<ul> <li>Up to 365 days (additional cost)</li> </ul>"},{"location":"aws/aws-kinesis-data-streams/#key-points","title":"Key Points","text":"<ul> <li>Data is stored across 3 AZs</li> <li>Data can be replayed within retention period</li> <li>Older data automatically expires</li> </ul>"},{"location":"aws/aws-kinesis-data-streams/#use-cases-for-long-retention","title":"Use Cases for Long Retention","text":"<ul> <li>Compliance requirements</li> <li>Late data processing</li> <li>Reprocessing for bug fixes</li> <li>Audit trails</li> </ul>"},{"location":"aws/aws-kinesis-data-streams/#scaling-and-resharding","title":"Scaling and Resharding","text":""},{"location":"aws/aws-kinesis-data-streams/#when-to-scale","title":"When to Scale","text":"<p>Scale Up (split shards):</p> <ul> <li>High write throughput</li> <li>ProvisionedThroughputExceeded errors</li> <li>Hot shard detected</li> </ul> <p>Scale Down (merge shards):</p> <ul> <li>Low utilization</li> <li>Cost optimization</li> </ul>"},{"location":"aws/aws-kinesis-data-streams/#resharding-operations","title":"Resharding Operations","text":"<p>Split Shard:</p> <pre><code>Shard 1 (1 MB/sec)\n    \u2193\nShard 1.1 (1 MB/sec) + Shard 1.2 (1 MB/sec)\n</code></pre> <p>Merge Shards:</p> <pre><code>Shard 1 (0.3 MB/sec) + Shard 2 (0.2 MB/sec)\n    \u2193\nShard 3 (1 MB/sec)\n</code></pre>"},{"location":"aws/aws-kinesis-data-streams/#important-notes","title":"Important Notes","text":"<ul> <li>Resharding is not instant</li> <li>Old shards remain until data expires</li> <li>Sequential process (one operation at a time)</li> <li>Can take several minutes</li> </ul>"},{"location":"aws/aws-kinesis-data-streams/#on-demand-mode","title":"On-Demand Mode","text":"<ul> <li>Automatic scaling</li> <li>No manual resharding</li> <li>Pay per GB</li> <li>4 MB/sec write per shard (default)</li> </ul>"},{"location":"aws/aws-kinesis-data-streams/#enhanced-fan-out","title":"Enhanced Fan-Out","text":""},{"location":"aws/aws-kinesis-data-streams/#standard-consumer","title":"Standard Consumer","text":"<ul> <li>2 MB/sec per shard (shared among all consumers)</li> <li>5 GetRecords API calls/sec per shard</li> <li>200 ms latency</li> </ul>"},{"location":"aws/aws-kinesis-data-streams/#enhanced-fan-out-consumer","title":"Enhanced Fan-Out Consumer","text":"<ul> <li>2 MB/sec per shard per consumer</li> <li>Push model (SubscribeToShard)</li> <li>70 ms latency</li> <li>Higher cost</li> </ul>"},{"location":"aws/aws-kinesis-data-streams/#example_2","title":"Example","text":"<pre><code>Standard:\nShard 1 (2 MB/sec total)\n    \u2193\nConsumer A (1 MB/sec) + Consumer B (1 MB/sec)\n\nEnhanced Fan-Out:\nShard 1\n    \u251c\u2500\u2500 Consumer A (2 MB/sec)\n    \u2514\u2500\u2500 Consumer B (2 MB/sec)\n</code></pre>"},{"location":"aws/aws-kinesis-data-streams/#when-to-use","title":"When to Use","text":"<ul> <li>Multiple real-time consumers</li> <li>Low latency required</li> <li>Consumers need independent throughput</li> </ul>"},{"location":"aws/aws-kinesis-data-streams/#real-application-example-clickstream-analytics","title":"Real Application Example: Clickstream Analytics","text":""},{"location":"aws/aws-kinesis-data-streams/#scenario","title":"Scenario","text":"<p>E-commerce site tracking user clicks in real-time.</p>"},{"location":"aws/aws-kinesis-data-streams/#architecture","title":"Architecture","text":"<pre><code>Web Application\n    \u2193\nKinesis Stream (3 shards)\n    \u2193\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2193           \u2193          \u2193            \u2193\nLambda    Lambda   Firehose    Analytics\n  \u2193           \u2193          \u2193            \u2193\nDynamoDB  Redshift     S3      Dashboard\n(real-time) (batch)  (archive) (monitoring)\n</code></pre>"},{"location":"aws/aws-kinesis-data-streams/#data-flow","title":"Data Flow","text":"<p>1. Producer (Web App):</p> <pre><code>kinesis.putRecord({\n  StreamName: \"ClickstreamData\",\n  PartitionKey: userId, // Same user \u2192 same shard \u2192 ordered\n  Data: JSON.stringify({\n    userId: \"user123\",\n    action: \"click\",\n    product: \"laptop-x1\",\n    timestamp: Date.now(),\n  }),\n});\n</code></pre> <p>2. Consumer 1 (Lambda \u2192 DynamoDB):</p> <ul> <li>Real-time user activity tracking</li> <li>Updates user profile</li> <li>Triggers personalized recommendations</li> </ul> <p>3. Consumer 2 (Lambda \u2192 Redshift):</p> <ul> <li>Batch analytics</li> <li>Aggregate hourly metrics</li> <li>Business intelligence</li> </ul> <p>4. Consumer 3 (Firehose \u2192 S3):</p> <ul> <li>Long-term storage</li> <li>Data lake for ML training</li> <li>Compliance archival</li> </ul> <p>5. Consumer 4 (Kinesis Analytics):</p> <ul> <li>Real-time dashboard</li> <li>Anomaly detection</li> <li>Live metrics</li> </ul>"},{"location":"aws/aws-kinesis-data-streams/#key-benefits","title":"Key Benefits","text":"<ul> <li>Real-time: User behavior tracked instantly</li> <li>Multiple consumers: Different teams process independently</li> <li>Replay: Can reprocess last 24 hours</li> <li>Ordered: Each user's clicks processed in order</li> <li>Scalable: Handles millions of events/sec</li> </ul>"},{"location":"aws/aws-kinesis-data-streams/#best-practices","title":"Best Practices","text":"<ol> <li>Choose partition keys wisely: High cardinality (userId, not status)</li> <li>Monitor shard metrics: Watch for hot shards</li> <li>Use KPL for producers: Better batching and throughput</li> <li>Use KCL for consumers: Handles distributed processing</li> <li>Enable enhanced fan-out: For multiple real-time consumers</li> <li>Set appropriate retention: Balance cost vs replay needs</li> <li>Use on-demand mode: For unpredictable workloads</li> <li>Implement error handling: ProvisionedThroughputExceeded</li> <li>Monitor CloudWatch metrics: IncomingBytes, IncomingRecords</li> <li>Consider Kinesis Firehose: For simple S3/Redshift delivery</li> </ol>"},{"location":"aws/aws-kinesis-data-streams/#common-exam-interview-notes","title":"Common Exam &amp; Interview Notes","text":"<ul> <li>1 shard = 1 MB/sec write, 2 MB/sec read</li> <li>Partition key determines shard assignment</li> <li>Same partition key = same shard = ordered</li> <li>Data retention: 24 hours to 365 days</li> <li>Multiple consumers can read independently</li> <li>Enhanced fan-out: 2 MB/sec per consumer per shard</li> <li>KCL uses DynamoDB for checkpointing</li> <li>Resharding is sequential and takes time</li> <li>Maximum record size: 1 MB</li> <li>Use Kinesis for streaming, SQS for queuing</li> <li>On-demand mode: Auto-scaling, no shard management</li> <li>Data stored across 3 AZs</li> <li>Kinesis is real-time (sub-second to seconds)</li> </ul>"},{"location":"aws/aws-kinesis-data-streams/#summary","title":"Summary","text":"<ul> <li>Kinesis handles real-time streaming data at scale</li> <li>Shards provide predictable capacity units</li> <li>Partition keys ensure ordering and distribution</li> <li>Multiple consumers can process data independently</li> <li>Data can be replayed within retention period</li> <li>Enhanced fan-out enables low-latency, high-throughput consumption</li> <li>Essential for real-time analytics and event-driven architectures</li> </ul>"},{"location":"aws/aws-sns-guide/","title":"AWS SNS Developer Guide","text":"<p>This guide explains Amazon SNS concepts in a practical, developer-focused way, including topics, subscriptions, message filtering, fanout patterns, and delivery policies, using real application examples.</p>"},{"location":"aws/aws-sns-guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>What is AWS SNS</li> <li>Where SNS Fits in an Application</li> <li>SNS Topics and Subscriptions</li> <li>Message Publishing</li> <li>Subscription Protocols</li> <li>Message Filtering</li> <li>SNS + SQS Fanout Pattern</li> <li>Message Attributes and Structure</li> <li>Delivery Policies and Retries</li> <li>Real Application Example: Order Notifications</li> <li>Best Practices</li> <li>Common Exam &amp; Interview Notes</li> </ol>"},{"location":"aws/aws-sns-guide/#what-is-aws-sns","title":"What is AWS SNS","text":"<p>Amazon SNS (Simple Notification Service) is a fully managed pub/sub messaging service that enables one-to-many message delivery to multiple subscribers.</p> <p>Core idea:</p> <p>SNS broadcasts messages from publishers to multiple subscribers simultaneously.</p>"},{"location":"aws/aws-sns-guide/#where-sns-fits-in-an-application","title":"Where SNS Fits in an Application","text":""},{"location":"aws/aws-sns-guide/#typical-architecture","title":"Typical Architecture","text":"<pre><code>Backend API (EC2 / Lambda)\n        \u2193\n      SNS Topic\n        \u2193\n    \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2193       \u2193       \u2193         \u2193\n  Email    SMS    SQS     Lambda\n</code></pre> <p>Use SNS when:</p> <ul> <li>Multiple systems need the same event</li> <li>You need to notify users (email, SMS, push)</li> <li>You want to decouple event producers from consumers</li> <li>You need to broadcast messages</li> </ul>"},{"location":"aws/aws-sns-guide/#sns-topics-and-subscriptions","title":"SNS Topics and Subscriptions","text":""},{"location":"aws/aws-sns-guide/#topic","title":"Topic","text":"<p>A topic is a communication channel that acts as an access point for publishers and subscribers.</p> <p>Key characteristics:</p> <ul> <li>Can have multiple subscribers</li> <li>Publishers send one message</li> <li>All subscribers receive a copy</li> </ul>"},{"location":"aws/aws-sns-guide/#subscription","title":"Subscription","text":"<p>A subscription connects an endpoint (email, SQS, Lambda, etc.) to a topic.</p>"},{"location":"aws/aws-sns-guide/#example-flow","title":"Example Flow","text":"<pre><code>Publisher \u2192 Topic \u2192 Subscription 1 (Email)\n                 \u2192 Subscription 2 (SQS)\n                 \u2192 Subscription 3 (Lambda)\n</code></pre>"},{"location":"aws/aws-sns-guide/#message-publishing","title":"Message Publishing","text":""},{"location":"aws/aws-sns-guide/#publishing-methods","title":"Publishing Methods","text":"<ul> <li>AWS SDK</li> <li>AWS CLI</li> <li>AWS Console</li> <li>AWS Services (S3, CloudWatch, etc.)</li> </ul>"},{"location":"aws/aws-sns-guide/#message-structure","title":"Message Structure","text":"<pre><code>{\n  \"Message\": \"Order #12345 has been shipped\",\n  \"Subject\": \"Order Update\",\n  \"MessageAttributes\": {\n    \"orderStatus\": {\n      \"DataType\": \"String\",\n      \"StringValue\": \"shipped\"\n    }\n  }\n}\n</code></pre>"},{"location":"aws/aws-sns-guide/#important-notes","title":"Important Notes","text":"<ul> <li>Maximum message size: 256 KB</li> <li>Messages are not stored (fire and forget)</li> <li>No message replay capability</li> </ul>"},{"location":"aws/aws-sns-guide/#subscription-protocols","title":"Subscription Protocols","text":"<p>SNS supports multiple protocols:</p>"},{"location":"aws/aws-sns-guide/#httphttps","title":"HTTP/HTTPS","text":"<ul> <li>Webhooks to external services</li> <li>Requires endpoint confirmation</li> </ul>"},{"location":"aws/aws-sns-guide/#email","title":"Email","text":"<ul> <li>Plain text email notifications</li> <li>Requires subscriber confirmation</li> </ul>"},{"location":"aws/aws-sns-guide/#email-json","title":"Email-JSON","text":"<ul> <li>Structured JSON in email body</li> <li>For programmatic parsing</li> </ul>"},{"location":"aws/aws-sns-guide/#sms","title":"SMS","text":"<ul> <li>Text messages to phone numbers</li> <li>Regional availability varies</li> </ul>"},{"location":"aws/aws-sns-guide/#sqs","title":"SQS","text":"<ul> <li>Delivers to SQS queue</li> <li>Enables asynchronous processing</li> </ul>"},{"location":"aws/aws-sns-guide/#lambda","title":"Lambda","text":"<ul> <li>Triggers Lambda function directly</li> <li>Serverless event processing</li> </ul>"},{"location":"aws/aws-sns-guide/#application-mobile-push","title":"Application (Mobile Push)","text":"<ul> <li>Push notifications to mobile devices</li> <li>Supports iOS, Android, Fire OS</li> </ul>"},{"location":"aws/aws-sns-guide/#firehose","title":"Firehose","text":"<ul> <li>Stream to data lakes</li> <li>For analytics and archival</li> </ul>"},{"location":"aws/aws-sns-guide/#message-filtering","title":"Message Filtering","text":"<p>Message filtering allows subscribers to receive only messages they're interested in.</p>"},{"location":"aws/aws-sns-guide/#filter-policy","title":"Filter Policy","text":"<p>Defined in JSON format on the subscription:</p> <pre><code>{\n  \"orderStatus\": [\"shipped\", \"delivered\"],\n  \"price\": [{ \"numeric\": [\"&gt;\", 100] }]\n}\n</code></pre>"},{"location":"aws/aws-sns-guide/#how-it-works","title":"How It Works","text":"<pre><code>SNS Topic\n    \u2193\n  Filter Policy (orderStatus = \"shipped\")\n    \u2193\nSubscription (Email) \u2190 Only receives shipped orders\n</code></pre>"},{"location":"aws/aws-sns-guide/#benefits","title":"Benefits","text":"<ul> <li>Reduces unnecessary message delivery</li> <li>Lowers costs</li> <li>Simplifies subscriber logic</li> </ul>"},{"location":"aws/aws-sns-guide/#sns-sqs-fanout-pattern","title":"SNS + SQS Fanout Pattern","text":"<p>The fanout pattern combines SNS and SQS for reliable, parallel processing.</p>"},{"location":"aws/aws-sns-guide/#architecture","title":"Architecture","text":"<pre><code>Backend API\n    \u2193\nSNS Topic\n    \u2193\n  \u250c\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2193       \u2193         \u2193          \u2193\nSQS 1   SQS 2    SQS 3      Email\n  \u2193       \u2193         \u2193\nWorker  Worker   Worker\n</code></pre>"},{"location":"aws/aws-sns-guide/#why-use-this-pattern","title":"Why Use This Pattern","text":"<ul> <li>Reliability: SQS queues buffer messages</li> <li>Parallel Processing: Multiple services process independently</li> <li>Decoupling: Services don't know about each other</li> <li>Retry Logic: Each SQS has its own retry policy</li> </ul>"},{"location":"aws/aws-sns-guide/#example-use-case","title":"Example Use Case","text":"<p>Order placed:</p> <ul> <li>Inventory service receives message</li> <li>Shipping service receives message</li> <li>Analytics service receives message</li> <li>Customer receives email</li> </ul> <p>All happen simultaneously and independently.</p>"},{"location":"aws/aws-sns-guide/#message-attributes-and-structure","title":"Message Attributes and Structure","text":""},{"location":"aws/aws-sns-guide/#standard-format","title":"Standard Format","text":"<p>When SNS delivers to SQS or HTTP, it wraps the message:</p> <pre><code>{\n  \"Type\": \"Notification\",\n  \"MessageId\": \"abc-123\",\n  \"TopicArn\": \"arn:aws:sns:us-east-1:123:MyTopic\",\n  \"Subject\": \"Order Update\",\n  \"Message\": \"Your actual message content\",\n  \"Timestamp\": \"2025-01-15T10:30:00.000Z\",\n  \"MessageAttributes\": {\n    \"orderStatus\": {\n      \"Type\": \"String\",\n      \"Value\": \"shipped\"\n    }\n  }\n}\n</code></pre>"},{"location":"aws/aws-sns-guide/#raw-message-delivery","title":"Raw Message Delivery","text":"<p>For SQS and HTTP subscriptions, you can enable RawMessageDelivery:</p> <ul> <li>Skips SNS envelope</li> <li>Delivers original message directly</li> <li>Cleaner for consumers</li> </ul>"},{"location":"aws/aws-sns-guide/#delivery-policies-and-retries","title":"Delivery Policies and Retries","text":""},{"location":"aws/aws-sns-guide/#default-retry-policy","title":"Default Retry Policy","text":"<p>SNS automatically retries failed deliveries:</p> <ul> <li>HTTP/HTTPS: 3 retries over 20 seconds</li> <li>Lambda: 2 retries</li> <li>SQS: Retries until delivery succeeds</li> </ul>"},{"location":"aws/aws-sns-guide/#custom-delivery-policy","title":"Custom Delivery Policy","text":"<pre><code>{\n  \"http\": {\n    \"defaultHealthyRetryPolicy\": {\n      \"minDelayTarget\": 5,\n      \"maxDelayTarget\": 300,\n      \"numRetries\": 5,\n      \"backoffFunction\": \"exponential\"\n    }\n  }\n}\n</code></pre>"},{"location":"aws/aws-sns-guide/#delivery-status-logging","title":"Delivery Status Logging","text":"<ul> <li>Track successful and failed deliveries</li> <li>Logs to CloudWatch</li> <li>Useful for monitoring</li> </ul>"},{"location":"aws/aws-sns-guide/#real-application-example-order-notifications","title":"Real Application Example: Order Notifications","text":""},{"location":"aws/aws-sns-guide/#scenario","title":"Scenario","text":"<p>User places an order for $150.</p>"},{"location":"aws/aws-sns-guide/#flow","title":"Flow","text":"<pre><code>1. API receives order\n2. API saves to RDS\n3. API publishes to SNS \"OrderEvents\" topic\n</code></pre>"},{"location":"aws/aws-sns-guide/#message","title":"Message","text":"<pre><code>{\n  \"orderId\": \"12345\",\n  \"status\": \"placed\",\n  \"amount\": 150,\n  \"userId\": \"user-789\"\n}\n</code></pre>"},{"location":"aws/aws-sns-guide/#subscriptions","title":"Subscriptions","text":"<p>Email Subscription (with filter):</p> <ul> <li>Filter: <code>amount &gt; 100</code></li> <li>Action: Send order confirmation email</li> </ul> <p>SQS Inventory Queue:</p> <ul> <li>No filter</li> <li>Action: Worker updates inventory</li> </ul> <p>SQS Analytics Queue:</p> <ul> <li>No filter</li> <li>Action: Worker logs event to data warehouse</li> </ul> <p>Lambda Function:</p> <ul> <li>Filter: <code>status = \"placed\"</code></li> <li>Action: Send push notification to mobile app</li> </ul>"},{"location":"aws/aws-sns-guide/#result","title":"Result","text":"<ul> <li>User receives email (amount &gt; 100)</li> <li>Inventory is updated</li> <li>Analytics logged</li> <li>Push notification sent</li> <li>All happen simultaneously</li> </ul>"},{"location":"aws/aws-sns-guide/#best-practices","title":"Best Practices","text":"<ol> <li>Use SNS for broadcasting, SQS for queuing</li> <li>Enable message filtering to reduce costs</li> <li>Use fanout pattern for parallel processing</li> <li>Enable raw message delivery for cleaner messages</li> <li>Set up CloudWatch alarms for failed deliveries</li> <li>Use message attributes for metadata</li> <li>Confirm subscriptions properly (email, HTTP)</li> <li>Consider message size limits (256 KB)</li> <li>Use DLQ with SQS subscriptions for reliability</li> <li>Monitor delivery metrics in CloudWatch</li> </ol>"},{"location":"aws/aws-sns-guide/#common-exam-interview-notes","title":"Common Exam &amp; Interview Notes","text":"<ul> <li>SNS is push-based, SQS is pull-based</li> <li>SNS delivers to multiple subscribers simultaneously</li> <li>Messages are not stored (no replay)</li> <li>Maximum message size: 256 KB</li> <li>SNS + SQS fanout enables parallel processing</li> <li>Message filtering reduces unnecessary deliveries</li> <li>Raw message delivery removes SNS envelope</li> <li>Subscriptions require confirmation (email, HTTP)</li> <li>SNS supports mobile push notifications</li> <li>Use SNS for event-driven architectures</li> </ul>"},{"location":"aws/aws-sns-guide/#summary","title":"Summary","text":"<ul> <li>SNS broadcasts messages to multiple subscribers</li> <li>Topics enable pub/sub patterns</li> <li>Supports multiple protocols (email, SMS, SQS, Lambda, HTTP)</li> <li>Message filtering reduces costs and complexity</li> <li>SNS + SQS fanout enables reliable parallel processing</li> <li>Essential for event-driven and microservices architectures</li> </ul>"},{"location":"aws/aws-sqs-guide/","title":"AWS SQS Developer Guide","text":"<p>This guide explains Amazon SQS concepts in a practical, developer-focused way, including Standard queues, FIFO queues, DLQ, retries, and polling strategies, using real application examples.</p>"},{"location":"aws/aws-sqs-guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>What is AWS SQS</li> <li>Where SQS Fits in an Application</li> <li>Standard SQS Queue</li> <li>FIFO SQS Queue</li> <li>Message Visibility Timeout &amp; Retries</li> <li>Dead Letter Queue (DLQ)</li> <li>Long Polling vs Short Polling</li> <li>Real Application Example: Order &amp; Email</li> <li>Best Practices</li> <li>Common Exam &amp; Interview Notes</li> </ol>"},{"location":"aws/aws-sqs-guide/#what-is-aws-sqs","title":"What is AWS SQS","text":"<p>Amazon SQS (Simple Queue Service) is a fully managed message queue that enables asynchronous communication between application components.</p> <p>Core idea:</p> <p>SQS decouples your API from background or failure-prone tasks.</p>"},{"location":"aws/aws-sqs-guide/#where-sqs-fits-in-an-application","title":"Where SQS Fits in an Application","text":""},{"location":"aws/aws-sqs-guide/#typical-architecture","title":"Typical Architecture","text":"<pre><code>Frontend (Amplify / Web)\n        \u2193\nBackend API (EC2 / Spring Boot)\n        \u2193\n       RDS\n        \u2193\n       SQS  \u2190 async tasks\n        \u2193\nWorker (EC2 / Lambda)\n</code></pre> <p>Use SQS when:</p> <ul> <li>Tasks are slow or heavy</li> <li>Tasks may fail</li> <li>Tasks should not block user requests</li> </ul>"},{"location":"aws/aws-sqs-guide/#standard-sqs-queue","title":"Standard SQS Queue","text":""},{"location":"aws/aws-sqs-guide/#characteristics","title":"Characteristics","text":"<ul> <li>Unlimited throughput</li> <li>At-least-once delivery</li> <li>Order is not guaranteed</li> </ul>"},{"location":"aws/aws-sqs-guide/#use-cases","title":"Use Cases","text":"<ul> <li>Sending emails</li> <li>Background jobs</li> <li>Image processing</li> <li>Analytics events</li> </ul>"},{"location":"aws/aws-sqs-guide/#notes","title":"Notes","text":"<ul> <li>Messages may be delivered more than once</li> <li>Consumers must be idempotent</li> </ul>"},{"location":"aws/aws-sqs-guide/#fifo-sqs-queue","title":"FIFO SQS Queue","text":"<p>FIFO = First In, First Out</p>"},{"location":"aws/aws-sqs-guide/#guarantees","title":"Guarantees","text":"<ul> <li>Exactly-once processing</li> <li>Strict ordering (per MessageGroupId)</li> </ul>"},{"location":"aws/aws-sqs-guide/#required-rules","title":"Required Rules","text":"<ul> <li>Queue name must end with:</li> </ul> <pre><code>.fifo\n</code></pre> <ul> <li>Every message must include:</li> </ul> <pre><code>MessageGroupId\n</code></pre>"},{"location":"aws/aws-sqs-guide/#ordering-behavior","title":"Ordering Behavior","text":"<p>Ordering is guaranteed within a MessageGroupId, not globally.</p>"},{"location":"aws/aws-sqs-guide/#use-cases_1","title":"Use Cases","text":"<ul> <li>Payments</li> <li>Order state transitions</li> <li>Inventory updates</li> </ul>"},{"location":"aws/aws-sqs-guide/#not-recommended-for","title":"Not Recommended For","text":"<ul> <li>Emails</li> <li>Logs</li> <li>High-throughput background jobs</li> </ul>"},{"location":"aws/aws-sqs-guide/#message-visibility-timeout-retries","title":"Message Visibility Timeout &amp; Retries","text":""},{"location":"aws/aws-sqs-guide/#visibility-timeout","title":"Visibility Timeout","text":"<p>When a consumer receives a message:</p> <ul> <li>Message becomes invisible</li> <li>Default: 30 seconds</li> </ul>"},{"location":"aws/aws-sqs-guide/#retry-flow","title":"Retry Flow","text":"<pre><code>Receive \u2192 Fail \u2192 Visibility Timeout \u2192 Retry\n</code></pre> <p>If the message is not deleted, it becomes visible again and can be retried.</p>"},{"location":"aws/aws-sqs-guide/#dead-letter-queue-dlq","title":"Dead Letter Queue (DLQ)","text":"<p>A Dead Letter Queue (DLQ) is a separate SQS queue that stores messages that fail processing multiple times.</p>"},{"location":"aws/aws-sqs-guide/#how-dlq-works","title":"How DLQ Works","text":"<ul> <li>Configure <code>MaxReceiveCount</code> (e.g. 5)</li> <li> <p>After exceeding retries:</p> </li> <li> <p>Message is moved to DLQ</p> </li> <li>Message is removed from the main queue</li> </ul>"},{"location":"aws/aws-sqs-guide/#important-facts","title":"Important Facts","text":"<ul> <li>DLQ does not retry messages</li> <li>DLQ does not process messages</li> <li>DLQ requires manual intervention</li> </ul>"},{"location":"aws/aws-sqs-guide/#purpose","title":"Purpose","text":"<ul> <li>Prevent infinite retry loops</li> <li>Isolate poison messages</li> <li>Enable debugging and recovery</li> </ul>"},{"location":"aws/aws-sqs-guide/#long-polling-vs-short-polling","title":"Long Polling vs Short Polling","text":""},{"location":"aws/aws-sqs-guide/#short-polling-default","title":"Short Polling (Default)","text":"<p>Behavior:</p> <ul> <li>Returns immediately</li> <li>Often returns empty responses</li> </ul> <p>Problems:</p> <ul> <li>Higher cost</li> <li>Wasted CPU</li> </ul> <pre><code>Worker \u2192 SQS \u2192 Empty\nWorker \u2192 SQS \u2192 Empty\n</code></pre>"},{"location":"aws/aws-sqs-guide/#long-polling-recommended","title":"Long Polling (Recommended)","text":"<p>Behavior:</p> <ul> <li>Waits up to 20 seconds</li> <li>Responds immediately when a message arrives</li> </ul> <p>Benefits:</p> <ul> <li>Lower cost</li> <li>Fewer empty responses</li> <li>Better performance</li> </ul> <pre><code>Worker \u2192 SQS (wait)\nSQS \u2192 Worker (message)\n</code></pre>"},{"location":"aws/aws-sqs-guide/#configuration","title":"Configuration","text":"<ul> <li><code>WaitTimeSeconds</code>: 1\u201320 seconds</li> <li>Can be set per queue or per request</li> </ul>"},{"location":"aws/aws-sqs-guide/#real-application-example-order-email","title":"Real Application Example: Order &amp; Email","text":""},{"location":"aws/aws-sqs-guide/#flow","title":"Flow","text":"<pre><code>User places order\n        \u2193\nAPI saves order to RDS\n        \u2193\nAPI sends SEND_EMAIL message to SQS\n        \u2193\nUser gets response immediately\n</code></pre>"},{"location":"aws/aws-sqs-guide/#email-worker","title":"Email Worker","text":"<pre><code>SQS \u2192 Worker \u2192 Send Email\n</code></pre>"},{"location":"aws/aws-sqs-guide/#failure-scenario","title":"Failure Scenario","text":"<ul> <li>Email fails multiple times</li> <li>Message is moved to DLQ</li> <li>Order is successful</li> <li>Email is not sent automatically</li> </ul>"},{"location":"aws/aws-sqs-guide/#recovery-steps","title":"Recovery Steps","text":"<ul> <li>CloudWatch alarm triggers</li> <li>Developer inspects DLQ message</li> <li>Fixes the issue</li> <li>Re-drives message to the main queue</li> </ul>"},{"location":"aws/aws-sqs-guide/#best-practices","title":"Best Practices","text":"<ol> <li>Use Standard queues by default</li> <li>Use FIFO only when order matters</li> <li>Always configure a DLQ</li> <li>Enable long polling</li> <li>Keep messages small (&lt;256 KB)</li> <li>Store large payloads in S3</li> <li>Make consumers idempotent</li> <li>Monitor DLQ with CloudWatch alarms</li> </ol>"},{"location":"aws/aws-sqs-guide/#common-exam-interview-notes","title":"Common Exam &amp; Interview Notes","text":"<ul> <li>FIFO ordering is per <code>MessageGroupId</code></li> <li>DLQ does not retry messages</li> <li>Long polling reduces cost and latency</li> <li>SQS is pull-based</li> <li>SQS enables decoupled, async processing</li> </ul>"},{"location":"aws/aws-sqs-guide/#summary","title":"Summary","text":"<ul> <li>SQS handles asynchronous work</li> <li>Standard queues are fast and scalable</li> <li>FIFO queues are ordered and safe</li> <li>DLQ isolates failures</li> <li>Long polling improves efficiency</li> </ul>"},{"location":"aws/sqs-vs-sns-vs-kinesis/","title":"SQS vs SNS vs Kinesis - Complete Comparison Guide","text":"<p>This guide provides a comprehensive comparison of AWS messaging and streaming services, helping you choose the right service for your use case with practical examples and decision frameworks.</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Quick Comparison Table</li> <li>Core Differences Explained</li> <li>SQS Deep Dive</li> <li>SNS Deep Dive</li> <li>Kinesis Deep Dive</li> <li>Decision Framework</li> <li>Common Patterns and Combinations</li> <li>Real-World Scenarios</li> <li>Cost Comparison</li> <li>When to Use What</li> </ol>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#quick-comparison-table","title":"Quick Comparison Table","text":"Feature SQS SNS Kinesis Data Streams Pattern Message Queue Pub/Sub Streaming Data Flow Pull (polling) Push Pull (polling) Consumers Single (per message) Multiple Multiple Ordering FIFO queues only No Per shard (partition key) Message Persistence Up to 14 days None (fire &amp; forget) 1-365 days Replay No No Yes Throughput Unlimited (Standard) Unlimited Per shard (1 MB/sec write) Latency Seconds Real-time (push) Real-time (200ms) Delivery At-least-once (Standard) At-least-once At-least-once Scaling Automatic Automatic Manual/Auto (shards) Cost Model Per request Per request Per shard-hour Primary Use Case Task queues Broadcasting events Stream processing"},{"location":"aws/sqs-vs-sns-vs-kinesis/#core-differences-explained","title":"Core Differences Explained","text":""},{"location":"aws/sqs-vs-sns-vs-kinesis/#1-message-vs-event-vs-stream","title":"1. Message vs Event vs Stream","text":"<p>SQS - Message Queue:</p> <pre><code>Producer \u2192 Queue \u2192 Consumer (one at a time)\n\nMessage: \"Send email to user@example.com\"\n</code></pre> <p>SNS - Event Notification:</p> <pre><code>Publisher \u2192 Topic \u2192 All Subscribers (simultaneously)\n\nEvent: \"Order #123 was placed\"\n</code></pre> <p>Kinesis - Data Stream:</p> <pre><code>Producer \u2192 Stream \u2192 Multiple Consumers (independently)\n\nStream: Continuous flow of clickstream data\n</code></pre>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#2-pull-vs-push","title":"2. Pull vs Push","text":"<p>SQS (Pull):</p> <pre><code>Consumer: \"Give me messages\" \u2192 SQS \u2192 Messages\nConsumer: \"Give me messages\" \u2192 SQS \u2192 Empty\n</code></pre> <p>SNS (Push):</p> <pre><code>Publisher \u2192 SNS \u2192 Pushes to all subscribers immediately\n</code></pre> <p>Kinesis (Pull):</p> <pre><code>Consumer: \"Give me records from shard 1\" \u2192 Kinesis \u2192 Records\nConsumer: \"Give me records from shard 2\" \u2192 Kinesis \u2192 Records\n</code></pre>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#3-one-consumer-vs-many","title":"3. One Consumer vs Many","text":"<p>SQS:</p> <pre><code>Message in queue \u2192 Consumed by ONE worker\n(Message deleted after processing)\n</code></pre> <p>SNS:</p> <pre><code>Event published \u2192 Delivered to ALL subscribers\n(Each subscriber gets a copy)\n</code></pre> <p>Kinesis:</p> <pre><code>Record in stream \u2192 Read by MULTIPLE consumers independently\n(Record stays in stream for retention period)\n</code></pre>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#sqs-deep-dive","title":"SQS Deep Dive","text":""},{"location":"aws/sqs-vs-sns-vs-kinesis/#what-it-does","title":"What It Does","text":"<p>Decouples producers from consumers using a reliable message queue.</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#architecture","title":"Architecture","text":"<pre><code>API (Producer)\n    \u2193\nSQS Queue (buffer)\n    \u2193\nWorker 1 \u2190 polls\nWorker 2 \u2190 polls\nWorker 3 \u2190 polls\n</code></pre>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#key-characteristics","title":"Key Characteristics","text":"<p>Standard Queue: * Unlimited throughput * At-least-once delivery (possible duplicates) * Best-effort ordering * Use for: Most async tasks</p> <p>FIFO Queue: * 300 TPS (without batching) * Exactly-once processing * Strict ordering (per MessageGroupId) * Use for: Payments, state transitions</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#when-to-use-sqs","title":"When to Use SQS","text":"<p>\u2705 Background job processing \u2705 Decoupling microservices \u2705 Load leveling \u2705 Delayed execution \u2705 Retry logic needed</p> <p>\u274c Broadcasting to multiple services \u274c Real-time streaming analytics \u274c Data replay required</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#example-use-case","title":"Example Use Case","text":"<p>Order Processing:</p> <pre><code>1. User places order\n2. API saves to database\n3. API sends message to SQS: \"Process order #123\"\n4. Worker polls SQS\n5. Worker processes order (inventory, shipping)\n6. Worker deletes message from queue\n</code></pre>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#sns-deep-dive","title":"SNS Deep Dive","text":""},{"location":"aws/sqs-vs-sns-vs-kinesis/#what-it-does_1","title":"What It Does","text":"<p>Broadcasts events to multiple subscribers simultaneously.</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#architecture_1","title":"Architecture","text":"<pre><code>Order Service (Publisher)\n    \u2193\nSNS Topic: \"OrderEvents\"\n    \u2193\n\u251c\u2500\u2500 Email (Customer notification)\n\u251c\u2500\u2500 SQS Queue (Inventory service)\n\u251c\u2500\u2500 Lambda (Analytics logging)\n\u2514\u2500\u2500 HTTP Endpoint (External system)\n</code></pre>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#key-characteristics_1","title":"Key Characteristics","text":"<ul> <li>Fire and forget: No storage</li> <li>Push model: SNS delivers to subscribers</li> <li>Multiple protocols: Email, SMS, SQS, Lambda, HTTP</li> <li>Message filtering: Subscribers receive only what they want</li> <li>No ordering guarantees</li> </ul>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#when-to-use-sns","title":"When to Use SNS","text":"<p>\u2705 Event-driven architectures \u2705 Broadcasting notifications \u2705 Fanout to multiple services \u2705 Mobile push notifications \u2705 Email/SMS alerts</p> <p>\u274c Single consumer task queue \u274c Message persistence needed \u274c Stream processing \u274c Replay capability required</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#example-use-case_1","title":"Example Use Case","text":"<p>User Registration:</p> <pre><code>1. User registers\n2. API publishes to SNS: \"UserRegistered\" event\n3. SNS delivers to:\n   - Welcome email\n   - SQS queue (onboarding workflow)\n   - Lambda (analytics)\n   - Webhook (CRM system)\n4. All happen simultaneously and independently\n</code></pre>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#kinesis-deep-dive","title":"Kinesis Deep Dive","text":""},{"location":"aws/sqs-vs-sns-vs-kinesis/#what-it-does_2","title":"What It Does","text":"<p>Captures and processes high-volume streaming data in real-time.</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#architecture_2","title":"Architecture","text":"<pre><code>1000s of IoT Devices / Apps\n    \u2193\nKinesis Data Stream (3 shards)\n    \u2193\n\u251c\u2500\u2500 Lambda (Real-time processing)\n\u251c\u2500\u2500 Flink (Analytics)\n\u251c\u2500\u2500 Firehose (S3 archival)\n\u2514\u2500\u2500 Custom Consumer (Dashboard)\n</code></pre>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#key-characteristics_2","title":"Key Characteristics","text":"<ul> <li>Streaming data: Continuous flow</li> <li>Data persistence: 1-365 days retention</li> <li>Ordering per shard: Same partition key \u2192 same shard</li> <li>Multiple consumers: Each reads independently</li> <li>Replay capability: Reprocess historical data</li> <li>High throughput: MB/sec per shard</li> </ul>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#when-to-use-kinesis","title":"When to Use Kinesis","text":"<p>\u2705 Real-time analytics \u2705 Log aggregation \u2705 Clickstream analysis \u2705 IoT telemetry \u2705 Multiple consumers need same data \u2705 Data replay required</p> <p>\u274c Simple background tasks \u274c Low-volume events \u274c One-time message processing \u274c Broadcasting to different endpoints</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#example-use-case_2","title":"Example Use Case","text":"<p>Clickstream Analytics:</p> <pre><code>1. User clicks on website\n2. Event sent to Kinesis: {userId, action, timestamp}\n3. Data flows to multiple consumers:\n   - Lambda \u2192 DynamoDB (real-time personalization)\n   - Flink \u2192 Real-time fraud detection\n   - Firehose \u2192 S3 (data lake for ML)\n   - Analytics \u2192 Live dashboard\n4. All consumers process independently\n5. Data retained for 7 days (can replay if needed)\n</code></pre>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#decision-framework","title":"Decision Framework","text":""},{"location":"aws/sqs-vs-sns-vs-kinesis/#choose-by-use-case","title":"Choose by Use Case","text":"<pre><code>Need to queue tasks for workers?\n    \u2192 SQS\n\nNeed to notify multiple services?\n    \u2192 SNS\n\nNeed to process streaming data?\n    \u2192 Kinesis\n</code></pre>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#choose-by-requirements","title":"Choose by Requirements","text":"<p>Volume &amp; Velocity:</p> <pre><code>Low volume, async tasks \u2192 SQS\nHigh volume, real-time \u2192 Kinesis\nNotifications, any volume \u2192 SNS\n</code></pre> <p>Consumer Pattern:</p> <pre><code>One consumer per message \u2192 SQS\nMultiple different endpoints \u2192 SNS\nMultiple consumers, same data \u2192 Kinesis\n</code></pre> <p>Data Persistence:</p> <pre><code>Temporary (until processed) \u2192 SQS\nNo persistence needed \u2192 SNS\nLong-term (replay needed) \u2192 Kinesis\n</code></pre> <p>Ordering:</p> <pre><code>Strict ordering required \u2192 SQS FIFO or Kinesis\nNo ordering needed \u2192 SQS Standard or SNS\n</code></pre> <p>Cost Sensitivity:</p> <pre><code>Pay per request (variable) \u2192 SQS or SNS\nPredictable throughput \u2192 Kinesis (shard-based)\n</code></pre>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#common-patterns-and-combinations","title":"Common Patterns and Combinations","text":""},{"location":"aws/sqs-vs-sns-vs-kinesis/#pattern-1-sns-sqs-fanout","title":"Pattern 1: SNS + SQS Fanout","text":"<p>Best of both worlds:</p> <pre><code>Order Service\n    \u2193\nSNS Topic\n    \u2193\n\u251c\u2500\u2500 SQS Queue 1 \u2192 Inventory Worker\n\u251c\u2500\u2500 SQS Queue 2 \u2192 Shipping Worker\n\u251c\u2500\u2500 SQS Queue 3 \u2192 Analytics Worker\n\u2514\u2500\u2500 Email (Customer)\n</code></pre> <p>Benefits: * SNS broadcasts to multiple services * Each SQS queue buffers for its service * Independent retry logic per service * Decoupled processing</p> <p>Use when: * Multiple services need the same event * Each service needs reliable processing * Services scale independently</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#pattern-2-kinesis-lambda-dynamodb","title":"Pattern 2: Kinesis + Lambda + DynamoDB","text":"<p>Real-time stream processing:</p> <pre><code>Mobile Apps\n    \u2193\nKinesis Data Stream\n    \u2193\nLambda (processes batches)\n    \u2193\nDynamoDB (stores results)\n</code></pre> <p>Benefits: * Real-time processing * Automatic scaling * Multiple Lambdas can read same stream * Built-in retry logic</p> <p>Use when: * Real-time data processing needed * Serverless architecture * Event-driven workflows</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#pattern-3-kinesis-firehose-s3-athena","title":"Pattern 3: Kinesis + Firehose + S3 + Athena","text":"<p>Data lake pipeline:</p> <pre><code>Application Logs\n    \u2193\nKinesis Data Stream\n    \u2193\nFirehose (transforms &amp; batches)\n    \u2193\nS3 (Parquet format)\n    \u2193\nAthena (SQL queries)\n</code></pre> <p>Benefits: * Real-time ingestion * Automatic formatting * Cost-efficient storage * SQL analytics</p> <p>Use when: * Building data lake * Log analytics * Historical analysis</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#pattern-4-api-gateway-sqs-lambda","title":"Pattern 4: API Gateway + SQS + Lambda","text":"<p>Async API pattern:</p> <pre><code>Client API Request\n    \u2193\nAPI Gateway\n    \u2193\nSQS Queue\n    \u2193\nLambda Worker (async)\n    \u2193\nReturns 202 Accepted immediately\n</code></pre> <p>Benefits: * API responds immediately * Backend processes asynchronously * Handles traffic spikes * Built-in retry logic</p> <p>Use when: * Long-running operations * Variable processing time * High availability required</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#real-world-scenarios","title":"Real-World Scenarios","text":""},{"location":"aws/sqs-vs-sns-vs-kinesis/#scenario-1-e-commerce-order-processing","title":"Scenario 1: E-Commerce Order Processing","text":"<p>Requirements: * User places order * Multiple steps: inventory, payment, shipping, email * Each step should not block the other * Retry failed steps</p> <p>Solution: SNS + SQS Fanout</p> <pre><code>Order API\n    \u2193\nSNS Topic: \"OrderPlaced\"\n    \u2193\n\u251c\u2500\u2500 SQS: InventoryQueue \u2192 Worker checks inventory\n\u251c\u2500\u2500 SQS: PaymentQueue \u2192 Worker processes payment\n\u251c\u2500\u2500 SQS: ShippingQueue \u2192 Worker creates shipment\n\u2514\u2500\u2500 Email: Customer confirmation\n</code></pre> <p>Why: * SNS broadcasts order event * Each queue handles its domain independently * Failed steps retry without affecting others * Customer gets immediate confirmation</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#scenario-2-real-time-gaming-leaderboard","title":"Scenario 2: Real-Time Gaming Leaderboard","text":"<p>Requirements: * Millions of game events per second * Real-time leaderboard updates * Historical analysis for ML * Multiple consumers need same data</p> <p>Solution: Kinesis Data Streams</p> <pre><code>Game Clients (millions)\n    \u2193\nKinesis Stream (100 shards)\n    \u2193\n\u251c\u2500\u2500 Lambda \u2192 DynamoDB (real-time leaderboard)\n\u251c\u2500\u2500 Flink \u2192 Detect cheating patterns\n\u2514\u2500\u2500 Firehose \u2192 S3 (ML training data)\n</code></pre> <p>Why: * High throughput (100 MB/sec) * Multiple consumers read independently * Data retained for replay * Real-time processing</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#scenario-3-microservices-communication","title":"Scenario 3: Microservices Communication","text":"<p>Requirements: * 20 microservices * Each service needs to notify others of events * Services should be loosely coupled * Easy to add new services</p> <p>Solution: SNS per Service + SQS per Subscriber</p> <pre><code>Service A\n    \u2193\nSNS: \"ServiceA.Events\"\n    \u2193\n\u251c\u2500\u2500 SQS: ServiceB.Queue\n\u251c\u2500\u2500 SQS: ServiceC.Queue\n\u2514\u2500\u2500 SQS: ServiceD.Queue\n</code></pre> <p>Why: * Each service publishes to its topic * Services subscribe only to events they care * Add new services without changing existing ones * Decoupled architecture</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#scenario-4-log-aggregation-pipeline","title":"Scenario 4: Log Aggregation Pipeline","text":"<p>Requirements: * Collect logs from 100 servers * Process and enrich logs * Store for compliance (1 year) * Query with SQL</p> <p>Solution: Kinesis + Firehose + S3 + Athena</p> <pre><code>100 Servers\n    \u2193\nKinesis Agent\n    \u2193\nKinesis Data Stream\n    \u2193\nFirehose (Lambda transform)\n    \u2193\nS3 (Parquet)\n    \u2193\nAthena (queries)\n</code></pre> <p>Why: * High-volume log ingestion * Transform with Lambda * Cost-efficient Parquet storage * SQL queries with Athena * 1-year retention in Kinesis</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#scenario-5-image-processing-pipeline","title":"Scenario 5: Image Processing Pipeline","text":"<p>Requirements: * Users upload images * Generate thumbnails (slow) * User should get immediate response * Retry failed processing</p> <p>Solution: SQS Standard Queue</p> <pre><code>User Upload \u2192 API\n    \u2193\nS3 (original image)\n    \u2193\nAPI \u2192 SQS: \"Process image-123.jpg\"\n    \u2193\nAPI \u2192 User: \"Upload successful\" (202 Accepted)\n\nWorker Pool (10 instances)\n    \u2193\nPoll SQS \u2192 Get message\n    \u2193\nDownload from S3 \u2192 Process \u2192 Upload thumbnail\n    \u2193\nDelete message from SQS\n</code></pre> <p>Why: * User gets immediate response * Workers process in background * Automatic retry (visibility timeout) * Scales workers independently * DLQ for poison messages</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#cost-comparison","title":"Cost Comparison","text":""},{"location":"aws/sqs-vs-sns-vs-kinesis/#sqs-pricing","title":"SQS Pricing","text":"<p>Standard Queue: * $0.40 per million requests * First 1M requests/month free</p> <p>Example: * 10M messages/month = $4.00</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#sns-pricing","title":"SNS Pricing","text":"<p>Publish: * $0.50 per million requests * First 1M requests/month free</p> <p>Delivery: * Email: $2.00 per 100,000 notifications * SQS: Free * Lambda: Free * HTTP: $0.60 per million notifications</p> <p>Example: * 10M publishes + 30M SQS deliveries = $5.00</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#kinesis-data-streams-pricing","title":"Kinesis Data Streams Pricing","text":"<p>Provisioned Mode: * $0.015 per shard-hour * $0.014 per million PUT requests</p> <p>Example: * 3 shards \u00d7 24 hours \u00d7 30 days = $32.40/month * 10M PUT requests = $0.14 * Total: $32.54/month</p> <p>On-Demand Mode: * $0.04 per GB ingested * $0.015 per GB retrieved</p> <p>Example: * 100 GB ingested = $4.00 * 200 GB retrieved = $3.00 * Total: $7.00/month</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#cost-comparison-summary","title":"Cost Comparison Summary","text":"<pre><code>Low Volume (1M events/month):\n    SQS: ~$0.40\n    SNS: ~$0.50\n    Kinesis: ~$32 (1 shard minimum)\n\nMedium Volume (100M events/month):\n    SQS: ~$40\n    SNS: ~$50\n    Kinesis: ~$100 (10 shards)\n\nHigh Volume (1B events/month):\n    SQS: ~$400\n    SNS: ~$500\n    Kinesis: ~$500 (100 shards)\n</code></pre> <p>Cost Insight: * SQS/SNS: Better for low-medium volume * Kinesis: Better for high volume with multiple consumers * Kinesis: Fixed cost (shard-based) vs variable (request-based)</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#when-to-use-what","title":"When to Use What","text":""},{"location":"aws/sqs-vs-sns-vs-kinesis/#use-sqs-when","title":"Use SQS When","text":"<p>\u2705 You need a task queue for background jobs \u2705 One consumer should process each message \u2705 You need retry logic with visibility timeout \u2705 Messages should be deleted after processing \u2705 Decoupling services is the goal \u2705 Ordering is critical (use FIFO) \u2705 Low to medium volume of messages</p> <p>Examples: * Email sending queue * Image processing * Order fulfillment * Payment processing * Report generation</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#use-sns-when","title":"Use SNS When","text":"<p>\u2705 You need to broadcast events to multiple subscribers \u2705 Different destinations need the same event (email, SQS, Lambda, HTTP) \u2705 You need push notifications (mobile, email, SMS) \u2705 Real-time delivery is important \u2705 You want event-driven architecture \u2705 No storage needed (fire and forget)</p> <p>Examples: * User notifications * System alerts * Microservices communication * Mobile push notifications * Webhooks to external systems</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#use-kinesis-when","title":"Use Kinesis When","text":"<p>\u2705 You're processing streaming data (continuous flow) \u2705 High throughput required (MB/sec) \u2705 Multiple consumers need the same data independently \u2705 You need to replay data (historical reprocessing) \u2705 Real-time analytics is required \u2705 Ordered processing per partition key \u2705 Data needs to be persisted (1-365 days)</p> <p>Examples: * Clickstream analytics * IoT telemetry * Log aggregation * Real-time dashboards * Fraud detection * Gaming leaderboards</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#combine-when","title":"Combine When","text":"<p>SNS + SQS: * Fanout pattern with reliable delivery * Multiple services processing same event</p> <p>Kinesis + Lambda: * Real-time stream processing * Serverless architecture</p> <p>Kinesis + Firehose: * Stream to data lake * Analytics pipeline</p> <p>API Gateway + SQS: * Async API responses * Long-running operations</p>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#summary","title":"Summary","text":""},{"location":"aws/sqs-vs-sns-vs-kinesis/#quick-decision-tree","title":"Quick Decision Tree","text":"<pre><code>Is it streaming data (continuous flow)?\n\u251c\u2500 Yes \u2192 Kinesis\n\u2514\u2500 No \u2193\n\nDo multiple different endpoints need it?\n\u251c\u2500 Yes \u2192 SNS\n\u2514\u2500 No \u2193\n\nIs it a task that one worker should process?\n\u2514\u2500 Yes \u2192 SQS\n</code></pre>"},{"location":"aws/sqs-vs-sns-vs-kinesis/#key-takeaways","title":"Key Takeaways","text":"<p>SQS: Queue for one-to-one async task processing SNS: Pub/Sub for one-to-many event broadcasting Kinesis: Stream for many-to-many real-time data processing</p> <p>Remember: * SQS = Task Queue (Pull) * SNS = Event Bus (Push) * Kinesis = Data Stream (Pull, with persistence)</p> <p>Most Common Pattern: SNS + SQS Fanout Most Scalable: Kinesis for streaming workloads Most Cost-Effective (low volume): SQS/SNS</p>"},{"location":"caddy/ssl-renewal/","title":"Troubleshooting Caddy: A Guide to Running and Renewing SSL","text":"<p>This guide outlines a methodical approach to diagnosing and resolving common issues with Caddy, including startup failures and problems with SSL certificate renewal. The process involves systematically checking Caddy's status, configuration, and permissions.</p>"},{"location":"caddy/ssl-renewal/#1-initial-caddy-status-check","title":"1. Initial Caddy Status Check","text":"<p>Start by verifying Caddy's operational status and location.</p> <ul> <li>Check the Caddy process: <code>bash     which caddy</code>     This command shows the path to the Caddy executable.</li> <li>Get Caddy's version: <code>bash     caddy version</code>     This helps in identifying potential version-specific issues.</li> <li>Find Caddy-related files: <code>bash     sudo find /etc -name \u201c*caddy*\u201d -o -name \u201cCaddyfile\u201d 2&gt;/dev/null</code>     This command helps locate Caddy's configuration and related files.</li> <li>View the Caddyfile: <code>bash     sudo cat /etc/caddy/Caddyfile</code>     Review the Caddyfile for any syntax errors or misconfigurations.</li> </ul>"},{"location":"caddy/ssl-renewal/#2-diagnosing-ssl-and-directory-issues","title":"2. Diagnosing SSL and Directory Issues","text":"<p>SSL certificate renewal and Caddy's ability to write to its data directory are frequent points of failure.</p> <ul> <li>List certificate files: <code>bash     sudo ls -la /var/lib/caddy/.local/share/caddy/certificates/</code>     Check if certificates exist and if their permissions are correct.</li> <li>Find certificate and ACME files: <code>bash     sudo find /var/lib/caddy -name \u201c*certificate*\u201d -o -name \u201c*acme*\u201d 2&gt;/dev/null</code>     This helps locate all related SSL and ACME challenge files.</li> <li>List Caddy's data directory contents: <code>bash     sudo ls -la /var/lib/caddy</code>     Verify the permissions and ownership of the Caddy data directory.</li> </ul>"},{"location":"caddy/ssl-renewal/#3-reviewing-logs-and-validation","title":"3. Reviewing Logs and Validation","text":"<p>Logs are crucial for understanding why Caddy might be failing.</p> <ul> <li>Check Caddy service logs: <code>bash     sudo journalctl -u caddy -no-page -n 50</code>     Examine the last 50 lines of the Caddy service journal for error messages.</li> <li>Validate the Caddyfile: <code>bash     sudo caddy validate --config /etc/caddy/Caddyfile</code>     A successful validation confirms that the Caddyfile syntax is correct.</li> </ul>"},{"location":"caddy/ssl-renewal/#4-remediation-steps","title":"4. Remediation Steps","text":"<p>If errors were found, follow these steps to fix common issues.</p> <ul> <li>Restart the Caddy service: <code>bash     sudo systemctl restart caddy</code>     A simple restart can resolve temporary glitches.</li> <li>View the Caddy service unit file: <code>bash     sudo cat /etc/systemd/system/caddy.service</code>     This file defines how Caddy runs and under which user.</li> <li>Test Caddy with its configuration: <code>bash     sudo /usr/local/bin/caddy run --environ --config /etc/caddy/Caddyfile</code>     This command runs Caddy in the foreground, providing real-time output that can reveal subtle errors.</li> <li>Clear old certificate data: <code>bash     sudo rm -rf /root/.local/share/caddy/certificates/     sudo rm -rf /root/.local/share/caddy/locks/</code>     Sometimes, corrupted or outdated certificates can prevent renewal. Deleting them forces Caddy to request new ones.</li> <li>Format the Caddyfile: <code>bash     sudo caddy fmt --overwrite /etc/caddy/Caddyfile</code>     This command standardizes the formatting of the Caddyfile, which can help in spotting syntax issues.</li> </ul>"},{"location":"caddy/ssl-renewal/#5-ensuring-proper-user-and-permissions","title":"5. Ensuring Proper User and Permissions","text":"<p>Incorrect user permissions are a very common cause of Caddy failures. The best practice is to run Caddy as a dedicated, unprivileged system user.</p> <ul> <li>Add a dedicated system user for Caddy: <code>bash     sudo user add --system --home /var/lib/caddy --create-home --shell /usr/sbin/nologin caddy</code></li> <li>Set proper ownership: <code>bash     sudo chown -R caddy:caddy /var/lib/caddy</code>     This ensures Caddy can read and write to its data directory.</li> </ul>"},{"location":"caddy/ssl-renewal/#6-fixing-the-systemd-service-file","title":"6. Fixing the Systemd Service File","text":"<p>The systemd service file dictates Caddy's behavior. An incorrect configuration can cause it to fail silently.</p> <ul> <li> <p>Create or correct the systemd service file:     ```bash     sudo tee /etc/systemd/system/caddy.service &gt; /dev/null &lt;&lt; 'EOF'     [Unit]     Description=Caddy Web Server     Documentation=https://caddyserver.com/docs/     After=network.target network-online.target     Requires=network-online.target</p> <p>[Service] Type=notify User=caddy Group=caddy ExecStart=/usr/local/bin/caddy run --environ --config /etc/caddy/Caddyfile ExecReload=/usr/local/bin/caddy reload --config /etc/caddy/Caddyfile --force TimeoutStopSec=5s LimitNOFILE=1048576 LimitNPROC=1048576 PrivateTmp=true ProtectSystem=full AmbientCapabilities=CAP_NET_BIND_SERVICE NoNewPrivileges=true</p> <p>[Install] WantedBy=multi-user.target EOF <code>* **Reload systemd:**</code>bash sudo systemctl daemon-reload ``` This command makes systemd aware of the new or changed service file.</p> </li> </ul>"},{"location":"caddy/ssl-renewal/#7-final-verification-and-cleanup","title":"7. Final Verification and Cleanup","text":"<ul> <li>Check logs for final errors: <code>bash     sudo journalctl -xeu caddy.service --no-pager -n 20</code>     Review the most recent logs to confirm the service is starting without issues.</li> <li>Check listening ports: <code>bash     sudo lsof -i :2019</code>     This command verifies that Caddy is listening on its default management port.</li> <li>Kill any old Caddy processes: <code>bash     sudo pkill -f caddy</code>     This ensures that no conflicting processes are running.</li> <li>Start the Caddy service: <code>bash     sudo systemctl start caddy</code></li> <li>Test connectivity and SSL: <code>bash     sleep 5 &amp;&amp; curl -I [https://api.nexplace.jp](https://api.nexplace.jp)     echo | openssl s_client -servername api.nexplace.jp -connect api.nexplace.jp:443 2&gt;/dev/null | openssl x509 -noout -dates</code>     These commands confirm that the site is accessible and that the SSL certificate is valid and has been renewed.</li> <li>Enable Caddy to start on boot: <code>bash     sudo systemctl enable caddy</code>     This final step ensures that Caddy will automatically start whenever the system reboots.</li> </ul>"},{"location":"docker/docker_comprehensive_guide/","title":"Comprehensive Docker Guide - Commands and Concepts","text":"<p>A complete reference guide to Docker containerization technology, covering core concepts, CLI commands, Dockerfile instructions, and Docker Compose for modern application development and deployment.</p>"},{"location":"docker/docker_comprehensive_guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>What is Docker?</li> <li>Docker Core Concepts</li> <li>Docker CLI Commands</li> <li>Dockerfile Instructions</li> <li>Docker Compose</li> <li>Best Practices</li> <li>Common Workflows</li> </ol>"},{"location":"docker/docker_comprehensive_guide/#what-is-docker","title":"What is Docker?","text":"<p>Docker is a containerization platform that packages applications with all their dependencies into lightweight, portable containers. Think of Docker as a \"lunchbox\" for your application - it packages your code with everything it needs to run consistently across different environments.</p>"},{"location":"docker/docker_comprehensive_guide/#key-benefits","title":"Key Benefits","text":"<ul> <li>Consistency: Runs the same everywhere</li> <li>Isolation: Applications don't interfere with each other</li> <li>Portability: Move applications between environments easily</li> <li>Efficiency: Lightweight compared to virtual machines</li> <li>Version Control: Track and manage different versions</li> <li>Scalability: Easy to scale applications up or down</li> <li>DevOps Integration: Streamlines development and deployment</li> </ul>"},{"location":"docker/docker_comprehensive_guide/#docker-core-concepts","title":"Docker Core Concepts","text":""},{"location":"docker/docker_comprehensive_guide/#docker-image","title":"Docker Image","text":"<p>A lightweight, standalone, executable package that includes everything needed to run software: - Application code - Runtime environment - Libraries and dependencies - System tools - Operating system components</p> <p>Think of it as a recipe for your application.</p>"},{"location":"docker/docker_comprehensive_guide/#docker-container","title":"Docker Container","text":"<p>A runnable instance of a Docker image. It represents the execution environment where your application actually runs. Multiple containers can be created from a single image.</p>"},{"location":"docker/docker_comprehensive_guide/#docker-volume","title":"Docker Volume","text":"<p>A persistent data storage mechanism that: - Shares data between container and host machine - Enables data sharing among multiple containers - Ensures data survives container restarts or removal - Provides data durability and persistence</p>"},{"location":"docker/docker_comprehensive_guide/#docker-network","title":"Docker Network","text":"<p>A communication channel that enables: - Container-to-container communication - Container-to-external world connectivity - Network isolation and security - Custom networking configurations</p>"},{"location":"docker/docker_comprehensive_guide/#docker-client","title":"Docker Client","text":"<p>The user interface for interacting with Docker through: - Command line interface (CLI) - Graphical user interfaces - API calls</p>"},{"location":"docker/docker_comprehensive_guide/#docker-host-docker-daemon","title":"Docker Host (Docker Daemon)","text":"<p>The background process that manages: - Container lifecycle - Image building and storage - Network and volume management - API request handling</p>"},{"location":"docker/docker_comprehensive_guide/#docker-registry-docker-hub","title":"Docker Registry (Docker Hub)","text":"<p>A centralized repository for Docker images: - Public and private image storage - Image versioning and distribution - Docker Hub is to Docker what GitHub is to Git</p>"},{"location":"docker/docker_comprehensive_guide/#docker-cli-commands","title":"Docker CLI Commands","text":""},{"location":"docker/docker_comprehensive_guide/#image-management","title":"Image Management","text":""},{"location":"docker/docker_comprehensive_guide/#downloading-images","title":"Downloading Images","text":"<pre><code># Download image from Docker Hub\ndocker pull &lt;image_name&gt;\ndocker pull ubuntu\ndocker pull node:18-alpine\n\n# Pull specific version/tag\ndocker pull mysql:8.0\ndocker pull nginx:latest\n</code></pre> <p>Explanation: Downloads images from Docker Hub to your local machine. If no tag is specified, it pulls the <code>latest</code> tag by default.</p>"},{"location":"docker/docker_comprehensive_guide/#listing-images","title":"Listing Images","text":"<pre><code># List all local images\ndocker images\ndocker image ls\n\n# Show image details\ndocker image inspect &lt;image_name&gt;\n</code></pre> <p>Explanation: Displays all Docker images stored locally with details like repository, tag, image ID, creation date, and size.</p>"},{"location":"docker/docker_comprehensive_guide/#building-images","title":"Building Images","text":"<pre><code># Build image from Dockerfile\ndocker build -t &lt;tag_name&gt; .\ndocker build -t hello-docker .\ndocker build -t myapp:v1.0 .\n\n# Build with custom Dockerfile name\ndocker build -f Dockerfile.prod -t myapp:prod .\n\n# Build with build arguments\ndocker build --build-arg NODE_ENV=production -t myapp .\n</code></pre> <p>Explanation: Creates a Docker image from a Dockerfile. The <code>-t</code> flag assigns a name/tag to the image, and <code>.</code> indicates the build context (current directory).</p>"},{"location":"docker/docker_comprehensive_guide/#managing-images","title":"Managing Images","text":"<pre><code># Tag an image\ndocker tag &lt;source_image&gt; &lt;target_image&gt;\ndocker tag react-docker username/react-docker:latest\n\n# Remove image\ndocker rmi &lt;image_name&gt;\ndocker rmi ubuntu:latest\n\n# Remove unused images\ndocker image prune\n</code></pre>"},{"location":"docker/docker_comprehensive_guide/#container-operations","title":"Container Operations","text":""},{"location":"docker/docker_comprehensive_guide/#running-containers","title":"Running Containers","text":"<pre><code># Basic container run\ndocker run &lt;image_name&gt;\ndocker run hello-world\n\n# Run interactively with terminal\ndocker run -it &lt;image_name&gt;\ndocker run -it ubuntu bash\n\n# Run in detached mode (background)\ndocker run -d nginx\n\n# Run with custom name\ndocker run --name my-container nginx\n</code></pre>"},{"location":"docker/docker_comprehensive_guide/#port-mapping","title":"Port Mapping","text":"<pre><code># Map container port to host port\ndocker run -p &lt;host_port&gt;:&lt;container_port&gt; &lt;image_name&gt;\ndocker run -p 8080:80 nginx\ndocker run -p 5173:5173 react-app\n</code></pre> <p>Explanation: The <code>-p</code> flag maps a port from the container to the host machine, making the container's service accessible externally.</p>"},{"location":"docker/docker_comprehensive_guide/#volume-mounting","title":"Volume Mounting","text":"<pre><code># Mount host directory to container\ndocker run -p 5173:5173 -v $(pwd):/app -v /app/node_modules react-docker\n</code></pre> <p>Explanation:  - <code>-v $(pwd):/app</code>: Mounts current directory to <code>/app</code> in container (enables hot reloading) - <code>-v /app/node_modules</code>: Creates named volume for node_modules (preserves dependencies)</p>"},{"location":"docker/docker_comprehensive_guide/#container-management","title":"Container Management","text":"<pre><code># List running containers\ndocker ps\n\n# List all containers (including stopped)\ndocker ps -a\n\n# Stop container\ndocker stop &lt;container_id_or_name&gt;\ndocker stop c3d  # Using first few characters of ID\n\n# Start stopped container\ndocker start &lt;container_id_or_name&gt;\n\n# Restart container\ndocker restart &lt;container_id_or_name&gt;\n\n# Remove container\ndocker rm &lt;container_id_or_name&gt;\ndocker rm --force &lt;container_id_or_name&gt;  # Force remove running container\n\n# Remove all stopped containers\ndocker container prune\n</code></pre>"},{"location":"docker/docker_comprehensive_guide/#registry-operations","title":"Registry Operations","text":"<pre><code># Login to Docker Hub\ndocker login\n\n# Push image to registry\ndocker push &lt;image_name&gt;\ndocker push username/myapp:latest\n\n# Logout from registry\ndocker logout\n</code></pre>"},{"location":"docker/docker_comprehensive_guide/#dockerfile-instructions","title":"Dockerfile Instructions","text":"<p>A Dockerfile is a text file containing instructions to build a Docker image. Each instruction creates a new layer in the image.</p>"},{"location":"docker/docker_comprehensive_guide/#essential-instructions","title":"Essential Instructions","text":""},{"location":"docker/docker_comprehensive_guide/#from-base-image","title":"FROM - Base Image","text":"<pre><code>FROM &lt;base_image&gt;:&lt;tag&gt;\nFROM node:20-alpine\nFROM ubuntu:22.04\nFROM python:3.9-slim\n</code></pre> <p>Explanation: Specifies the base image. Must be the first instruction in every Dockerfile.</p>"},{"location":"docker/docker_comprehensive_guide/#workdir-working-directory","title":"WORKDIR - Working Directory","text":"<pre><code>WORKDIR /app\nWORKDIR /usr/src/app\n</code></pre> <p>Explanation: Sets the working directory for subsequent instructions (RUN, CMD, ENTRYPOINT, COPY, ADD).</p>"},{"location":"docker/docker_comprehensive_guide/#copy-copy-files","title":"COPY - Copy Files","text":"<pre><code>COPY &lt;source&gt; &lt;destination&gt;\nCOPY . .                    # Copy all files to current WORKDIR\nCOPY package.json ./        # Copy specific file\nCOPY src/ ./src/           # Copy directory\n</code></pre> <p>Explanation: Copies files/directories from build context (local machine) to the image.</p>"},{"location":"docker/docker_comprehensive_guide/#run-execute-commands","title":"RUN - Execute Commands","text":"<pre><code>RUN &lt;command&gt;\nRUN npm install\nRUN apt-get update &amp;&amp; apt-get install -y curl\nRUN pip install -r requirements.txt\n</code></pre> <p>Explanation: Executes commands during image build, creating new layers. Use for installing dependencies and setting up the environment.</p>"},{"location":"docker/docker_comprehensive_guide/#expose-port-declaration","title":"EXPOSE - Port Declaration","text":"<pre><code>EXPOSE &lt;port&gt;\nEXPOSE 3000\nEXPOSE 80 443\n</code></pre> <p>Explanation: Documents which ports the container listens on. Informational only - doesn't actually publish ports.</p>"},{"location":"docker/docker_comprehensive_guide/#env-environment-variables","title":"ENV - Environment Variables","text":"<pre><code>ENV &lt;key&gt;=&lt;value&gt;\nENV NODE_ENV=production\nENV API_URL=https://api.example.com\nENV PORT=3000\n</code></pre> <p>Explanation: Sets environment variables available during build and runtime.</p>"},{"location":"docker/docker_comprehensive_guide/#arg-build-arguments","title":"ARG - Build Arguments","text":"<pre><code>ARG &lt;variable_name&gt;\nARG NODE_VERSION=18\nARG BUILD_ENV\n</code></pre> <p>Explanation: Defines build-time variables passed with <code>--build-arg</code> flag during build.</p>"},{"location":"docker/docker_comprehensive_guide/#volume-mount-points","title":"VOLUME - Mount Points","text":"<pre><code>VOLUME &lt;mount_point&gt;\nVOLUME /app/data\nVOLUME [\"/var/log\", \"/var/db\"]\n</code></pre> <p>Explanation: Creates mount point for external volumes, specifying persistent data locations.</p>"},{"location":"docker/docker_comprehensive_guide/#cmd-default-command","title":"CMD - Default Command","text":"<pre><code>CMD [\"executable\", \"param1\", \"param2\"]\nCMD [\"node\", \"server.js\"]\nCMD [\"npm\", \"start\"]\n</code></pre> <p>Explanation: Provides default command when container starts. Can be overridden when running the container.</p>"},{"location":"docker/docker_comprehensive_guide/#entrypoint-main-executable","title":"ENTRYPOINT - Main Executable","text":"<pre><code>ENTRYPOINT [\"executable\", \"param1\"]\nENTRYPOINT [\"node\", \"app.js\"]\n</code></pre> <p>Explanation: Sets the main command that cannot be easily overridden. Used with CMD for flexible argument passing.</p>"},{"location":"docker/docker_comprehensive_guide/#dockerignore-file","title":".dockerignore File","text":"<pre><code>node_modules\nnpm-debug.log\n.git\n.gitignore\nREADME.md\n.env\n.nyc_output\ncoverage\n.DS_Store\n</code></pre> <p>Explanation: Similar to <code>.gitignore</code>, excludes files/directories from build context, reducing image size and build time.</p>"},{"location":"docker/docker_comprehensive_guide/#docker-compose","title":"Docker Compose","text":"<p>Docker Compose manages multi-container applications using a YAML configuration file.</p>"},{"location":"docker/docker_comprehensive_guide/#docker-compose-commands","title":"Docker Compose Commands","text":""},{"location":"docker/docker_comprehensive_guide/#initialization","title":"Initialization","text":"<pre><code># Initialize new application\ndocker init\n</code></pre> <p>Explanation: CLI tool that generates Dockerfile, .dockerignore, and compose.yaml files based on your project.</p>"},{"location":"docker/docker_comprehensive_guide/#application-management","title":"Application Management","text":"<pre><code># Start all services\ndocker compose up\n\n# Start in detached mode with rebuild\ndocker compose up -d --build\n\n# Stop all services\ndocker compose down\n\n# Stop and remove volumes\ndocker compose down -v\n\n# View running services\ndocker compose ps\n\n# View logs\ndocker compose logs\ndocker compose logs &lt;service_name&gt;\n\n# Scale services\ndocker compose up --scale web=3\n</code></pre>"},{"location":"docker/docker_comprehensive_guide/#composeyaml-structure","title":"compose.yaml Structure","text":"<pre><code>version: '3.8'\nservices:\n  web:\n    build: .\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - .:/app\n      - /app/node_modules\n    environment:\n      - NODE_ENV=development\n    depends_on:\n      - database\n\n  database:\n    image: postgres:13\n    environment:\n      - POSTGRES_DB=myapp\n      - POSTGRES_USER=user\n      - POSTGRES_PASSWORD=password\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\nvolumes:\n  postgres_data:\n</code></pre>"},{"location":"docker/docker_comprehensive_guide/#best-practices","title":"Best Practices","text":""},{"location":"docker/docker_comprehensive_guide/#dockerfile-optimization","title":"Dockerfile Optimization","text":"<pre><code># Use specific tags instead of 'latest'\nFROM node:18-alpine\n\n# Use multi-stage builds\nFROM node:18-alpine AS builder\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\n\nFROM node:18-alpine AS runtime\nWORKDIR /app\nCOPY --from=builder /app/node_modules ./node_modules\nCOPY . .\nEXPOSE 3000\nCMD [\"npm\", \"start\"]\n</code></pre>"},{"location":"docker/docker_comprehensive_guide/#security-best-practices","title":"Security Best Practices","text":"<pre><code># Create non-root user\nRUN addgroup -g 1001 -S nodejs\nRUN adduser -S nextjs -u 1001\n\n# Use non-root user\nUSER nextjs\n\n# Minimize attack surface\nRUN rm -rf /var/cache/apk/*\n</code></pre>"},{"location":"docker/docker_comprehensive_guide/#image-size-optimization","title":"Image Size Optimization","text":"<pre><code># Use alpine images\nFROM node:18-alpine\n\n# Combine RUN commands\nRUN apk add --no-cache git \\\n    &amp;&amp; npm install \\\n    &amp;&amp; apk del git\n\n# Use .dockerignore effectively\n</code></pre>"},{"location":"docker/docker_comprehensive_guide/#common-workflows","title":"Common Workflows","text":""},{"location":"docker/docker_comprehensive_guide/#development-workflow","title":"Development Workflow","text":"<pre><code># 1. Build image\ndocker build -t myapp:dev .\n\n# 2. Run with volume mounting for hot reload\ndocker run -p 3000:3000 -v $(pwd):/app -v /app/node_modules myapp:dev\n\n# 3. Use Docker Compose for full stack\ndocker compose up -d --build\n</code></pre>"},{"location":"docker/docker_comprehensive_guide/#production-workflow","title":"Production Workflow","text":"<pre><code># 1. Build production image\ndocker build -t myapp:prod .\n\n# 2. Tag for registry\ndocker tag myapp:prod username/myapp:latest\n\n# 3. Push to registry\ndocker push username/myapp:latest\n\n# 4. Deploy\ndocker run -d -p 80:3000 --name myapp-prod username/myapp:latest\n</code></pre>"},{"location":"docker/docker_comprehensive_guide/#debugging-workflow","title":"Debugging Workflow","text":"<pre><code># View container logs\ndocker logs &lt;container_name&gt;\n\n# Execute commands in running container\ndocker exec -it &lt;container_name&gt; bash\n\n# Inspect container details\ndocker inspect &lt;container_name&gt;\n\n# Check resource usage\ndocker stats &lt;container_name&gt;\n</code></pre>"},{"location":"docker/docker_comprehensive_guide/#quick-reference","title":"Quick Reference","text":""},{"location":"docker/docker_comprehensive_guide/#most-used-commands","title":"Most Used Commands","text":"Command Description <code>docker build -t name .</code> Build image from Dockerfile <code>docker run -p 8080:80 image</code> Run container with port mapping <code>docker ps</code> List running containers <code>docker ps -a</code> List all containers <code>docker stop container</code> Stop running container <code>docker rm container</code> Remove stopped container <code>docker images</code> List local images <code>docker rmi image</code> Remove image <code>docker compose up -d</code> Start services in background <code>docker compose down</code> Stop and remove services"},{"location":"docker/docker_comprehensive_guide/#essential-dockerfile-instructions","title":"Essential Dockerfile Instructions","text":"Instruction Purpose <code>FROM</code> Base image <code>WORKDIR</code> Set working directory <code>COPY</code> Copy files to image <code>RUN</code> Execute commands during build <code>EXPOSE</code> Document port usage <code>ENV</code> Set environment variables <code>CMD</code> Default startup command <p>Docker revolutionizes application deployment by providing consistency, isolation, and portability across all environments. Master these commands and concepts to efficiently containerize and manage your applications!</p>"},{"location":"docker/docker_process_guide/","title":"Docker Process Guide","text":"<p>This guide covers the complete Docker workflow from building Spring Boot applications to managing PostgreSQL databases with Docker.</p>"},{"location":"docker/docker_process_guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Spring Boot Docker Image Creation</li> <li>Essential Docker Commands</li> <li>PostgreSQL Database Setup</li> <li>Docker Network Configuration</li> <li>PostgreSQL Service Management</li> </ol>"},{"location":"docker/docker_process_guide/#spring-boot-docker-image-creation","title":"Spring Boot Docker Image Creation","text":""},{"location":"docker/docker_process_guide/#building-the-docker-image","title":"Building the Docker Image","text":"<pre><code>./mvnw spring-boot:build-image \"-Dspring-boot.build-image.imageName=orandian2511/jobappimage\"\n</code></pre> <p>Explanation: This command uses Maven Wrapper to build a Docker image for your Spring Boot application. The <code>-Dspring-boot.build-image.imageName</code> parameter specifies the image name and tag. Spring Boot's build-image goal uses Cloud Native Buildpacks to create an optimized Docker image.</p>"},{"location":"docker/docker_process_guide/#pushing-to-docker-hub","title":"Pushing to Docker Hub","text":"<pre><code>docker push orandian2511/jobappimage\n</code></pre> <p>Explanation: Uploads your locally built Docker image to Docker Hub registry under the username <code>orandian2511</code>. This makes the image available for deployment on any system with Docker.</p>"},{"location":"docker/docker_process_guide/#running-the-application","title":"Running the Application","text":"<pre><code># Run in foreground (interactive mode)\ndocker run -p 8080:8080 orandian2511/jobappimage\n\n# Run in background (detached mode)\ndocker run -d -p 8080:8080 orandian2511/jobappimage\n</code></pre> <p>Explanation:  - <code>-p 8080:8080</code>: Maps port 8080 on your host machine to port 8080 inside the container - <code>-d</code>: Runs the container in detached mode (background)</p>"},{"location":"docker/docker_process_guide/#monitoring-the-application","title":"Monitoring the Application","text":"<pre><code># List running containers\ndocker ps\n\n# View container logs\ndocker logs a6c0aa9393dc\n</code></pre> <p>Explanation: <code>docker ps</code> shows active containers, while <code>docker logs</code> displays the output from a specific container using its ID.</p>"},{"location":"docker/docker_process_guide/#essential-docker-commands","title":"Essential Docker Commands","text":""},{"location":"docker/docker_process_guide/#image-management","title":"Image Management","text":"<pre><code># Pull an image from registry\ndocker pull &lt;image&gt;\n\n# Push an image to registry\ndocker push &lt;username/image&gt;\n\n# Build an image from Dockerfile\ndocker build -t &lt;username/image&gt; .\n\n# List all images\ndocker images\n\n# Remove an image\ndocker rmi &lt;image-id/image-name&gt;\n</code></pre>"},{"location":"docker/docker_process_guide/#container-management","title":"Container Management","text":"<pre><code># Run a container\ndocker run -it -d -p &lt;host-port&gt;:&lt;container-port&gt; --name &lt;name&gt; &lt;image&gt;\n\n# Stop a running container\ndocker stop &lt;container-id/container-name&gt;\n\n# Start a stopped container\ndocker start &lt;container-id/container-name&gt;\n\n# Remove a container\ndocker rm &lt;container-id/container-name&gt;\n\n# List running containers\ndocker ps\n\n# List all containers (including stopped)\ndocker ps -a\n</code></pre>"},{"location":"docker/docker_process_guide/#container-interaction","title":"Container Interaction","text":"<pre><code># Execute commands inside a running container\ndocker exec -it &lt;container-name/container-id&gt; bash\n\n# View container logs\ndocker logs &lt;container-name/container-id&gt;\n\n# Inspect container configuration\ndocker inspect &lt;container-name/container-id&gt;\n</code></pre> <p>Key Flags Explained: - <code>-it</code>: Interactive terminal mode - <code>-d</code>: Detached mode (runs in background) - <code>-p</code>: Port mapping - <code>--name</code>: Assigns a custom name to the container</p>"},{"location":"docker/docker_process_guide/#postgresql-database-setup","title":"PostgreSQL Database Setup","text":""},{"location":"docker/docker_process_guide/#basic-postgresql-container","title":"Basic PostgreSQL Container","text":"<pre><code># Run PostgreSQL database\ndocker run -d --name db -e POSTGRES_PASSWORD=postgres postgres\n</code></pre> <p>Explanation: Creates a PostgreSQL container named <code>db</code> with the default password set to <code>postgres</code>. The <code>-e</code> flag sets environment variables.</p>"},{"location":"docker/docker_process_guide/#pgadmin-web-interface","title":"PgAdmin Web Interface","text":"<pre><code># Run PgAdmin for database management\ndocker run -d --name pgadmin \\\n  -e PGADMIN_DEFAULT_EMAIL=admin@gmail.com \\\n  -e PGADMIN_DEFAULT_PASSWORD=admin \\\n  dpage/pgadmin4\n</code></pre> <p>Explanation: Starts PgAdmin4 web interface for PostgreSQL management. You can access it via a web browser once the container is running.</p>"},{"location":"docker/docker_process_guide/#testing-container-communication","title":"Testing Container Communication","text":"<pre><code># Test network connectivity between containers\ndocker exec -it pgadmin ping db\n</code></pre> <p>Explanation: This command attempts to ping the <code>db</code> container from inside the <code>pgadmin</code> container. Initially, this may fail because containers aren't on the same network.</p>"},{"location":"docker/docker_process_guide/#cleanup","title":"Cleanup","text":"<pre><code># Force remove both containers\ndocker rm -f db pgadmin\n</code></pre> <p>Explanation: The <code>-f</code> flag forcefully stops and removes running containers.</p>"},{"location":"docker/docker_process_guide/#docker-network-configuration","title":"Docker Network Configuration","text":""},{"location":"docker/docker_process_guide/#creating-a-custom-network","title":"Creating a Custom Network","text":"<pre><code># Create a user-defined bridge network\ndocker network create my-network\n</code></pre> <p>Explanation: Creates a custom network that allows containers to communicate using container names as hostnames.</p>"},{"location":"docker/docker_process_guide/#running-containers-on-custom-network","title":"Running Containers on Custom Network","text":"<pre><code># Run PostgreSQL on custom network\ndocker run -d --name db --network my-network \\\n  -e POSTGRES_PASSWORD=postgres postgres\n\n# Run PgAdmin on the same network\ndocker run -d --name pgadmin --network my-network \\\n  -e PGADMIN_DEFAULT_EMAIL=user@domain.com \\\n  -e PGADMIN_DEFAULT_PASSWORD=SuperSecret \\\n  dpage/pgadmin4\n</code></pre> <p>Explanation: Both containers are now on the same custom network, enabling them to communicate using container names (<code>db</code>, <code>pgadmin</code>) as hostnames.</p>"},{"location":"docker/docker_process_guide/#verify-network-connectivity","title":"Verify Network Connectivity","text":"<pre><code># Test connectivity between networked containers\ndocker exec -it pgadmin ping db\n</code></pre> <p>Explanation: This should now work successfully since both containers are on the same network.</p>"},{"location":"docker/docker_process_guide/#postgresql-service-management","title":"PostgreSQL Service Management","text":""},{"location":"docker/docker_process_guide/#stopping-local-postgresql-services","title":"Stopping Local PostgreSQL Services","text":"<pre><code># Stop PostgreSQL service (macOS with Homebrew)\nsudo brew services stop postgresql\n\n# Kill any running PostgreSQL processes\nsudo pkill -f postgres\n</code></pre> <p>Explanation: These commands stop local PostgreSQL installations that might conflict with Docker containers. This is useful when you want to use PostgreSQL in Docker instead of a local installation.</p>"},{"location":"docker/docker_process_guide/#best-practices","title":"Best Practices","text":"<ol> <li>Use specific tags: Instead of <code>latest</code>, use version tags for production</li> <li>Environment variables: Store sensitive data in environment variables, not in the image</li> <li>Volume mapping: Use volumes for persistent data storage</li> <li>Resource limits: Set memory and CPU limits for containers</li> <li>Health checks: Implement health checks for better monitoring</li> <li>Multi-stage builds: Use multi-stage Dockerfiles for smaller production images</li> </ol>"},{"location":"docker/docker_process_guide/#troubleshooting-tips","title":"Troubleshooting Tips","text":"<ul> <li>Use <code>docker logs &lt;container-name&gt;</code> to debug container issues</li> <li>Check port conflicts with <code>netstat -tlnp | grep :8080</code></li> <li>Verify network connectivity with <code>docker network ls</code> and <code>docker network inspect</code></li> <li>Clean up unused resources with <code>docker system prune</code></li> </ul>"},{"location":"docker/essential_docker_commands/","title":"Essential Docker Commands Reference","text":"<p>A comprehensive guide to the most important Docker commands every developer should know.</p>"},{"location":"docker/essential_docker_commands/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Docker Image Commands</li> <li>Docker Container Commands</li> <li>Docker Network Commands</li> <li>Docker Volume Commands</li> <li>Docker System Commands</li> <li>Docker Compose Commands</li> <li>Common Flags and Options</li> <li>Practical Examples</li> </ol>"},{"location":"docker/essential_docker_commands/#docker-image-commands","title":"Docker Image Commands","text":""},{"location":"docker/essential_docker_commands/#building-images","title":"Building Images","text":"<pre><code># Build image from Dockerfile in current directory\ndocker build -t myapp:latest .\n\n# Build with custom Dockerfile name\ndocker build -f Dockerfile.prod -t myapp:prod .\n\n# Build with build arguments\ndocker build --build-arg ENV=production -t myapp:prod .\n</code></pre>"},{"location":"docker/essential_docker_commands/#managing-images","title":"Managing Images","text":"<pre><code># List all images\ndocker images\ndocker image ls\n\n# Pull image from registry\ndocker pull nginx:latest\ndocker pull mysql:8.0\n\n# Push image to registry\ndocker push username/myapp:latest\n\n# Remove image\ndocker rmi image_id\ndocker rmi nginx:latest\n\n# Remove unused images\ndocker image prune\n\n# Remove all images\ndocker rmi $(docker images -q)\n</code></pre>"},{"location":"docker/essential_docker_commands/#image-information","title":"Image Information","text":"<pre><code># Show image details\ndocker inspect nginx:latest\n\n# Show image history/layers\ndocker history nginx:latest\n\n# Search images on Docker Hub\ndocker search mysql\n</code></pre>"},{"location":"docker/essential_docker_commands/#docker-container-commands","title":"Docker Container Commands","text":""},{"location":"docker/essential_docker_commands/#running-containers","title":"Running Containers","text":"<pre><code># Basic container run\ndocker run nginx\n\n# Run in detached mode (background)\ndocker run -d nginx\n\n# Run with custom name\ndocker run --name webserver nginx\n\n# Run with port mapping\ndocker run -p 8080:80 nginx\n\n# Run with environment variables\ndocker run -e MYSQL_ROOT_PASSWORD=secret mysql\n\n# Run with volume mount\ndocker run -v /host/path:/container/path nginx\n\n# Interactive container with terminal\ndocker run -it ubuntu bash\n\n# Run and remove container when it stops\ndocker run --rm nginx\n</code></pre>"},{"location":"docker/essential_docker_commands/#container-lifecycle","title":"Container Lifecycle","text":"<pre><code># List running containers\ndocker ps\n\n# List all containers (including stopped)\ndocker ps -a\n\n# Start stopped container\ndocker start container_name\n\n# Stop running container\ndocker stop container_name\n\n# Restart container\ndocker restart container_name\n\n# Pause container\ndocker pause container_name\n\n# Unpause container\ndocker unpause container_name\n\n# Kill container (force stop)\ndocker kill container_name\n</code></pre>"},{"location":"docker/essential_docker_commands/#container-interaction","title":"Container Interaction","text":"<pre><code># Execute command in running container\ndocker exec -it container_name bash\ndocker exec container_name ls /app\n\n# View container logs\ndocker logs container_name\ndocker logs -f container_name  # Follow logs\ndocker logs --tail 100 container_name  # Last 100 lines\n\n# Copy files to/from container\ndocker cp file.txt container_name:/path/\ndocker cp container_name:/path/file.txt ./\n\n# Show container resource usage\ndocker stats container_name\n\n# Show running processes in container\ndocker top container_name\n</code></pre>"},{"location":"docker/essential_docker_commands/#container-management","title":"Container Management","text":"<pre><code># Remove stopped container\ndocker rm container_name\n\n# Remove running container (force)\ndocker rm -f container_name\n\n# Remove all stopped containers\ndocker container prune\n\n# Remove all containers\ndocker rm $(docker ps -aq)\n\n# Inspect container details\ndocker inspect container_name\n</code></pre>"},{"location":"docker/essential_docker_commands/#docker-network-commands","title":"Docker Network Commands","text":""},{"location":"docker/essential_docker_commands/#network-management","title":"Network Management","text":"<pre><code># List networks\ndocker network ls\n\n# Create custom network\ndocker network create mynetwork\ndocker network create --driver bridge mynetwork\n\n# Remove network\ndocker network rm mynetwork\n\n# Connect container to network\ndocker network connect mynetwork container_name\n\n# Disconnect container from network\ndocker network disconnect mynetwork container_name\n\n# Inspect network details\ndocker network inspect mynetwork\n\n# Remove unused networks\ndocker network prune\n</code></pre>"},{"location":"docker/essential_docker_commands/#running-containers-with-networks","title":"Running Containers with Networks","text":"<pre><code># Run container on specific network\ndocker run --network mynetwork nginx\n\n# Run with network alias\ndocker run --network mynetwork --network-alias web nginx\n</code></pre>"},{"location":"docker/essential_docker_commands/#docker-volume-commands","title":"Docker Volume Commands","text":""},{"location":"docker/essential_docker_commands/#volume-management","title":"Volume Management","text":"<pre><code># List volumes\ndocker volume ls\n\n# Create volume\ndocker volume create myvolume\n\n# Remove volume\ndocker volume rm myvolume\n\n# Remove unused volumes\ndocker volume prune\n\n# Inspect volume details\ndocker volume inspect myvolume\n</code></pre>"},{"location":"docker/essential_docker_commands/#using-volumes","title":"Using Volumes","text":"<pre><code># Mount named volume\ndocker run -v myvolume:/data nginx\n\n# Mount host directory (bind mount)\ndocker run -v /host/path:/container/path nginx\n\n# Read-only mount\ndocker run -v /host/path:/container/path:ro nginx\n\n# Create and mount volume in one command\ndocker run -v mydata:/app/data nginx\n</code></pre>"},{"location":"docker/essential_docker_commands/#docker-system-commands","title":"Docker System Commands","text":""},{"location":"docker/essential_docker_commands/#system-information","title":"System Information","text":"<pre><code># Show Docker system info\ndocker info\n\n# Show Docker version\ndocker version\n\n# Show disk usage\ndocker system df\n\n# Show real-time events\ndocker events\n</code></pre>"},{"location":"docker/essential_docker_commands/#system-cleanup","title":"System Cleanup","text":"<pre><code># Remove unused data (containers, networks, images, build cache)\ndocker system prune\n\n# Remove everything including volumes\ndocker system prune -a --volumes\n\n# Remove unused containers\ndocker container prune\n\n# Remove unused images\ndocker image prune\n\n# Remove unused networks\ndocker network prune\n\n# Remove unused volumes\ndocker volume prune\n</code></pre>"},{"location":"docker/essential_docker_commands/#docker-compose-commands","title":"Docker Compose Commands","text":""},{"location":"docker/essential_docker_commands/#basic-operations","title":"Basic Operations","text":"<pre><code># Start services defined in docker-compose.yml\ndocker-compose up\n\n# Start in detached mode\ndocker-compose up -d\n\n# Stop services\ndocker-compose down\n\n# Stop and remove volumes\ndocker-compose down -v\n\n# Build services\ndocker-compose build\n\n# Pull service images\ndocker-compose pull\n</code></pre>"},{"location":"docker/essential_docker_commands/#service-management","title":"Service Management","text":"<pre><code># Scale services\ndocker-compose up -d --scale web=3\n\n# View running services\ndocker-compose ps\n\n# View logs\ndocker-compose logs\ndocker-compose logs web\n\n# Execute command in service\ndocker-compose exec web bash\n\n# Restart services\ndocker-compose restart\n</code></pre>"},{"location":"docker/essential_docker_commands/#common-flags-and-options","title":"Common Flags and Options","text":""},{"location":"docker/essential_docker_commands/#universal-flags","title":"Universal Flags","text":"<ul> <li><code>-d, --detach</code>: Run container in background</li> <li><code>-it</code>: Interactive terminal (combine -i and -t)</li> <li><code>-p, --publish</code>: Publish container port to host</li> <li><code>-v, --volume</code>: Mount volume</li> <li><code>-e, --env</code>: Set environment variables</li> <li><code>--name</code>: Assign name to container</li> <li><code>--rm</code>: Remove container when it exits</li> <li><code>-f, --force</code>: Force operation</li> </ul>"},{"location":"docker/essential_docker_commands/#port-mapping-options","title":"Port Mapping Options","text":"<pre><code># Map specific port\n-p 8080:80\n\n# Map to random host port\n-p 80\n\n# Map specific interface\n-p 127.0.0.1:8080:80\n\n# Map UDP port\n-p 8080:80/udp\n</code></pre>"},{"location":"docker/essential_docker_commands/#volume-mount-options","title":"Volume Mount Options","text":"<pre><code># Named volume\n-v volume_name:/path\n\n# Bind mount\n-v /host/path:/container/path\n\n# Read-only mount\n-v /host/path:/container/path:ro\n\n# Temporary filesystem\n--tmpfs /path\n</code></pre>"},{"location":"docker/essential_docker_commands/#practical-examples","title":"Practical Examples","text":""},{"location":"docker/essential_docker_commands/#example-1-web-application-with-database","title":"Example 1: Web Application with Database","text":"<pre><code># Create network\ndocker network create webapp-net\n\n# Run MySQL database\ndocker run -d \\\n  --name mysql-db \\\n  --network webapp-net \\\n  -e MYSQL_ROOT_PASSWORD=secret \\\n  -e MYSQL_DATABASE=myapp \\\n  -v mysql-data:/var/lib/mysql \\\n  mysql:8.0\n\n# Run web application\ndocker run -d \\\n  --name web-app \\\n  --network webapp-net \\\n  -p 8080:80 \\\n  -e DB_HOST=mysql-db \\\n  -e DB_NAME=myapp \\\n  myapp:latest\n</code></pre>"},{"location":"docker/essential_docker_commands/#example-2-development-environment","title":"Example 2: Development Environment","text":"<pre><code># Run container with code volume for development\ndocker run -it \\\n  --name dev-env \\\n  -v $(pwd):/workspace \\\n  -p 3000:3000 \\\n  -w /workspace \\\n  node:16 \\\n  bash\n</code></pre>"},{"location":"docker/essential_docker_commands/#example-3-temporary-testing-container","title":"Example 3: Temporary Testing Container","text":"<pre><code># Run temporary container that auto-removes\ndocker run --rm -it \\\n  -v $(pwd):/test \\\n  -w /test \\\n  python:3.9 \\\n  python script.py\n</code></pre>"},{"location":"docker/essential_docker_commands/#example-4-multi-container-application-logs","title":"Example 4: Multi-container Application Logs","text":"<pre><code># View logs from multiple containers\ndocker logs -f web-app &amp;\ndocker logs -f mysql-db &amp;\n\n# Or use docker-compose\ndocker-compose logs -f\n</code></pre>"},{"location":"docker/essential_docker_commands/#troubleshooting-commands","title":"Troubleshooting Commands","text":""},{"location":"docker/essential_docker_commands/#debugging-containers","title":"Debugging Containers","text":"<pre><code># Check container status\ndocker ps -a\n\n# Inspect container configuration\ndocker inspect container_name\n\n# Check container logs\ndocker logs --details container_name\n\n# Get shell access to debug\ndocker exec -it container_name /bin/sh\n\n# Check container processes\ndocker top container_name\n\n# Monitor resource usage\ndocker stats\n</code></pre>"},{"location":"docker/essential_docker_commands/#common-issues-solutions","title":"Common Issues Solutions","text":"<pre><code># Port already in use\ndocker ps | grep :8080\nsudo lsof -i :8080\n\n# Container won't start\ndocker logs container_name\ndocker inspect container_name\n\n# Out of disk space\ndocker system df\ndocker system prune -a\n\n# Permission issues\ndocker exec -it container_name ls -la /path\n</code></pre>"},{"location":"docker/essential_docker_commands/#quick-reference-cheat-sheet","title":"Quick Reference Cheat Sheet","text":"Command Description <code>docker run</code> Create and start container <code>docker ps</code> List running containers <code>docker stop</code> Stop container <code>docker rm</code> Remove container <code>docker images</code> List images <code>docker rmi</code> Remove image <code>docker pull</code> Download image <code>docker build</code> Build image <code>docker logs</code> View container logs <code>docker exec</code> Execute command in container <code>docker inspect</code> View detailed info <code>docker network</code> Manage networks <code>docker volume</code> Manage volumes <code>docker system prune</code> Clean up unused resources <p>Remember: Always refer to <code>docker --help</code> or <code>docker COMMAND --help</code> for the most up-to-date information and additional options!</p>"},{"location":"expo/expo_solutions_guide/","title":"Complete Expo React Native Solutions Guide","text":"<p>A comprehensive guide to developing, building, and deploying React Native applications using Expo, covering everything from local development to app store submission.</p>"},{"location":"expo/expo_solutions_guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Getting Started</li> <li>Local Development</li> <li>Project Health &amp; Diagnostics</li> <li>EAS CLI Setup</li> <li>Building for Preview</li> <li>Production Builds</li> <li>App Store Submission</li> <li>Updates &amp; Hot Fixes</li> <li>Advanced Commands</li> <li>Troubleshooting</li> </ol>"},{"location":"expo/expo_solutions_guide/#getting-started","title":"Getting Started","text":""},{"location":"expo/expo_solutions_guide/#prerequisites","title":"Prerequisites","text":"<pre><code># Install Node.js (LTS version recommended)\nnode --version\nnpm --version\n\n# Install Expo CLI globally\nnpm install -g @expo/cli\n\n# Verify installation\nexpo --version\n</code></pre>"},{"location":"expo/expo_solutions_guide/#project-initialization","title":"Project Initialization","text":"<pre><code># Create new Expo project\nnpx create-expo-app MyApp\ncd MyApp\n\n# Initialize existing React Native project with Expo\nnpx create-expo-app --template blank MyApp\nnpx create-expo-app --template typescript MyApp\n\n# Install dependencies\nnpm install\n</code></pre>"},{"location":"expo/expo_solutions_guide/#local-development","title":"Local Development","text":""},{"location":"expo/expo_solutions_guide/#running-your-app-locally","title":"Running Your App Locally","text":"<pre><code># Start development server\nnpm start\n# or\nnpx expo start\n\n# Start with specific options\nnpx expo start --clear        # Clear cache\nnpx expo start --offline      # Offline mode\nnpx expo start --localhost    # Force localhost\nnpx expo start --lan         # Use LAN instead of localhost\nnpx expo start --dev-client  # Use development client\n</code></pre>"},{"location":"expo/expo_solutions_guide/#network-troubleshooting","title":"Network Troubleshooting","text":"<pre><code># Use tunnel when local network doesn't work\nnpx expo start --tunnel\n\n# Force tunnel mode\nnpx expo start --tunnel --force\n\n# Start with specific host\nnpx expo start --host tunnel\nnpx expo start --host lan\nnpx expo start --host localhost\n</code></pre>"},{"location":"expo/expo_solutions_guide/#testing-on-devices","title":"Testing on Devices","text":"<p>Android: - Open Expo Go app - Scan QR code from terminal - Or manually enter the URL shown in terminal</p> <p>iOS: - Use iPhone camera to scan QR code - Tap notification to open in Expo Go - Or use Expo Go app's scanner</p>"},{"location":"expo/expo_solutions_guide/#project-health-diagnostics","title":"Project Health &amp; Diagnostics","text":""},{"location":"expo/expo_solutions_guide/#health-check-commands","title":"Health Check Commands","text":"<pre><code># Check project configuration and dependencies\nnpx expo-doctor\n\n# Install and run doctor\nnpx expo install --fix\n\n# Check for common issues\nnpx expo config\n\n# Validate app.json/app.config.js\nnpx expo config --type public\n\n# Check dependencies compatibility\nnpx expo install --check\n</code></pre>"},{"location":"expo/expo_solutions_guide/#dependency-management","title":"Dependency Management","text":"<pre><code># Install Expo-compatible versions\nnpx expo install package-name\n\n# Update dependencies to compatible versions\nnpx expo install --fix\n\n# Check outdated packages\nnpm outdated\n\n# Update specific package\nnpx expo install react-native@latest\n</code></pre>"},{"location":"expo/expo_solutions_guide/#eas-cli-setup","title":"EAS CLI Setup","text":""},{"location":"expo/expo_solutions_guide/#installation-and-authentication","title":"Installation and Authentication","text":"<pre><code># Install EAS CLI globally\nnpm install -g eas-cli\n\n# Verify installation\neas --version\n\n# Login to your Expo account\neas login\n\n# Check current user\neas whoami\n\n# Logout\neas logout\n</code></pre>"},{"location":"expo/expo_solutions_guide/#project-configuration","title":"Project Configuration","text":"<pre><code># Initialize EAS in your project\neas init\n\n# Configure build profiles\neas build:configure\n\n# View current configuration\neas config\n\n# Link project to Expo account\neas project:init\n</code></pre>"},{"location":"expo/expo_solutions_guide/#building-for-preview","title":"Building for Preview","text":""},{"location":"expo/expo_solutions_guide/#android-preview-builds","title":"Android Preview Builds","text":"<pre><code># Build Android preview (APK)\neas build --profile preview --platform android\n\n# Build with custom profile\neas build --profile development --platform android\n\n# Build and auto-submit to internal testing\neas build --profile preview --platform android --auto-submit\n</code></pre>"},{"location":"expo/expo_solutions_guide/#ios-preview-builds","title":"iOS Preview Builds","text":"<pre><code># Build iOS preview (IPA for simulator)\neas build --profile preview --platform ios\n\n# Build for device testing\neas build --profile preview-device --platform ios\n\n# Build iOS simulator build\neas build --profile preview --platform ios --simulator\n</code></pre>"},{"location":"expo/expo_solutions_guide/#installing-preview-builds","title":"Installing Preview Builds","text":"<p>iOS: 1. Download the <code>.tar.gz</code> file from EAS dashboard 2. Extract the folder 3. Drag the <code>.app</code> file to iOS Simulator 4. App will install and appear on home screen</p> <p>Android: 1. Copy the download link from EAS dashboard 2. Open link on Android device 3. Download and install the APK file 4. Enable \"Install from unknown sources\" if prompted</p>"},{"location":"expo/expo_solutions_guide/#production-builds","title":"Production Builds","text":""},{"location":"expo/expo_solutions_guide/#building-for-app-stores","title":"Building for App Stores","text":"<pre><code># Build for both platforms\neas build\n\n# Build for specific platform\neas build --platform android\neas build --platform ios\n\n# Build with specific profile\neas build --profile production\n\n# Build and auto-submit\neas build --platform android --auto-submit\n</code></pre>"},{"location":"expo/expo_solutions_guide/#build-profiles-configuration","title":"Build Profiles Configuration","text":"<pre><code># View build profiles\ncat eas.json\n\n# Build with custom message\neas build --message \"Release version 1.0.0\"\n\n# Build specific version\neas build --profile production --platform all\n</code></pre>"},{"location":"expo/expo_solutions_guide/#app-store-submission","title":"App Store Submission","text":""},{"location":"expo/expo_solutions_guide/#google-play-store-submission","title":"Google Play Store Submission","text":"<pre><code># Submit to Google Play Store\neas submit --platform android\n\n# Submit with specific build\neas submit --platform android --id [build-id]\n\n# Submit latest build\neas submit --platform android --latest\n\n# Submit with custom track\neas submit --platform android --track internal\n</code></pre>"},{"location":"expo/expo_solutions_guide/#apple-app-store-submission","title":"Apple App Store Submission","text":"<pre><code># Submit to App Store\neas submit --platform ios\n\n# Submit with specific build\neas submit --platform ios --id [build-id]\n\n# Submit latest build\neas submit --platform ios --latest\n\n# Submit to TestFlight only\neas submit --platform ios --apple-team-id [team-id]\n</code></pre>"},{"location":"expo/expo_solutions_guide/#updates-hot-fixes","title":"Updates &amp; Hot Fixes","text":""},{"location":"expo/expo_solutions_guide/#eas-update-commands","title":"EAS Update Commands","text":"<pre><code># Create and publish update\neas update\n\n# Update with message\neas update --message \"Fix critical login bug\"\n\n# Update specific branch/channel\neas update --branch production\neas update --channel production\n\n# Update with automatic message\neas update --auto\n\n# Republish latest update\neas update --republish\n</code></pre>"},{"location":"expo/expo_solutions_guide/#managing-updates","title":"Managing Updates","text":"<pre><code># View update history\neas update:list\n\n# View update details\neas update:view [update-id]\n\n# Delete update\neas update:delete [update-id]\n\n# Roll back to previous update\neas update:rollback\n</code></pre>"},{"location":"expo/expo_solutions_guide/#advanced-commands","title":"Advanced Commands","text":""},{"location":"expo/expo_solutions_guide/#development-and-debugging","title":"Development and Debugging","text":"<pre><code># Generate native code (for custom native code)\neas prebuild\n\n# Prebuild for specific platform\neas prebuild --platform android\neas prebuild --platform ios\n\n# Clean prebuild\neas prebuild --clean\n\n# Run with custom development client\nnpx expo run:android\nnpx expo run:ios\n</code></pre>"},{"location":"expo/expo_solutions_guide/#build-management","title":"Build Management","text":"<pre><code># List all builds\neas build:list\n\n# View build details\neas build:view [build-id]\n\n# Cancel running build\neas build:cancel [build-id]\n\n# Download build artifacts\neas build:download [build-id]\n</code></pre>"},{"location":"expo/expo_solutions_guide/#project-management","title":"Project Management","text":"<pre><code># View project information\neas project:info\n\n# Transfer project ownership\neas project:transfer\n\n# View build queue\neas build:queue\n\n# Check account usage\neas account:usage\n</code></pre>"},{"location":"expo/expo_solutions_guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"expo/expo_solutions_guide/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"expo/expo_solutions_guide/#local-development-issues","title":"Local Development Issues","text":"<pre><code># Clear Expo cache\nnpx expo start --clear\n\n# Reset Metro cache\nnpx expo start --reset-cache\n\n# Fix port conflicts\nnpx expo start --port 8081\n\n# Debug network issues\nnpx expo start --tunnel --offline\n</code></pre>"},{"location":"expo/expo_solutions_guide/#build-issues","title":"Build Issues","text":"<pre><code># Check build logs\neas build:list\neas build:view [build-id]\n\n# Rebuild with clean state\neas build --clear-cache\n\n# Check credentials\neas credentials\n\n# Reset credentials\neas credentials --reset\n</code></pre>"},{"location":"expo/expo_solutions_guide/#update-issues","title":"Update Issues","text":"<pre><code># Check update status\neas channel:list\neas branch:list\n\n# Fix update not showing\neas update --force\n\n# Clear update cache on device\n# (Restart Expo Go or development client)\n</code></pre>"},{"location":"expo/expo_solutions_guide/#debugging-commands","title":"Debugging Commands","text":"<pre><code># Enable verbose logging\nDEBUG=* eas build\n\n# Check system info\nnpx expo diagnostics\n\n# Validate configuration\nnpx expo config --type public\n\n# Check environment\nnpx expo env:info\n</code></pre>"},{"location":"expo/expo_solutions_guide/#complete-development-workflow","title":"Complete Development Workflow","text":""},{"location":"expo/expo_solutions_guide/#daily-development","title":"Daily Development","text":"<pre><code># 1. Start development\nnpx expo start\n\n# 2. Install new packages\nnpx expo install package-name\n\n# 3. Check project health\nnpx expo-doctor\n\n# 4. Test on devices via QR code\n</code></pre>"},{"location":"expo/expo_solutions_guide/#pre-release-checklist","title":"Pre-Release Checklist","text":"<pre><code># 1. Health check\nnpx expo-doctor\n\n# 2. Update dependencies\nnpx expo install --fix\n\n# 3. Build preview\neas build --profile preview --platform all\n\n# 4. Test preview builds\n# 5. Create production build\neas build --profile production\n\n# 6. Submit to stores\neas submit --platform all\n</code></pre>"},{"location":"expo/expo_solutions_guide/#hot-fix-workflow","title":"Hot Fix Workflow","text":"<pre><code># 1. Fix the issue in code\n# 2. Test locally\nnpx expo start\n\n# 3. Publish update\neas update --message \"Fix critical issue\"\n\n# 4. Monitor deployment\neas update:list\n</code></pre>"},{"location":"expo/expo_solutions_guide/#configuration-files","title":"Configuration Files","text":""},{"location":"expo/expo_solutions_guide/#key-configuration-files","title":"Key Configuration Files","text":"<pre><code>\u251c\u2500\u2500 app.json              # Expo configuration\n\u251c\u2500\u2500 eas.json              # EAS Build configuration\n\u251c\u2500\u2500 package.json          # Dependencies and scripts\n\u251c\u2500\u2500 babel.config.js       # Babel configuration\n\u2514\u2500\u2500 .easignore           # Files to ignore during build\n</code></pre>"},{"location":"expo/expo_solutions_guide/#essential-scripts-packagejson","title":"Essential Scripts (package.json)","text":"<pre><code>{\n  \"scripts\": {\n    \"start\": \"expo start\",\n    \"android\": \"expo start --android\",\n    \"ios\": \"expo start --ios\",\n    \"web\": \"expo start --web\",\n    \"build\": \"eas build\",\n    \"build:preview\": \"eas build --profile preview\",\n    \"submit\": \"eas submit\",\n    \"update\": \"eas update\"\n  }\n}\n</code></pre>"},{"location":"expo/expo_solutions_guide/#quick-reference","title":"Quick Reference","text":""},{"location":"expo/expo_solutions_guide/#most-used-commands","title":"Most Used Commands","text":"Command Description <code>npx expo start</code> Start development server <code>npx expo start --tunnel</code> Start with tunnel (network issues) <code>npx expo-doctor</code> Check project health <code>eas login</code> Login to Expo account <code>eas build --profile preview</code> Build preview version <code>eas build</code> Build for production <code>eas submit</code> Submit to app stores <code>eas update</code> Deploy hot fix update"},{"location":"expo/expo_solutions_guide/#build-profiles","title":"Build Profiles","text":"<ul> <li>development: For development with debugging</li> <li>preview: For internal testing (APK/IPA)</li> <li>production: For app store submission</li> </ul>"},{"location":"expo/expo_solutions_guide/#platforms","title":"Platforms","text":"<ul> <li>android: Google Play Store</li> <li>ios: Apple App Store</li> <li>all: Both platforms</li> </ul> <p>Remember to always run <code>npx expo-doctor</code> before building to ensure your project is configured correctly!</p>"},{"location":"git/branching-merging/","title":"Branching &amp; Merging","text":"<p>Problem <code>error: Your local changes to the following files would be overwritten by merge</code></p> <p>Solution</p> <pre><code># Create a new branch\ngit branch {branchName}\n</code></pre> <pre><code># Switch to a branch\ngit checkout {branchName}\n</code></pre> <pre><code># Merge a remote branch into your current branch\ngit merge origin/{branchName}\n</code></pre>"},{"location":"git/industry_git_commands/","title":"Industry Git Commands for Project Management","text":"<p>A comprehensive guide to Git commands essential for professional software development and project management in enterprise environments.</p>"},{"location":"git/industry_git_commands/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Repository Setup &amp; Initialization</li> <li>Branch Management Strategies</li> <li>Collaborative Development</li> <li>Code Review &amp; Pull Requests</li> <li>Release Management</li> <li>Hotfix &amp; Emergency Procedures</li> <li>Advanced Git Operations</li> <li>Git Flow Workflows</li> <li>Troubleshooting &amp; Recovery</li> <li>Enterprise Best Practices</li> </ol>"},{"location":"git/industry_git_commands/#repository-setup-initialization","title":"Repository Setup &amp; Initialization","text":""},{"location":"git/industry_git_commands/#initial-repository-setup","title":"Initial Repository Setup","text":"<pre><code># Initialize new repository\ngit init\n\n# Clone existing repository\ngit clone https://github.com/company/project.git\ngit clone git@github.com:company/project.git  # SSH\n\n# Clone specific branch\ngit clone -b develop https://github.com/company/project.git\n\n# Clone with custom directory name\ngit clone https://github.com/company/project.git my-project\n\n# Set up remote repositories\ngit remote add origin https://github.com/company/project.git\ngit remote add upstream https://github.com/original/project.git\n</code></pre>"},{"location":"git/industry_git_commands/#initial-configuration","title":"Initial Configuration","text":"<pre><code># Configure user information\ngit config --global user.name \"John Doe\"\ngit config --global user.email \"john.doe@company.com\"\n\n# Set default branch name\ngit config --global init.defaultBranch main\n\n# Set up SSH key (for secure access)\nssh-keygen -t ed25519 -C \"john.doe@company.com\"\nssh-add ~/.ssh/id_ed25519\n\n# Configure line endings (important for cross-platform teams)\ngit config --global core.autocrlf true  # Windows\ngit config --global core.autocrlf input # Mac/Linux\n\n# Set default editor\ngit config --global core.editor \"code --wait\"  # VS Code\n</code></pre>"},{"location":"git/industry_git_commands/#branch-management-strategies","title":"Branch Management Strategies","text":""},{"location":"git/industry_git_commands/#creating-and-managing-feature-branches","title":"Creating and Managing Feature Branches","text":"<pre><code># Create and switch to new feature branch\ngit checkout -b feature/user-authentication\ngit switch -c feature/user-authentication  # Modern syntax\n\n# Create branch from specific commit\ngit checkout -b hotfix/security-patch main\n\n# List all branches (local and remote)\ngit branch -a\ngit branch -r  # Remote only\n\n# Switch between branches\ngit checkout develop\ngit switch develop  # Modern syntax\n\n# Rename branch\ngit branch -m old-name new-name\ngit branch -m new-name  # Rename current branch\n\n# Delete branch (safe - prevents deletion if not merged)\ngit branch -d feature/completed-feature\n\n# Force delete branch\ngit branch -D feature/abandoned-feature\n\n# Delete remote branch\ngit push origin --delete feature/old-feature\n</code></pre>"},{"location":"git/industry_git_commands/#branch-tracking-and-updates","title":"Branch Tracking and Updates","text":"<pre><code># Set upstream branch for current branch\ngit push -u origin feature/new-feature\n\n# Track remote branch\ngit checkout --track origin/feature/existing-feature\n\n# Update local branch with remote changes\ngit pull origin develop\ngit pull --rebase origin develop  # Rebase instead of merge\n\n# Fetch all remote changes without merging\ngit fetch --all\n\n# Show branch relationships\ngit branch -vv\n</code></pre>"},{"location":"git/industry_git_commands/#collaborative-development","title":"Collaborative Development","text":""},{"location":"git/industry_git_commands/#daily-development-workflow","title":"Daily Development Workflow","text":"<pre><code># Start work day - update local repository\ngit fetch --all\ngit pull origin develop\n\n# Create feature branch for new work\ngit checkout -b feature/payment-integration\n\n# Stage and commit changes\ngit add .\ngit add -p  # Interactive staging (recommended)\ngit commit -m \"feat: implement stripe payment integration\"\n\n# Push feature branch to remote\ngit push -u origin feature/payment-integration\n\n# Regular commits during development\ngit add .\ngit commit -m \"feat: add payment validation logic\"\ngit push origin feature/payment-integration\n</code></pre>"},{"location":"git/industry_git_commands/#working-with-team-changes","title":"Working with Team Changes","text":"<pre><code># Update feature branch with latest develop\ngit checkout develop\ngit pull origin develop\ngit checkout feature/payment-integration\ngit merge develop\n\n# Alternative: Rebase feature branch\ngit checkout feature/payment-integration\ngit rebase develop\n\n# Interactive rebase for cleaning up commits\ngit rebase -i HEAD~3  # Last 3 commits\n\n# Resolve merge conflicts\ngit status\n# Edit conflicted files\ngit add resolved-file.js\ngit commit  # Complete merge\n</code></pre>"},{"location":"git/industry_git_commands/#stashing-work-in-progress","title":"Stashing Work in Progress","text":"<pre><code># Stash uncommitted changes\ngit stash\ngit stash save \"work in progress on login feature\"\n\n# List stashes\ngit stash list\n\n# Apply most recent stash\ngit stash pop\ngit stash apply  # Apply without removing from stash\n\n# Apply specific stash\ngit stash apply stash@{2}\n\n# Create branch from stash\ngit stash branch feature/from-stash\n</code></pre>"},{"location":"git/industry_git_commands/#code-review-pull-requests","title":"Code Review &amp; Pull Requests","text":""},{"location":"git/industry_git_commands/#preparing-code-for-review","title":"Preparing Code for Review","text":"<pre><code># Ensure branch is up to date\ngit checkout develop\ngit pull origin develop\ngit checkout feature/payment-integration\ngit rebase develop\n\n# Clean up commit history before PR\ngit rebase -i HEAD~5  # Interactive rebase last 5 commits\n\n# Push cleaned branch\ngit push --force-with-lease origin feature/payment-integration\n\n# Create tag for release candidate\ngit tag -a v1.2.0-rc1 -m \"Release candidate 1.2.0\"\ngit push origin v1.2.0-rc1\n</code></pre>"},{"location":"git/industry_git_commands/#code-review-commands","title":"Code Review Commands","text":"<pre><code># Checkout PR for local testing\ngit fetch origin pull/123/head:pr-123\ngit checkout pr-123\n\n# Compare branches\ngit diff develop..feature/payment-integration\ngit diff --name-only develop..feature/payment-integration\n\n# Show commits in feature branch not in develop\ngit log develop..feature/payment-integration --oneline\n\n# Check which branches contain specific commit\ngit branch --contains commit-hash\n</code></pre>"},{"location":"git/industry_git_commands/#release-management","title":"Release Management","text":""},{"location":"git/industry_git_commands/#creating-releases","title":"Creating Releases","text":"<pre><code># Create release branch\ngit checkout -b release/v1.2.0 develop\n\n# Bump version files, update changelog\ngit add .\ngit commit -m \"chore: bump version to 1.2.0\"\n\n# Merge release to main\ngit checkout main\ngit merge --no-ff release/v1.2.0\n\n# Tag the release\ngit tag -a v1.2.0 -m \"Release version 1.2.0\"\n\n# Merge back to develop\ngit checkout develop\ngit merge --no-ff release/v1.2.0\n\n# Push everything\ngit push origin main develop v1.2.0\n\n# Delete release branch\ngit branch -d release/v1.2.0\ngit push origin --delete release/v1.2.0\n</code></pre>"},{"location":"git/industry_git_commands/#managing-tags-and-versions","title":"Managing Tags and Versions","text":"<pre><code># List all tags\ngit tag\ngit tag -l \"v1.*\"  # Filter tags\n\n# Show tag information\ngit show v1.2.0\n\n# Create signed tag\ngit tag -s v1.2.0 -m \"Signed release 1.2.0\"\n\n# Push specific tag\ngit push origin v1.2.0\n\n# Push all tags\ngit push origin --tags\n\n# Delete tag locally and remotely\ngit tag -d v1.2.0\ngit push origin --delete v1.2.0\n\n# Checkout specific version\ngit checkout v1.2.0\n</code></pre>"},{"location":"git/industry_git_commands/#hotfix-emergency-procedures","title":"Hotfix &amp; Emergency Procedures","text":""},{"location":"git/industry_git_commands/#emergency-hotfix-process","title":"Emergency Hotfix Process","text":"<pre><code># Create hotfix branch from main\ngit checkout main\ngit pull origin main\ngit checkout -b hotfix/security-vulnerability\n\n# Make critical fix\ngit add fixed-file.js\ngit commit -m \"fix: resolve critical security vulnerability\"\n\n# Test the fix\n# Run automated tests, security scans\n\n# Merge hotfix to main\ngit checkout main\ngit merge --no-ff hotfix/security-vulnerability\n\n# Tag hotfix release\ngit tag -a v1.2.1 -m \"Hotfix release 1.2.1\"\n\n# Merge hotfix to develop\ngit checkout develop\ngit merge --no-ff hotfix/security-vulnerability\n\n# Push everything\ngit push origin main develop v1.2.1\n\n# Clean up\ngit branch -d hotfix/security-vulnerability\ngit push origin --delete hotfix/security-vulnerability\n</code></pre>"},{"location":"git/industry_git_commands/#reverting-changes-in-production","title":"Reverting Changes in Production","text":"<pre><code># Revert specific commit\ngit revert commit-hash\ngit revert HEAD  # Revert last commit\n\n# Revert merge commit\ngit revert -m 1 merge-commit-hash\n\n# Revert multiple commits\ngit revert HEAD~3..HEAD\n\n# Create revert PR instead of direct revert\ngit revert --no-commit HEAD~3..HEAD\ngit commit -m \"Revert problematic changes from release 1.2.0\"\n</code></pre>"},{"location":"git/industry_git_commands/#advanced-git-operations","title":"Advanced Git Operations","text":""},{"location":"git/industry_git_commands/#rebasing-and-history-management","title":"Rebasing and History Management","text":"<pre><code># Interactive rebase to clean up history\ngit rebase -i HEAD~5\n\n# Rebase feature branch onto develop\ngit rebase develop feature/payment-integration\n\n# Continue rebase after resolving conflicts\ngit rebase --continue\n\n# Abort rebase\ngit rebase --abort\n\n# Squash commits\ngit rebase -i HEAD~3\n# Change 'pick' to 'squash' for commits to combine\n\n# Split a commit\ngit rebase -i HEAD~2\n# Change 'pick' to 'edit' for commit to split\ngit reset HEAD~\n# Stage and commit files separately\ngit add file1.js\ngit commit -m \"first part of original commit\"\ngit add file2.js\ngit commit -m \"second part of original commit\"\ngit rebase --continue\n</code></pre>"},{"location":"git/industry_git_commands/#cherry-picking-and-selective-merging","title":"Cherry-picking and Selective Merging","text":"<pre><code># Cherry-pick specific commit\ngit cherry-pick commit-hash\n\n# Cherry-pick without committing\ngit cherry-pick --no-commit commit-hash\n\n# Cherry-pick range of commits\ngit cherry-pick commit1..commit2\n\n# Cherry-pick from another branch\ngit cherry-pick feature/other-branch~3\n</code></pre>"},{"location":"git/industry_git_commands/#advanced-merging-strategies","title":"Advanced Merging Strategies","text":"<pre><code># Merge with no fast-forward (creates merge commit)\ngit merge --no-ff feature/payment-integration\n\n# Merge and squash all commits\ngit merge --squash feature/payment-integration\n\n# Merge with specific strategy\ngit merge -X ours feature/conflicting-branch  # Prefer our changes\ngit merge -X theirs feature/conflicting-branch  # Prefer their changes\n\n# Abort merge\ngit merge --abort\n\n# Continue merge after resolving conflicts\ngit commit  # Complete the merge\n</code></pre>"},{"location":"git/industry_git_commands/#git-flow-workflows","title":"Git Flow Workflows","text":""},{"location":"git/industry_git_commands/#git-flow-commands-using-git-flow-extension","title":"Git Flow Commands (using git-flow extension)","text":"<pre><code># Initialize git flow\ngit flow init\n\n# Start new feature\ngit flow feature start payment-integration\n\n# Finish feature (merges to develop)\ngit flow feature finish payment-integration\n\n# Start release\ngit flow release start 1.2.0\n\n# Finish release (merges to main and develop, creates tag)\ngit flow release finish 1.2.0\n\n# Start hotfix\ngit flow hotfix start security-patch\n\n# Finish hotfix\ngit flow hotfix finish security-patch\n</code></pre>"},{"location":"git/industry_git_commands/#github-flow-simplified","title":"GitHub Flow (Simplified)","text":"<pre><code># Create feature branch\ngit checkout -b feature/new-feature\n\n# Work and commit\ngit add .\ngit commit -m \"implement new feature\"\ngit push origin feature/new-feature\n\n# Create pull request (via GitHub UI)\n# After review and approval, merge via GitHub\n\n# Clean up local branch\ngit checkout main\ngit pull origin main\ngit branch -d feature/new-feature\n</code></pre>"},{"location":"git/industry_git_commands/#troubleshooting-recovery","title":"Troubleshooting &amp; Recovery","text":""},{"location":"git/industry_git_commands/#undoing-changes","title":"Undoing Changes","text":"<pre><code># Undo last commit (keep changes staged)\ngit reset --soft HEAD~1\n\n# Undo last commit (unstage changes)\ngit reset HEAD~1\ngit reset --mixed HEAD~1  # Same as above\n\n# Undo last commit (lose changes)\ngit reset --hard HEAD~1\n\n# Undo changes to specific file\ngit checkout HEAD -- file.js\ngit restore file.js  # Modern syntax\n\n# Unstage file\ngit reset HEAD file.js\ngit restore --staged file.js  # Modern syntax\n</code></pre>"},{"location":"git/industry_git_commands/#recovery-operations","title":"Recovery Operations","text":"<pre><code># Find lost commits\ngit reflog\ngit log --oneline --all\n\n# Recover deleted branch\ngit checkout -b recovered-branch commit-hash\n\n# Find commit that introduced bug\ngit bisect start\ngit bisect bad HEAD\ngit bisect good v1.1.0\n# Git will checkout commits to test\ngit bisect good/bad  # Mark each commit\ngit bisect reset  # End bisect session\n\n# Recover file from specific commit\ngit show commit-hash:path/to/file.js &gt; recovered-file.js\n\n# Find commits that changed specific file\ngit log --follow -- file.js\n</code></pre>"},{"location":"git/industry_git_commands/#cleaning-repository","title":"Cleaning Repository","text":"<pre><code># Remove untracked files\ngit clean -n  # Dry run\ngit clean -f  # Remove files\ngit clean -fd  # Remove files and directories\n\n# Remove ignored files\ngit clean -fX\n\n# Garbage collect\ngit gc --aggressive --prune=now\n\n# Verify repository integrity\ngit fsck\n</code></pre>"},{"location":"git/industry_git_commands/#enterprise-best-practices","title":"Enterprise Best Practices","text":""},{"location":"git/industry_git_commands/#commit-message-standards","title":"Commit Message Standards","text":"<pre><code># Conventional Commits format\ngit commit -m \"feat: add user authentication system\"\ngit commit -m \"fix: resolve login validation bug\"\ngit commit -m \"docs: update API documentation\"\ngit commit -m \"chore: update dependencies\"\ngit commit -m \"test: add unit tests for payment service\"\n\n# Multi-line commit messages\ngit commit -m \"feat: implement payment processing\n\n- Add Stripe integration\n- Handle payment validation\n- Add error handling for failed payments\n- Update user interface for payment flow\"\n</code></pre>"},{"location":"git/industry_git_commands/#branch-protection-and-policies","title":"Branch Protection and Policies","text":"<pre><code># Check if branch is protected (GitHub CLI)\ngh api repos/:owner/:repo/branches/main\n\n# Enforce signed commits\ngit config --global commit.gpgsign true\n\n# Set up commit template\ngit config --global commit.template ~/.gitmessage\n</code></pre>"},{"location":"git/industry_git_commands/#repository-maintenance","title":"Repository Maintenance","text":"<pre><code># Update all remote references\ngit remote update --prune\n\n# Show repository statistics\ngit count-objects -v\n\n# Find large files in repository\ngit rev-list --objects --all | git cat-file --batch-check='%(objecttype) %(objectname) %(objectsize) %(rest)' | awk '/^blob/ {print substr($0,6)}' | sort --numeric-sort --key=2\n\n# Remove file from entire history (use carefully!)\ngit filter-branch --force --index-filter 'git rm --cached --ignore-unmatch large-file.zip' --prune-empty --tag-name-filter cat -- --all\n</code></pre>"},{"location":"git/industry_git_commands/#security-and-access-management","title":"Security and Access Management","text":"<pre><code># Sign commits with GPG\ngit commit -S -m \"signed commit message\"\n\n# Verify signatures\ngit log --show-signature\n\n# Configure GPG key\ngit config --global user.signingkey GPG-KEY-ID\n\n# Use SSH for authentication\ngit config --global url.\"git@github.com:\".insteadOf \"https://github.com/\"\n</code></pre>"},{"location":"git/industry_git_commands/#daily-workflow-checklist","title":"Daily Workflow Checklist","text":""},{"location":"git/industry_git_commands/#morning-routine","title":"Morning Routine","text":"<pre><code># 1. Update local repository\ngit fetch --all\ngit checkout develop\ngit pull origin develop\n\n# 2. Check status of current work\ngit status\ngit stash list\n\n# 3. Review what happened overnight\ngit log --oneline --since=\"1 day ago\"\n</code></pre>"},{"location":"git/industry_git_commands/#before-leaving-work","title":"Before Leaving Work","text":"<pre><code># 1. Commit or stash work in progress\ngit add .\ngit commit -m \"wip: progress on feature X\" || git stash\n\n# 2. Push current branch\ngit push origin current-branch-name\n\n# 3. Clean up workspace\ngit status  # Ensure clean working directory\n</code></pre>"},{"location":"git/industry_git_commands/#weekly-maintenance","title":"Weekly Maintenance","text":"<pre><code># 1. Update all branches\ngit fetch --all --prune\n\n# 2. Clean up merged branches\ngit branch --merged | grep -v \"\\*\\|main\\|develop\" | xargs -n 1 git branch -d\n\n# 3. Update main and develop\ngit checkout main &amp;&amp; git pull origin main\ngit checkout develop &amp;&amp; git pull origin develop\n</code></pre>"},{"location":"git/industry_git_commands/#emergency-commands-reference","title":"Emergency Commands Reference","text":"Situation Command Need to switch branches quickly <code>git stash &amp;&amp; git checkout branch-name</code> Accidentally committed to wrong branch <code>git reset --soft HEAD~1</code> Need to undo last commit but keep changes <code>git reset --soft HEAD~1</code> Made mistake in commit message <code>git commit --amend -m \"corrected message\"</code> Need to find when bug was introduced <code>git bisect start &amp;&amp; git bisect bad HEAD &amp;&amp; git bisect good v1.0.0</code> Accidentally deleted branch <code>git reflog</code> then <code>git checkout -b branch-name commit-hash</code> Production is broken, need quick revert <code>git revert commit-hash &amp;&amp; git push origin main</code> Merge conflict during important release <code>git merge --abort</code> then resolve properly <p>Remember: Always backup important work, communicate with your team before force-pushing, and follow your company's specific Git workflows and policies!</p>"},{"location":"git/merge-conflict/","title":"Fixing Merge Conflict in Git","text":"<p>Problem <code>error: Your local changes to the following files would be overwritten by merge</code></p> <p>Solution</p> <pre><code>git stash\ngit pull origin main\ngit stash pop\n# Resolve any merge conflicts if they arise\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/","title":"Complete Kubernetes Guide - Commands and Concepts","text":"<p>A comprehensive guide to Kubernetes (K8s) container orchestration platform, covering core concepts, CLI commands, deployment strategies, and best practices for managing containerized applications at scale.</p>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>What is Kubernetes?</li> <li>Core Kubernetes Concepts</li> <li>Kubernetes Architecture</li> <li>Installation and Setup</li> <li>Essential kubectl Commands</li> <li>Working with YAML Manifests</li> <li>Networking and Services</li> <li>Storage and Volumes</li> <li>Configuration Management</li> <li>Scaling and Updates</li> <li>Monitoring and Debugging</li> <li>Advanced Topics</li> <li>Best Practices</li> <li>Common Workflows</li> </ol>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#what-is-kubernetes","title":"What is Kubernetes?","text":"<p>Kubernetes (K8s) is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications across a cluster of machines.</p>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#why-kubernetes-is-necessary","title":"Why Kubernetes is Necessary","text":"<p>The Problem: - Single containers can't handle increasing traffic (CPU/memory bottlenecks) - Single point of failure - if one container fails, entire application goes down - Manual scaling and management becomes complex - No built-in load balancing or service discovery</p> <p>The Solution: - Multiple replicas of applications for high availability - Automatic scaling based on demand - Self-healing - replaces failed containers automatically - Load balancing and service discovery - Rolling updates without downtime</p>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#how-kubernetes-works","title":"How Kubernetes Works","text":"<ul> <li>Docker provides individual containers that package your applications</li> <li>Kubernetes decides how, where, and when these containers run</li> <li>Declarative approach - you describe desired state using YAML files</li> <li>Kubernetes maintains that state automatically</li> </ul>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#core-kubernetes-concepts","title":"Core Kubernetes Concepts","text":""},{"location":"kubernetes/kubernetes_comprehensive_guide/#1-cluster","title":"1. Cluster","text":"<p>A group of physical or virtual machines working as a single system.</p> <p>Components: - Control Plane: Makes decisions, schedules pods, reconciles states, monitors health - Worker Nodes: Machines where containers actually run</p>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#2-pods","title":"2. Pods","text":"<p>The smallest deployable unit in Kubernetes: - Wraps one or more containers - Each pod gets its own IP address - Containers in a pod share storage and network - Ephemeral - pods can be created and destroyed</p>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#3-replicaset","title":"3. ReplicaSet","text":"<p>Ensures a specified number of identical pods are always running: - If a pod dies, automatically spins up a new one - Maintains desired state - Handles pod lifecycle management</p>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#4-deployment","title":"4. Deployment","text":"<p>A higher-level object that manages ReplicaSets: - Enables rolling updates - Gradually replaces old pods with new ones - Zero-downtime deployments - Rollback capabilities</p>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#5-service","title":"5. Service","text":"<p>A stable network endpoint that routes traffic to available pods: - Permanent IP or DNS name - Load balances requests across multiple replicas - Abstracts away pod temporary nature - Service discovery mechanism</p>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#6-configmaps-and-secrets","title":"6. ConfigMaps and Secrets","text":"<ul> <li>ConfigMaps: Store non-sensitive configuration data (database URLs, settings)</li> <li>Secrets: Store sensitive data (passwords, API keys, certificates)</li> <li>Both are securely injected into pods</li> </ul>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#7-ingress","title":"7. Ingress","text":"<p>Acts as a smart router: - Exposes HTTP/HTTPS routes from outside cluster - Routes to internal services - SSL termination - Load balancing</p>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#8-volumes","title":"8. Volumes","text":"<p>Provide persistent storage: - Data survives pod restarts - Can be shared between pods - Various storage backends supported</p>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#kubernetes-architecture","title":"Kubernetes Architecture","text":""},{"location":"kubernetes/kubernetes_comprehensive_guide/#control-plane-components","title":"Control Plane Components","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Control Plane              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  API Server  \u2502  etcd  \u2502  Scheduler     \u2502\n\u2502  Controller Manager   \u2502  Cloud Manager \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#worker-node-components","title":"Worker Node Components","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Worker Node                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Kubelet  \u2502  Container Runtime  \u2502 Proxy \u2502\n\u2502           \u2502     (Docker/containerd)      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"kubernetes/kubernetes_comprehensive_guide/#installing-kubectl-kubernetes-cli","title":"Installing kubectl (Kubernetes CLI)","text":""},{"location":"kubernetes/kubernetes_comprehensive_guide/#macos-apple-silicon","title":"macOS (Apple Silicon)","text":"<pre><code># Download kubectl\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/arm64/kubectl\"\n\n# Make executable\nchmod +x kubectl\n\n# Move to PATH\nsudo mv kubectl /usr/local/bin/\n\n# Verify installation\nkubectl version --client\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#macos-intel","title":"macOS (Intel)","text":"<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/amd64/kubectl\"\nchmod +x kubectl\nsudo mv kubectl /usr/local/bin/\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#linux","title":"Linux","text":"<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nchmod +x kubectl\nsudo mv kubectl /usr/local/bin/\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#windows","title":"Windows","text":"<pre><code>curl -LO \"https://dl.k8s.io/release/v1.28.0/bin/windows/amd64/kubectl.exe\"\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#installing-minikube-local-development","title":"Installing Minikube (Local Development)","text":""},{"location":"kubernetes/kubernetes_comprehensive_guide/#macos","title":"macOS","text":"<pre><code># Using Homebrew\nbrew install minikube\n\n# Or direct download\ncurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-arm64\nsudo install minikube-darwin-arm64 /usr/local/bin/minikube\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#linux_1","title":"Linux","text":"<pre><code>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#verify-installation","title":"Verify Installation","text":"<pre><code>minikube version\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#alternative-local-development-options","title":"Alternative Local Development Options","text":"<pre><code># Kind (Kubernetes in Docker)\ncurl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64\nchmod +x ./kind\nsudo mv ./kind /usr/local/bin/kind\n\n# K3s (Lightweight Kubernetes)\ncurl -sfL https://get.k3s.io | sh -\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#essential-kubectl-commands","title":"Essential kubectl Commands","text":""},{"location":"kubernetes/kubernetes_comprehensive_guide/#cluster-management","title":"Cluster Management","text":"<pre><code># Start local Minikube cluster\nminikube start\n\n# Start with specific resources\nminikube start --memory=4096 --cpus=2\n\n# Start with specific Kubernetes version\nminikube start --kubernetes-version=v1.28.0\n\n# Stop Minikube cluster\nminikube stop\n\n# Delete Minikube cluster\nminikube delete\n\n# Get cluster information\nkubectl cluster-info\nkubectl cluster-info dump\n\n# Check cluster nodes\nkubectl get nodes\nkubectl get nodes -o wide\n\n# Check cluster status\nkubectl get componentstatuses\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#working-with-resources","title":"Working with Resources","text":""},{"location":"kubernetes/kubernetes_comprehensive_guide/#viewing-resources","title":"Viewing Resources","text":"<pre><code># List all pods\nkubectl get pods\nkubectl get po  # Short form\n\n# List pods with more details\nkubectl get pods -o wide\n\n# List pods in all namespaces\nkubectl get pods --all-namespaces\nkubectl get pods -A\n\n# Watch pods in real-time\nkubectl get pods -w\nkubectl get pods --watch\n\n# List other resources\nkubectl get services\nkubectl get deployments\nkubectl get replicasets\nkubectl get configmaps\nkubectl get secrets\nkubectl get ingress\nkubectl get pv,pvc  # Persistent volumes and claims\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#describing-resources","title":"Describing Resources","text":"<pre><code># Get detailed information about a pod\nkubectl describe pod &lt;pod-name&gt;\n\n# Describe other resources\nkubectl describe service &lt;service-name&gt;\nkubectl describe deployment &lt;deployment-name&gt;\nkubectl describe node &lt;node-name&gt;\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#creating-and-managing-resources","title":"Creating and Managing Resources","text":"<pre><code># Apply configuration from file\nkubectl apply -f deployment.yaml\nkubectl apply -f service.yaml\n\n# Apply all files in directory\nkubectl apply -f k8s/\nkubectl apply -f . --recursive\n\n# Create resource imperatively\nkubectl create deployment nginx --image=nginx\nkubectl create service clusterip my-service --tcp=80:80\n\n# Delete resources\nkubectl delete -f deployment.yaml\nkubectl delete deployment &lt;deployment-name&gt;\nkubectl delete pod &lt;pod-name&gt;\n\n# Delete all resources in namespace\nkubectl delete all --all\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#working-with-yaml-manifests","title":"Working with YAML Manifests","text":""},{"location":"kubernetes/kubernetes_comprehensive_guide/#pod-manifest","title":"Pod Manifest","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\n  labels:\n    app: my-app\nspec:\n  containers:\n  - name: my-container\n    image: nginx:1.21\n    ports:\n    - containerPort: 80\n    env:\n    - name: ENV_VAR\n      value: \"production\"\n    resources:\n      requests:\n        memory: \"64Mi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"128Mi\"\n        cpu: \"500m\"\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#deployment-manifest","title":"Deployment Manifest","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app-deployment\n  labels:\n    app: my-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: my-app\n  template:\n    metadata:\n      labels:\n        app: my-app\n    spec:\n      containers:\n      - name: my-app\n        image: my-app:latest\n        ports:\n        - containerPort: 8080\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            configMapKeyRef:\n              name: app-config\n              key: database-url\n        - name: API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: app-secrets\n              key: api-key\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#service-manifest","title":"Service Manifest","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-app-service\nspec:\n  selector:\n    app: my-app\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n  type: ClusterIP  # ClusterIP, NodePort, LoadBalancer\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#configmap-manifest","title":"ConfigMap Manifest","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  database-url: \"postgresql://db:5432/myapp\"\n  log-level: \"info\"\n  config.properties: |\n    property1=value1\n    property2=value2\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#secret-manifest","title":"Secret Manifest","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secrets\ntype: Opaque\ndata:\n  api-key: YWJjZGVmZ2hpams=  # base64 encoded\n  password: cGFzc3dvcmQ=     # base64 encoded\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#networking-and-services","title":"Networking and Services","text":""},{"location":"kubernetes/kubernetes_comprehensive_guide/#service-types","title":"Service Types","text":"<pre><code># ClusterIP (default) - Internal cluster access only\nkubectl expose deployment my-app --port=80 --target-port=8080\n\n# NodePort - Access via node IP:port\nkubectl expose deployment my-app --type=NodePort --port=80\n\n# LoadBalancer - External load balancer (cloud providers)\nkubectl expose deployment my-app --type=LoadBalancer --port=80\n\n# Access service in Minikube\nminikube service my-app-service\nminikube service my-app-service --url\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#ingress-configuration","title":"Ingress Configuration","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-app-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n  - host: my-app.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: my-app-service\n            port:\n              number: 80\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#network-policies","title":"Network Policies","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-all\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#storage-and-volumes","title":"Storage and Volumes","text":""},{"location":"kubernetes/kubernetes_comprehensive_guide/#persistent-volume-pv","title":"Persistent Volume (PV)","text":"<pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: my-pv\nspec:\n  capacity:\n    storage: 1Gi\n  accessModes:\n  - ReadWriteOnce\n  hostPath:\n    path: /data/my-app\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#persistent-volume-claim-pvc","title":"Persistent Volume Claim (PVC)","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#using-volumes-in-pods","title":"Using Volumes in Pods","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: my-container\n    image: nginx\n    volumeMounts:\n    - name: my-storage\n      mountPath: /data\n  volumes:\n  - name: my-storage\n    persistentVolumeClaim:\n      claimName: my-pvc\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#storage-commands","title":"Storage Commands","text":"<pre><code># List persistent volumes\nkubectl get pv\n\n# List persistent volume claims\nkubectl get pvc\n\n# Describe storage\nkubectl describe pv my-pv\nkubectl describe pvc my-pvc\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#configuration-management","title":"Configuration Management","text":""},{"location":"kubernetes/kubernetes_comprehensive_guide/#configmaps","title":"ConfigMaps","text":"<pre><code># Create ConfigMap from literal values\nkubectl create configmap app-config --from-literal=key1=value1 --from-literal=key2=value2\n\n# Create ConfigMap from file\nkubectl create configmap app-config --from-file=config.properties\n\n# Create ConfigMap from directory\nkubectl create configmap app-config --from-file=config/\n\n# Get ConfigMap\nkubectl get configmap\nkubectl describe configmap app-config\n\n# Edit ConfigMap\nkubectl edit configmap app-config\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#secrets","title":"Secrets","text":"<pre><code># Create Secret from literal values\nkubectl create secret generic app-secrets --from-literal=username=admin --from-literal=password=secret\n\n# Create Secret from files\nkubectl create secret generic app-secrets --from-file=username.txt --from-file=password.txt\n\n# Create Docker registry secret\nkubectl create secret docker-registry regcred \\\n  --docker-server=docker.io \\\n  --docker-username=myuser \\\n  --docker-password=mypass \\\n  --docker-email=myemail@example.com\n\n# Get Secrets\nkubectl get secrets\nkubectl describe secret app-secrets\n\n# Decode secret values\nkubectl get secret app-secrets -o jsonpath='{.data.password}' | base64 --decode\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#scaling-and-updates","title":"Scaling and Updates","text":""},{"location":"kubernetes/kubernetes_comprehensive_guide/#manual-scaling","title":"Manual Scaling","text":"<pre><code># Scale deployment\nkubectl scale deployment my-app --replicas=5\n\n# Scale ReplicaSet\nkubectl scale rs my-replicaset --replicas=3\n\n# Auto-scaling (Horizontal Pod Autoscaler)\nkubectl autoscale deployment my-app --min=2 --max=10 --cpu-percent=80\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#rolling-updates","title":"Rolling Updates","text":"<pre><code># Update deployment image\nkubectl set image deployment/my-app container-name=new-image:tag\n\n# Update with new image\nkubectl set image deployment/my-app my-app=my-app:v2\n\n# Check rollout status\nkubectl rollout status deployment/my-app\n\n# Check rollout history\nkubectl rollout history deployment/my-app\n\n# Rollback to previous version\nkubectl rollout undo deployment/my-app\n\n# Rollback to specific revision\nkubectl rollout undo deployment/my-app --to-revision=2\n\n# Pause rollout\nkubectl rollout pause deployment/my-app\n\n# Resume rollout\nkubectl rollout resume deployment/my-app\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#horizontal-pod-autoscaler-hpa","title":"Horizontal Pod Autoscaler (HPA)","text":"<pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: my-app-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: my-app\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"kubernetes/kubernetes_comprehensive_guide/#pod-logs-and-debugging","title":"Pod Logs and Debugging","text":"<pre><code># View pod logs\nkubectl logs &lt;pod-name&gt;\nkubectl logs &lt;pod-name&gt; -c &lt;container-name&gt;  # Multi-container pod\n\n# Follow logs in real-time\nkubectl logs -f &lt;pod-name&gt;\n\n# Get logs from previous container instance\nkubectl logs &lt;pod-name&gt; --previous\n\n# Get logs from all containers in pod\nkubectl logs &lt;pod-name&gt; --all-containers\n\n# Execute commands in pod\nkubectl exec -it &lt;pod-name&gt; -- /bin/bash\nkubectl exec -it &lt;pod-name&gt; -c &lt;container-name&gt; -- /bin/sh\n\n# Copy files to/from pod\nkubectl cp &lt;pod-name&gt;:/path/to/file ./local-file\nkubectl cp ./local-file &lt;pod-name&gt;:/path/to/file\n\n# Port forwarding for debugging\nkubectl port-forward &lt;pod-name&gt; 8080:80\nkubectl port-forward service/&lt;service-name&gt; 8080:80\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#resource-usage-and-events","title":"Resource Usage and Events","text":"<pre><code># Check resource usage\nkubectl top nodes\nkubectl top pods\nkubectl top pods --containers\n\n# View cluster events\nkubectl get events\nkubectl get events --sort-by='.metadata.creationTimestamp'\n\n# Check resource quotas\nkubectl get resourcequota\nkubectl describe resourcequota\n\n# Check limits\nkubectl get limitrange\nkubectl describe limitrange\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#advanced-topics","title":"Advanced Topics","text":""},{"location":"kubernetes/kubernetes_comprehensive_guide/#namespaces","title":"Namespaces","text":"<pre><code># Create namespace\nkubectl create namespace production\nkubectl create ns dev  # Short form\n\n# List namespaces\nkubectl get namespaces\nkubectl get ns\n\n# Set default namespace\nkubectl config set-context --current --namespace=production\n\n# Delete namespace\nkubectl delete namespace dev\n\n# Work with specific namespace\nkubectl get pods -n production\nkubectl apply -f deployment.yaml -n production\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#labels-and-selectors","title":"Labels and Selectors","text":"<pre><code># Add labels to resources\nkubectl label pod my-pod environment=production\nkubectl label pod my-pod version=v1.0\n\n# Remove labels\nkubectl label pod my-pod environment-\n\n# Select resources by labels\nkubectl get pods -l environment=production\nkubectl get pods -l 'environment in (production,staging)'\nkubectl get pods -l environment=production,version=v1.0\n\n# Show labels\nkubectl get pods --show-labels\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#jobs-and-cronjobs","title":"Jobs and CronJobs","text":"<pre><code># Job\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: my-job\nspec:\n  template:\n    spec:\n      containers:\n      - name: my-job\n        image: busybox\n        command: [\"echo\", \"Hello World\"]\n      restartPolicy: Never\n  backoffLimit: 4\n\n# CronJob\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: my-cronjob\nspec:\n  schedule: \"0 2 * * *\"  # Daily at 2 AM\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: my-cronjob\n            image: busybox\n            command: [\"echo\", \"Scheduled task\"]\n          restartPolicy: Never\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#best-practices","title":"Best Practices","text":""},{"location":"kubernetes/kubernetes_comprehensive_guide/#resource-management","title":"Resource Management","text":"<pre><code># Always set resource requests and limits\nresources:\n  requests:\n    memory: \"64Mi\"\n    cpu: \"250m\"\n  limits:\n    memory: \"128Mi\"\n    cpu: \"500m\"\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#health-checks","title":"Health Checks","text":"<pre><code># Liveness and readiness probes\nlivenessProbe:\n  httpGet:\n    path: /health\n    port: 8080\n  initialDelaySeconds: 30\n  periodSeconds: 10\n\nreadinessProbe:\n  httpGet:\n    path: /ready\n    port: 8080\n  initialDelaySeconds: 5\n  periodSeconds: 5\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#security-best-practices","title":"Security Best Practices","text":"<pre><code># Security context\nsecurityContext:\n  runAsNonRoot: true\n  runAsUser: 1000\n  allowPrivilegeEscalation: false\n  readOnlyRootFilesystem: true\n  capabilities:\n    drop:\n    - ALL\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#common-workflows","title":"Common Workflows","text":""},{"location":"kubernetes/kubernetes_comprehensive_guide/#development-workflow","title":"Development Workflow","text":"<pre><code># 1. Prepare Docker image\ndocker build -t my-app:latest .\ndocker tag my-app:latest username/my-app:latest\ndocker push username/my-app:latest\n\n# 2. Deploy to Kubernetes\nkubectl apply -f k8s/deployment.yaml\nkubectl apply -f k8s/service.yaml\n\n# 3. Check deployment\nkubectl get pods -w\nkubectl get services\n\n# 4. Access application\nminikube service my-app-service\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#automated-deployment-script","title":"Automated Deployment Script","text":"<pre><code>#!/bin/bash\n# deploy.sh\n\nset -e\n\nIMAGE=\"username/my-app:latest\"\nNAME=\"my-app\"\n\necho \"Building Docker image...\"\ndocker build -t $IMAGE .\n\necho \"Pushing to Docker Hub...\"\ndocker push $IMAGE\n\necho \"Deploying to Kubernetes...\"\nkubectl apply -f k8s/deployment.yaml\nkubectl apply -f k8s/service.yaml\n\necho \"Checking deployment status...\"\nkubectl get pods\nkubectl get services\n\necho \"Getting service URL...\"\nminikube service $NAME-service --url\n\necho \"Deployment completed successfully!\"\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#production-deployment-checklist","title":"Production Deployment Checklist","text":"<pre><code># 1. Resource limits set\nkubectl describe pod &lt;pod-name&gt; | grep -A 5 \"Limits\"\n\n# 2. Health checks configured\nkubectl describe pod &lt;pod-name&gt; | grep -A 5 \"Liveness\"\n\n# 3. Security context applied\nkubectl describe pod &lt;pod-name&gt; | grep -A 10 \"Security Context\"\n\n# 4. Monitoring and logging\nkubectl logs &lt;pod-name&gt;\nkubectl top pod &lt;pod-name&gt;\n\n# 5. Backup and disaster recovery\nkubectl get all -o yaml &gt; backup.yaml\n</code></pre>"},{"location":"kubernetes/kubernetes_comprehensive_guide/#quick-reference","title":"Quick Reference","text":""},{"location":"kubernetes/kubernetes_comprehensive_guide/#essential-commands","title":"Essential Commands","text":"Command Description <code>kubectl get pods</code> List all pods <code>kubectl apply -f file.yaml</code> Apply configuration <code>kubectl describe pod &lt;name&gt;</code> Get detailed pod info <code>kubectl logs &lt;pod-name&gt;</code> View pod logs <code>kubectl exec -it &lt;pod&gt; -- bash</code> Shell into pod <code>kubectl scale deployment &lt;name&gt; --replicas=3</code> Scale deployment <code>kubectl delete pod &lt;name&gt;</code> Delete pod <code>kubectl get services</code> List services <code>minikube service &lt;name&gt;</code> Access service in Minikube <code>kubectl rollout restart deployment/&lt;name&gt;</code> Restart deployment"},{"location":"kubernetes/kubernetes_comprehensive_guide/#resource-short-names","title":"Resource Short Names","text":"Resource Short Name pods po services svc deployments deploy replicasets rs namespaces ns configmaps cm persistentvolumes pv persistentvolumeclaims pvc"},{"location":"kubernetes/kubernetes_comprehensive_guide/#common-flags","title":"Common Flags","text":"Flag Description <code>-n &lt;namespace&gt;</code> Specify namespace <code>-o wide</code> Show more columns <code>-o yaml</code> Output in YAML format <code>-w, --watch</code> Watch for changes <code>--dry-run=client</code> Preview without applying <code>-f &lt;file&gt;</code> Specify file <p>Kubernetes provides powerful orchestration capabilities for containerized applications, enabling scalable, resilient, and manageable deployments across distributed systems!</p>"},{"location":"nest/server_setup/","title":"NestJS Server Setup Guide","text":"<p>This guide covers deploying a NestJS application to an EC2 server with PM2 for process management and Caddy as a reverse proxy.</p>"},{"location":"nest/server_setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Ubuntu EC2 instance</li> <li>SSH access with PEM key</li> <li>Node.js and npm installed on server</li> <li>PM2 installed globally (<code>npm install -g pm2</code>)</li> <li>Caddy web server installed</li> </ul>"},{"location":"nest/server_setup/#1-copy-project-to-server","title":"1. Copy Project to Server","text":"<p>Copy your NestJS project to the server, excluding <code>node_modules</code>:</p> <pre><code>rsync -avz --exclude 'node_modules' -e \"ssh -i server.pem\" server-folder ubuntu@**.**.**.***:~/\n</code></pre>"},{"location":"nest/server_setup/#2-server-setup","title":"2. Server Setup","text":"<p>SSH into your server:</p> <pre><code>ssh -i server.pem ubuntu@**.**.**.***\n</code></pre> <p>Navigate to project directory:</p> <pre><code>cd ~/server-folder\n</code></pre> <p>Install dependencies:</p> <pre><code>npm install\n</code></pre> <p>Build the application:</p> <pre><code>npm run build\n</code></pre>"},{"location":"nest/server_setup/#3-configure-pm2","title":"3. Configure PM2","text":"<p>Create a PM2 ecosystem configuration file:</p> <pre><code>cat &gt; ecosystem.config.js &lt;&lt; 'EOF'\nmodule.exports = {\n  apps: [{\n    name: 'server-name',\n    script: 'dist/main.js',\n    instances: 1,\n    exec_mode: 'cluster',\n    env: {\n      NODE_ENV: 'production'\n    },\n    error_file: './logs/pm2-error.log',\n    out_file: './logs/pm2-out.log',\n    log_date_format: 'YYYY-MM-DD HH:mm:ss Z',\n    merge_logs: true,\n    autorestart: true,\n    watch: false,\n    max_memory_restart: '1G'\n  }]\n};\nEOF\n</code></pre> <p>Start the application with PM2:</p> <pre><code>pm2 start ecosystem.config.js\n</code></pre> <p>Save PM2 process list:</p> <pre><code>pm2 save\n</code></pre> <p>Configure PM2 to start on system boot:</p> <pre><code>sudo env PATH=$PATH:/usr/bin pm2 startup systemd -u ubuntu --hp /home/ubuntu\n</code></pre>"},{"location":"nest/server_setup/#4-configure-caddy","title":"4. Configure Caddy","text":"<p>Edit the Caddyfile:</p> <pre><code>sudo nano /etc/caddy/Caddyfile\n</code></pre> <p>Add the following configuration (replace with your domain):</p> <pre><code>**.**.**.***.nip.io {\n    reverse_proxy localhost:3000\n}\n</code></pre> <p>Reload Caddy:</p> <pre><code>sudo systemctl reload caddy\n</code></pre>"},{"location":"nest/server_setup/#5-configure-ec2-security-group","title":"5. Configure EC2 Security Group","text":"<p>Ensure your EC2 Security Group allows inbound traffic:</p> <ul> <li>Port 22 (SSH): From your IP</li> <li>Port 80 (HTTP): From 0.0.0.0/0</li> <li>Port 443 (HTTPS): From 0.0.0.0/0</li> </ul>"},{"location":"nest/server_setup/#via-aws-console","title":"Via AWS Console:","text":"<ol> <li>Go to EC2 \u2192 Instances</li> <li>Select your instance</li> <li>Click on the Security tab</li> <li>Click on the Security Group link</li> <li>Edit inbound rules</li> <li>Add rules for ports 80 and 443</li> </ol>"},{"location":"nest/server_setup/#via-aws-cli","title":"Via AWS CLI:","text":"<pre><code>aws ec2 authorize-security-group-ingress --group-id &lt;YOUR_SECURITY_GROUP_ID&gt; --protocol tcp --port 80 --cidr 0.0.0.0/0\naws ec2 authorize-security-group-ingress --group-id &lt;YOUR_SECURITY_GROUP_ID&gt; --protocol tcp --port 443 --cidr 0.0.0.0/0\n</code></pre>"},{"location":"nest/server_setup/#6-verify-setup","title":"6. Verify Setup","text":"<p>Check PM2 status:</p> <pre><code>pm2 status\n</code></pre> <p>Check application logs:</p> <pre><code>pm2 logs server-name\n</code></pre> <p>Test local connection:</p> <pre><code>curl http://localhost:3000\n</code></pre> <p>Check Caddy status:</p> <pre><code>sudo systemctl status caddy\n</code></pre>"},{"location":"nest/server_setup/#useful-pm2-commands","title":"Useful PM2 Commands","text":"<ul> <li><code>pm2 status</code> - Check application status</li> <li><code>pm2 logs [app-name]</code> - View logs</li> <li><code>pm2 restart [app-name]</code> - Restart application</li> <li><code>pm2 stop [app-name]</code> - Stop application</li> <li><code>pm2 delete [app-name]</code> - Remove application from PM2</li> <li><code>pm2 monit</code> - Monitor in real-time</li> <li><code>pm2 save</code> - Save current process list</li> <li><code>pm2 resurrect</code> - Restore saved processes</li> </ul>"},{"location":"nest/server_setup/#useful-caddy-commands","title":"Useful Caddy Commands","text":"<ul> <li><code>sudo systemctl status caddy</code> - Check Caddy status</li> <li><code>sudo systemctl reload caddy</code> - Reload configuration</li> <li><code>sudo systemctl restart caddy</code> - Restart Caddy</li> <li><code>sudo journalctl -u caddy -f</code> - View Caddy logs in real-time</li> <li><code>caddy validate --config /etc/caddy/Caddyfile</code> - Validate configuration</li> </ul>"},{"location":"nest/server_setup/#updating-the-application","title":"Updating the Application","text":"<p>When you need to update the application:</p> <pre><code># On your local machine, sync changes\nrsync -avz --exclude 'node_modules' -e \"ssh -i BusPinDev.pem\" ybs-app-server ubuntu@54.249.36.137:~/\n\n# On the server\ncd ~/server-folder\nnpm install  # If dependencies changed\nnpm run build\npm2 restart server-name\n</code></pre>"},{"location":"nest/server_setup/#environment-variables","title":"Environment Variables","text":"<p>Ensure your <code>.env</code> file is properly configured on the server with production values:</p> <pre><code>PORT=3000\nNODE_ENV=production\nDATABASE_URL=your_database_url\nJWT_SECRET=your_jwt_secret\n# Add other environment variables as needed\n</code></pre>"},{"location":"nest/server_setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"nest/server_setup/#application-not-starting","title":"Application not starting","text":"<p>Check PM2 logs:</p> <pre><code>pm2 logs server-name --err\n</code></pre>"},{"location":"nest/server_setup/#cant-access-from-browser","title":"Can't access from browser","text":"<ol> <li>Check if the app is running: <code>pm2 status</code></li> <li>Check if Caddy is running: <code>sudo systemctl status caddy</code></li> <li>Verify security group allows ports 80 and 443</li> <li>Check local connectivity: <code>curl http://localhost:3000</code></li> </ol>"},{"location":"nest/server_setup/#ssl-certificate-issues","title":"SSL Certificate issues","text":"<p>Caddy automatically obtains SSL certificates. If there are issues:</p> <pre><code>sudo journalctl -u caddy -n 50\n</code></pre> <p>Ensure your domain points to the correct IP address.</p>"},{"location":"nest/server_setup/#security-recommendations","title":"Security Recommendations","text":"<ol> <li>Keep your PEM key secure and never commit it to version control</li> <li>Use environment variables for sensitive data</li> <li>Regularly update packages: <code>npm audit fix</code></li> <li>Configure proper firewall rules (UFW or Security Groups)</li> <li>Set up regular backups of your data</li> <li>Monitor application logs for suspicious activity</li> <li>Use strong passwords for database and other services</li> </ol>"},{"location":"nest/server_setup/#additional-resources","title":"Additional Resources","text":"<ul> <li>PM2 Documentation</li> <li>Caddy Documentation</li> <li>NestJS Documentation</li> </ul>"},{"location":"react/state_management/","title":"React State Management: Zero to Hero","text":"<p>A comprehensive guide to mastering state management in React, from basic concepts to advanced patterns.</p>"},{"location":"react/state_management/#1-state-concept","title":"1. State Concept","text":"<p>State represents data that changes over time in your application. When state changes, React re-renders the component to reflect the new data.</p> <p>Key principles:</p> <ul> <li>State is local to a component by default</li> <li>Changing state triggers a re-render</li> <li>State updates may be asynchronous</li> <li>Never mutate state directly</li> </ul> <pre><code>// State is data that changes\n// When count changes, the UI updates\nfunction Example() {\n  const [count, setCount] = useState(0);\n\n  return &lt;div&gt;{count}&lt;/div&gt;; // Shows current count\n}\n</code></pre>"},{"location":"react/state_management/#2-usestate","title":"2. useState","text":"<p>The fundamental hook for managing state in functional components.</p> <pre><code>import { useState } from \"react\";\n\nfunction Counter() {\n  // Declare state variable with initial value\n  const [count, setCount] = useState(0);\n\n  return (\n    &lt;div&gt;\n      &lt;p&gt;Count: {count}&lt;/p&gt;\n      &lt;button onClick={() =&gt; setCount(count + 1)}&gt;Increment&lt;/button&gt;\n      &lt;button onClick={() =&gt; setCount((c) =&gt; c - 1)}&gt;\n        Decrement (functional update)\n      &lt;/button&gt;\n    &lt;/div&gt;\n  );\n}\n</code></pre> <p>Advanced useState patterns:</p> <pre><code>// Complex state object\nconst [user, setUser] = useState({\n  name: \"Alice\",\n  age: 25,\n  email: \"alice@example.com\",\n});\n\n// Update nested state (always create new object)\nconst updateEmail = (newEmail) =&gt; {\n  setUser((prev) =&gt; ({\n    ...prev,\n    email: newEmail,\n  }));\n};\n\n// Lazy initialization (runs only once)\nconst [data, setData] = useState(() =&gt; {\n  return expensiveComputation();\n});\n\n// Array state\nconst [items, setItems] = useState([]);\n\nconst addItem = (item) =&gt; {\n  setItems((prev) =&gt; [...prev, item]);\n};\n\nconst removeItem = (id) =&gt; {\n  setItems((prev) =&gt; prev.filter((item) =&gt; item.id !== id));\n};\n</code></pre>"},{"location":"react/state_management/#3-controlled-components","title":"3. Controlled Components","text":"<p>Components where form data is handled by React state rather than the DOM.</p> <pre><code>function LoginForm() {\n  const [email, setEmail] = useState(\"\");\n  const [password, setPassword] = useState(\"\");\n\n  const handleSubmit = (e) =&gt; {\n    e.preventDefault();\n    console.log({ email, password });\n  };\n\n  return (\n    &lt;form onSubmit={handleSubmit}&gt;\n      &lt;input\n        type=\"email\"\n        value={email}\n        onChange={(e) =&gt; setEmail(e.target.value)}\n        placeholder=\"Email\"\n      /&gt;\n      &lt;input\n        type=\"password\"\n        value={password}\n        onChange={(e) =&gt; setPassword(e.target.value)}\n        placeholder=\"Password\"\n      /&gt;\n      &lt;button type=\"submit\"&gt;Login&lt;/button&gt;\n    &lt;/form&gt;\n  );\n}\n</code></pre> <p>Multiple inputs pattern:</p> <pre><code>function FormWithMultipleInputs() {\n  const [formData, setFormData] = useState({\n    username: \"\",\n    email: \"\",\n    age: \"\",\n  });\n\n  const handleChange = (e) =&gt; {\n    const { name, value } = e.target;\n    setFormData((prev) =&gt; ({\n      ...prev,\n      [name]: value,\n    }));\n  };\n\n  return (\n    &lt;form&gt;\n      &lt;input\n        name=\"username\"\n        value={formData.username}\n        onChange={handleChange}\n      /&gt;\n      &lt;input name=\"email\" value={formData.email} onChange={handleChange} /&gt;\n      &lt;input name=\"age\" value={formData.age} onChange={handleChange} /&gt;\n    &lt;/form&gt;\n  );\n}\n</code></pre>"},{"location":"react/state_management/#4-lifting-state-up","title":"4. Lifting State Up","text":"<p>Moving state to the closest common ancestor when multiple components need to share it.</p> <pre><code>// Parent component holds shared state\nfunction ParentComponent() {\n  const [sharedValue, setSharedValue] = useState(\"\");\n\n  return (\n    &lt;div&gt;\n      &lt;ChildA value={sharedValue} onChange={setSharedValue} /&gt;\n      &lt;ChildB value={sharedValue} /&gt;\n    &lt;/div&gt;\n  );\n}\n\n// Child A can modify the state\nfunction ChildA({ value, onChange }) {\n  return &lt;input value={value} onChange={(e) =&gt; onChange(e.target.value)} /&gt;;\n}\n\n// Child B reads the same state\nfunction ChildB({ value }) {\n  return &lt;p&gt;You typed: {value}&lt;/p&gt;;\n}\n</code></pre> <p>Real-world example:</p> <pre><code>function TemperatureConverter() {\n  const [celsius, setCelsius] = useState(\"\");\n\n  const fahrenheit = celsius ? ((celsius * 9) / 5 + 32).toFixed(2) : \"\";\n\n  return (\n    &lt;div&gt;\n      &lt;TemperatureInput\n        scale=\"Celsius\"\n        temperature={celsius}\n        onTemperatureChange={setCelsius}\n      /&gt;\n      &lt;TemperatureInput\n        scale=\"Fahrenheit\"\n        temperature={fahrenheit}\n        onTemperatureChange={(f) =&gt; setCelsius((((f - 32) * 5) / 9).toFixed(2))}\n      /&gt;\n    &lt;/div&gt;\n  );\n}\n\nfunction TemperatureInput({ scale, temperature, onTemperatureChange }) {\n  return (\n    &lt;fieldset&gt;\n      &lt;legend&gt;{scale}&lt;/legend&gt;\n      &lt;input\n        value={temperature}\n        onChange={(e) =&gt; onTemperatureChange(e.target.value)}\n      /&gt;\n    &lt;/fieldset&gt;\n  );\n}\n</code></pre>"},{"location":"react/state_management/#5-prop-drilling-pain","title":"5. Prop Drilling Pain","text":"<p>The problem of passing props through multiple layers of components.</p> <pre><code>// \u274c Bad: Prop drilling through multiple levels\nfunction App() {\n  const [user, setUser] = useState({ name: \"Alice\" });\n\n  return &lt;Layout user={user} setUser={setUser} /&gt;;\n}\n\nfunction Layout({ user, setUser }) {\n  return (\n    &lt;div&gt;\n      &lt;Header user={user} setUser={setUser} /&gt;\n    &lt;/div&gt;\n  );\n}\n\nfunction Header({ user, setUser }) {\n  return (\n    &lt;nav&gt;\n      &lt;UserMenu user={user} setUser={setUser} /&gt;\n    &lt;/nav&gt;\n  );\n}\n\nfunction UserMenu({ user, setUser }) {\n  return (\n    &lt;div&gt;\n      &lt;span&gt;{user.name}&lt;/span&gt;\n      &lt;button onClick={() =&gt; setUser({ name: \"Bob\" })}&gt;Change User&lt;/button&gt;\n    &lt;/div&gt;\n  );\n}\n\n// Problems:\n// 1. Layout and Header don't use user/setUser but must pass them\n// 2. Hard to maintain and refactor\n// 3. Components are tightly coupled\n</code></pre>"},{"location":"react/state_management/#6-context","title":"6. Context","text":"<p>React's built-in solution for sharing state across the component tree without prop drilling.</p> <pre><code>import { createContext, useContext, useState } from \"react\";\n\n// 1. Create context\nconst UserContext = createContext();\n\n// 2. Create provider component\nfunction UserProvider({ children }) {\n  const [user, setUser] = useState({ name: \"Alice\", role: \"admin\" });\n\n  const value = {\n    user,\n    setUser,\n    updateName: (name) =&gt; setUser((prev) =&gt; ({ ...prev, name })),\n  };\n\n  return &lt;UserContext.Provider value={value}&gt;{children}&lt;/UserContext.Provider&gt;;\n}\n\n// 3. Create custom hook for consuming context\nfunction useUser() {\n  const context = useContext(UserContext);\n  if (!context) {\n    throw new Error(\"useUser must be used within UserProvider\");\n  }\n  return context;\n}\n\n// 4. Use in components\nfunction App() {\n  return (\n    &lt;UserProvider&gt;\n      &lt;Layout /&gt;\n    &lt;/UserProvider&gt;\n  );\n}\n\nfunction Layout() {\n  return &lt;Header /&gt;; // No props needed!\n}\n\nfunction Header() {\n  return &lt;UserMenu /&gt;;\n}\n\nfunction UserMenu() {\n  const { user, updateName } = useUser();\n\n  return (\n    &lt;div&gt;\n      &lt;span&gt;{user.name}&lt;/span&gt;\n      &lt;button onClick={() =&gt; updateName(\"Bob\")}&gt;Change Name&lt;/button&gt;\n    &lt;/div&gt;\n  );\n}\n</code></pre> <p>Multiple contexts pattern:</p> <pre><code>function App() {\n  return (\n    &lt;UserProvider&gt;\n      &lt;ThemeProvider&gt;\n        &lt;CartProvider&gt;\n          &lt;Layout /&gt;\n        &lt;/CartProvider&gt;\n      &lt;/ThemeProvider&gt;\n    &lt;/UserProvider&gt;\n  );\n}\n</code></pre> <p>Context with useReducer: Complete Guide</p> <p>When your Context state becomes complex with multiple related values and actions, useReducer provides a more scalable pattern than multiple useState calls. It's like Redux but simpler and built into React.</p> <p>Why useReducer with Context?</p> <ul> <li>Centralizes state logic in one place</li> <li>Makes state updates predictable and testable</li> <li>Easier to track what changes state and how</li> <li>Better for complex state with multiple sub-values</li> <li>Reduces bugs from scattered setState calls</li> </ul> <p>Basic Shopping Cart Example:</p> <pre><code>import { createContext, useContext, useReducer } from \"react\";\n\n// 1. Create Context\nconst CartContext = createContext();\n\n// 2. Define initial state\nconst initialState = {\n  items: [],\n  total: 0,\n  itemCount: 0,\n};\n\n// 3. Create reducer function\nfunction cartReducer(state, action) {\n  switch (action.type) {\n    case \"ADD_ITEM\": {\n      const newItem = action.payload;\n      const existingItem = state.items.find((item) =&gt; item.id === newItem.id);\n\n      if (existingItem) {\n        // Item exists, increase quantity\n        return {\n          ...state,\n          items: state.items.map((item) =&gt;\n            item.id === newItem.id\n              ? { ...item, quantity: item.quantity + 1 }\n              : item\n          ),\n          total: state.total + newItem.price,\n          itemCount: state.itemCount + 1,\n        };\n      } else {\n        // New item\n        return {\n          ...state,\n          items: [...state.items, { ...newItem, quantity: 1 }],\n          total: state.total + newItem.price,\n          itemCount: state.itemCount + 1,\n        };\n      }\n    }\n\n    case \"REMOVE_ITEM\": {\n      const itemId = action.payload;\n      const item = state.items.find((i) =&gt; i.id === itemId);\n\n      if (!item) return state;\n\n      if (item.quantity &gt; 1) {\n        // Decrease quantity\n        return {\n          ...state,\n          items: state.items.map((i) =&gt;\n            i.id === itemId ? { ...i, quantity: i.quantity - 1 } : i\n          ),\n          total: state.total - item.price,\n          itemCount: state.itemCount - 1,\n        };\n      } else {\n        // Remove item completely\n        return {\n          ...state,\n          items: state.items.filter((i) =&gt; i.id !== itemId),\n          total: state.total - item.price,\n          itemCount: state.itemCount - 1,\n        };\n      }\n    }\n\n    case \"UPDATE_QUANTITY\": {\n      const { id, quantity } = action.payload;\n      const item = state.items.find((i) =&gt; i.id === id);\n\n      if (!item || quantity &lt; 0) return state;\n\n      const quantityDiff = quantity - item.quantity;\n\n      if (quantity === 0) {\n        return {\n          ...state,\n          items: state.items.filter((i) =&gt; i.id !== id),\n          total: state.total - item.price * item.quantity,\n          itemCount: state.itemCount - item.quantity,\n        };\n      }\n\n      return {\n        ...state,\n        items: state.items.map((i) =&gt; (i.id === id ? { ...i, quantity } : i)),\n        total: state.total + item.price * quantityDiff,\n        itemCount: state.itemCount + quantityDiff,\n      };\n    }\n\n    case \"CLEAR_CART\":\n      return initialState;\n\n    case \"APPLY_DISCOUNT\": {\n      const discountPercent = action.payload;\n      return {\n        ...state,\n        total: state.total * (1 - discountPercent / 100),\n      };\n    }\n\n    default:\n      throw new Error(`Unhandled action type: ${action.type}`);\n  }\n}\n\n// 4. Create Provider component\nfunction CartProvider({ children }) {\n  const [state, dispatch] = useReducer(cartReducer, initialState);\n\n  // Optional: Create helper functions\n  const addItem = (item) =&gt; {\n    dispatch({ type: \"ADD_ITEM\", payload: item });\n  };\n\n  const removeItem = (id) =&gt; {\n    dispatch({ type: \"REMOVE_ITEM\", payload: id });\n  };\n\n  const updateQuantity = (id, quantity) =&gt; {\n    dispatch({ type: \"UPDATE_QUANTITY\", payload: { id, quantity } });\n  };\n\n  const clearCart = () =&gt; {\n    dispatch({ type: \"CLEAR_CART\" });\n  };\n\n  const applyDiscount = (percent) =&gt; {\n    dispatch({ type: \"APPLY_DISCOUNT\", payload: percent });\n  };\n\n  const value = {\n    state,\n    dispatch,\n    // Helper functions (optional but cleaner to use)\n    addItem,\n    removeItem,\n    updateQuantity,\n    clearCart,\n    applyDiscount,\n  };\n\n  return &lt;CartContext.Provider value={value}&gt;{children}&lt;/CartContext.Provider&gt;;\n}\n\n// 5. Create custom hook\nfunction useCart() {\n  const context = useContext(CartContext);\n  if (context === undefined) {\n    throw new Error(\"useCart must be used within CartProvider\");\n  }\n  return context;\n}\n\n// 6. Export everything\nexport { CartProvider, useCart };\n</code></pre> <p>Using the Cart Context:</p> <pre><code>// App.jsx\nfunction App() {\n  return (\n    &lt;CartProvider&gt;\n      &lt;ShoppingApp /&gt;\n    &lt;/CartProvider&gt;\n  );\n}\n\n// ProductList.jsx\nfunction ProductList() {\n  const { addItem } = useCart();\n\n  const products = [\n    { id: 1, name: \"Laptop\", price: 999 },\n    { id: 2, name: \"Mouse\", price: 29 },\n    { id: 3, name: \"Keyboard\", price: 79 },\n  ];\n\n  return (\n    &lt;div&gt;\n      &lt;h2&gt;Products&lt;/h2&gt;\n      {products.map((product) =&gt; (\n        &lt;div key={product.id}&gt;\n          &lt;h3&gt;{product.name}&lt;/h3&gt;\n          &lt;p&gt;${product.price}&lt;/p&gt;\n          &lt;button onClick={() =&gt; addItem(product)}&gt;Add to Cart&lt;/button&gt;\n        &lt;/div&gt;\n      ))}\n    &lt;/div&gt;\n  );\n}\n\n// Cart.jsx\nfunction Cart() {\n  const { state, removeItem, updateQuantity, clearCart, applyDiscount } =\n    useCart();\n\n  return (\n    &lt;div&gt;\n      &lt;h2&gt;Shopping Cart&lt;/h2&gt;\n      &lt;p&gt;Items: {state.itemCount}&lt;/p&gt;\n      &lt;p&gt;Total: ${state.total.toFixed(2)}&lt;/p&gt;\n\n      {state.items.map((item) =&gt; (\n        &lt;div key={item.id}&gt;\n          &lt;h3&gt;{item.name}&lt;/h3&gt;\n          &lt;p&gt;Price: ${item.price}&lt;/p&gt;\n          &lt;p&gt;Quantity: {item.quantity}&lt;/p&gt;\n          &lt;input\n            type=\"number\"\n            value={item.quantity}\n            onChange={(e) =&gt; updateQuantity(item.id, parseInt(e.target.value))}\n            min=\"0\"\n          /&gt;\n          &lt;button onClick={() =&gt; removeItem(item.id)}&gt;Remove One&lt;/button&gt;\n        &lt;/div&gt;\n      ))}\n\n      &lt;button onClick={() =&gt; applyDiscount(10)}&gt;Apply 10% Discount&lt;/button&gt;\n      &lt;button onClick={clearCart}&gt;Clear Cart&lt;/button&gt;\n    &lt;/div&gt;\n  );\n}\n</code></pre> <p>Advanced: Authentication System with useReducer</p> <p>A real-world authentication flow with loading states, errors, and user data:</p> <pre><code>import { createContext, useContext, useReducer, useEffect } from \"react\";\n\nconst AuthContext = createContext();\n\nconst initialState = {\n  user: null,\n  token: null,\n  isAuthenticated: false,\n  isLoading: true,\n  error: null,\n};\n\nfunction authReducer(state, action) {\n  switch (action.type) {\n    case \"AUTH_START\":\n      return {\n        ...state,\n        isLoading: true,\n        error: null,\n      };\n\n    case \"LOGIN_SUCCESS\":\n      return {\n        ...state,\n        user: action.payload.user,\n        token: action.payload.token,\n        isAuthenticated: true,\n        isLoading: false,\n        error: null,\n      };\n\n    case \"LOGIN_FAILURE\":\n      return {\n        ...state,\n        user: null,\n        token: null,\n        isAuthenticated: false,\n        isLoading: false,\n        error: action.payload,\n      };\n\n    case \"LOGOUT\":\n      return {\n        ...initialState,\n        isLoading: false,\n      };\n\n    case \"UPDATE_USER\":\n      return {\n        ...state,\n        user: { ...state.user, ...action.payload },\n      };\n\n    case \"RESTORE_SESSION\":\n      return {\n        ...state,\n        user: action.payload.user,\n        token: action.payload.token,\n        isAuthenticated: true,\n        isLoading: false,\n      };\n\n    case \"CLEAR_ERROR\":\n      return {\n        ...state,\n        error: null,\n      };\n\n    default:\n      return state;\n  }\n}\n\nfunction AuthProvider({ children }) {\n  const [state, dispatch] = useReducer(authReducer, initialState);\n\n  // Restore session on mount\n  useEffect(() =&gt; {\n    const token = localStorage.getItem(\"token\");\n    const user = localStorage.getItem(\"user\");\n\n    if (token &amp;&amp; user) {\n      dispatch({\n        type: \"RESTORE_SESSION\",\n        payload: { token, user: JSON.parse(user) },\n      });\n    } else {\n      dispatch({ type: \"LOGOUT\" });\n    }\n  }, []);\n\n  // Login function\n  const login = async (email, password) =&gt; {\n    dispatch({ type: \"AUTH_START\" });\n\n    try {\n      const response = await fetch(\"/api/login\", {\n        method: \"POST\",\n        headers: { \"Content-Type\": \"application/json\" },\n        body: JSON.stringify({ email, password }),\n      });\n\n      if (!response.ok) {\n        throw new Error(\"Invalid credentials\");\n      }\n\n      const data = await response.json();\n\n      // Save to localStorage\n      localStorage.setItem(\"token\", data.token);\n      localStorage.setItem(\"user\", JSON.stringify(data.user));\n\n      dispatch({\n        type: \"LOGIN_SUCCESS\",\n        payload: { user: data.user, token: data.token },\n      });\n\n      return { success: true };\n    } catch (error) {\n      dispatch({\n        type: \"LOGIN_FAILURE\",\n        payload: error.message,\n      });\n      return { success: false, error: error.message };\n    }\n  };\n\n  // Register function\n  const register = async (userData) =&gt; {\n    dispatch({ type: \"AUTH_START\" });\n\n    try {\n      const response = await fetch(\"/api/register\", {\n        method: \"POST\",\n        headers: { \"Content-Type\": \"application/json\" },\n        body: JSON.stringify(userData),\n      });\n\n      if (!response.ok) {\n        throw new Error(\"Registration failed\");\n      }\n\n      const data = await response.json();\n\n      localStorage.setItem(\"token\", data.token);\n      localStorage.setItem(\"user\", JSON.stringify(data.user));\n\n      dispatch({\n        type: \"LOGIN_SUCCESS\",\n        payload: { user: data.user, token: data.token },\n      });\n\n      return { success: true };\n    } catch (error) {\n      dispatch({\n        type: \"LOGIN_FAILURE\",\n        payload: error.message,\n      });\n      return { success: false, error: error.message };\n    }\n  };\n\n  // Logout function\n  const logout = () =&gt; {\n    localStorage.removeItem(\"token\");\n    localStorage.removeItem(\"user\");\n    dispatch({ type: \"LOGOUT\" });\n  };\n\n  // Update user profile\n  const updateUser = (userData) =&gt; {\n    const updatedUser = { ...state.user, ...userData };\n    localStorage.setItem(\"user\", JSON.stringify(updatedUser));\n    dispatch({ type: \"UPDATE_USER\", payload: userData });\n  };\n\n  const clearError = () =&gt; {\n    dispatch({ type: \"CLEAR_ERROR\" });\n  };\n\n  const value = {\n    ...state,\n    login,\n    register,\n    logout,\n    updateUser,\n    clearError,\n  };\n\n  return &lt;AuthContext.Provider value={value}&gt;{children}&lt;/AuthContext.Provider&gt;;\n}\n\nfunction useAuth() {\n  const context = useContext(AuthContext);\n  if (context === undefined) {\n    throw new Error(\"useAuth must be used within AuthProvider\");\n  }\n  return context;\n}\n\nexport { AuthProvider, useAuth };\n</code></pre> <p>Using the Auth Context:</p> <pre><code>// LoginForm.jsx\nfunction LoginForm() {\n  const { login, isLoading, error, clearError } = useAuth();\n  const [email, setEmail] = useState(\"\");\n  const [password, setPassword] = useState(\"\");\n\n  useEffect(() =&gt; {\n    return () =&gt; clearError(); // Clear error on unmount\n  }, [clearError]);\n\n  const handleSubmit = async (e) =&gt; {\n    e.preventDefault();\n    const result = await login(email, password);\n\n    if (result.success) {\n      // Redirect or show success message\n      console.log(\"Login successful!\");\n    }\n  };\n\n  return (\n    &lt;form onSubmit={handleSubmit}&gt;\n      {error &amp;&amp; &lt;div className=\"error\"&gt;{error}&lt;/div&gt;}\n\n      &lt;input\n        type=\"email\"\n        value={email}\n        onChange={(e) =&gt; setEmail(e.target.value)}\n        disabled={isLoading}\n      /&gt;\n\n      &lt;input\n        type=\"password\"\n        value={password}\n        onChange={(e) =&gt; setPassword(e.target.value)}\n        disabled={isLoading}\n      /&gt;\n\n      &lt;button type=\"submit\" disabled={isLoading}&gt;\n        {isLoading ? \"Logging in...\" : \"Login\"}\n      &lt;/button&gt;\n    &lt;/form&gt;\n  );\n}\n\n// ProtectedRoute.jsx\nfunction ProtectedRoute({ children }) {\n  const { isAuthenticated, isLoading } = useAuth();\n\n  if (isLoading) {\n    return &lt;div&gt;Loading...&lt;/div&gt;;\n  }\n\n  if (!isAuthenticated) {\n    return &lt;Navigate to=\"/login\" /&gt;;\n  }\n\n  return children;\n}\n\n// UserProfile.jsx\nfunction UserProfile() {\n  const { user, updateUser, logout } = useAuth();\n  const [name, setName] = useState(user?.name || \"\");\n\n  const handleUpdate = () =&gt; {\n    updateUser({ name });\n  };\n\n  return (\n    &lt;div&gt;\n      &lt;h2&gt;Welcome, {user?.name}&lt;/h2&gt;\n      &lt;input value={name} onChange={(e) =&gt; setName(e.target.value)} /&gt;\n      &lt;button onClick={handleUpdate}&gt;Update Name&lt;/button&gt;\n      &lt;button onClick={logout}&gt;Logout&lt;/button&gt;\n    &lt;/div&gt;\n  );\n}\n</code></pre> <p>Key Benefits of useReducer with Context:</p> <ol> <li>Centralized Logic: All state transitions in one place</li> <li>Predictable: Easy to understand what changes state</li> <li>Testable: Reducers are pure functions, easy to test</li> <li>Scalable: Add new actions without cluttering components</li> <li>Debug-friendly: Clear action types help track state changes</li> <li>Type-safe: Works great with TypeScript for action types</li> </ol>"},{"location":"react/state_management/#7-zustand","title":"7. Zustand","text":"<p>A lightweight, modern state management library with minimal boilerplate.</p> <p>Installation:</p> <pre><code>npm install zustand\n</code></pre> <p>Basic usage:</p> <pre><code>import { create } from \"zustand\";\n\n// Create store\nconst useStore = create((set) =&gt; ({\n  // State\n  count: 0,\n  user: null,\n\n  // Actions\n  increment: () =&gt; set((state) =&gt; ({ count: state.count + 1 })),\n  decrement: () =&gt; set((state) =&gt; ({ count: state.count - 1 })),\n  setUser: (user) =&gt; set({ user }),\n  reset: () =&gt; set({ count: 0, user: null }),\n}));\n\n// Use in components\nfunction Counter() {\n  const count = useStore((state) =&gt; state.count);\n  const increment = useStore((state) =&gt; state.increment);\n\n  return (\n    &lt;div&gt;\n      &lt;p&gt;Count: {count}&lt;/p&gt;\n      &lt;button onClick={increment}&gt;Increment&lt;/button&gt;\n    &lt;/div&gt;\n  );\n}\n\nfunction UserProfile() {\n  const user = useStore((state) =&gt; state.user);\n  const setUser = useStore((state) =&gt; state.setUser);\n\n  return &lt;div&gt;{user ? &lt;p&gt;Hello, {user.name}&lt;/p&gt; : &lt;p&gt;Not logged in&lt;/p&gt;}&lt;/div&gt;;\n}\n</code></pre> <p>Advanced Zustand Patterns: Complete Guide</p> <p>Zustand shines when you need advanced features while keeping your code simple. Let's explore powerful patterns for real-world applications.</p>"},{"location":"react/state_management/#1-async-actions-with-loading-states","title":"1. Async Actions with Loading States","text":"<p>Handle API calls with proper loading and error states:</p> <pre><code>import { create } from \"zustand\";\n\nconst usePostStore = create((set, get) =&gt; ({\n  posts: [],\n  loading: false,\n  error: null,\n  selectedPost: null,\n\n  // Fetch all posts\n  fetchPosts: async () =&gt; {\n    set({ loading: true, error: null });\n    try {\n      const res = await fetch(\"https://api.example.com/posts\");\n      if (!res.ok) throw new Error(\"Failed to fetch posts\");\n      const posts = await res.json();\n      set({ posts, loading: false });\n    } catch (error) {\n      set({ error: error.message, loading: false });\n    }\n  },\n\n  // Fetch single post\n  fetchPost: async (id) =&gt; {\n    set({ loading: true, error: null });\n    try {\n      const res = await fetch(`https://api.example.com/posts/${id}`);\n      if (!res.ok) throw new Error(\"Post not found\");\n      const post = await res.json();\n      set({ selectedPost: post, loading: false });\n    } catch (error) {\n      set({ error: error.message, loading: false });\n    }\n  },\n\n  // Create post with optimistic update\n  createPost: async (newPost) =&gt; {\n    const tempId = `temp-${Date.now()}`;\n    const optimisticPost = { ...newPost, id: tempId };\n\n    // Optimistically add to UI\n    set((state) =&gt; ({\n      posts: [...state.posts, optimisticPost],\n    }));\n\n    try {\n      const res = await fetch(\"https://api.example.com/posts\", {\n        method: \"POST\",\n        headers: { \"Content-Type\": \"application/json\" },\n        body: JSON.stringify(newPost),\n      });\n\n      if (!res.ok) throw new Error(\"Failed to create post\");\n      const createdPost = await res.json();\n\n      // Replace temp post with real one\n      set((state) =&gt; ({\n        posts: state.posts.map((p) =&gt; (p.id === tempId ? createdPost : p)),\n      }));\n    } catch (error) {\n      // Rollback on error\n      set((state) =&gt; ({\n        posts: state.posts.filter((p) =&gt; p.id !== tempId),\n        error: error.message,\n      }));\n    }\n  },\n\n  // Delete with optimistic update\n  deletePost: async (id) =&gt; {\n    const previousPosts = get().posts;\n\n    // Optimistically remove\n    set((state) =&gt; ({\n      posts: state.posts.filter((p) =&gt; p.id !== id),\n    }));\n\n    try {\n      const res = await fetch(`https://api.example.com/posts/${id}`, {\n        method: \"DELETE\",\n      });\n\n      if (!res.ok) throw new Error(\"Failed to delete\");\n    } catch (error) {\n      // Rollback on error\n      set({\n        posts: previousPosts,\n        error: error.message,\n      });\n    }\n  },\n\n  // Clear error\n  clearError: () =&gt; set({ error: null }),\n}));\n\n// Using in component\nfunction PostsList() {\n  const { posts, loading, error, fetchPosts, deletePost, clearError } =\n    usePostStore();\n\n  useEffect(() =&gt; {\n    fetchPosts();\n  }, [fetchPosts]);\n\n  if (loading) return &lt;div&gt;Loading...&lt;/div&gt;;\n  if (error)\n    return (\n      &lt;div&gt;\n        Error: {error} &lt;button onClick={clearError}&gt;Dismiss&lt;/button&gt;\n      &lt;/div&gt;\n    );\n\n  return (\n    &lt;div&gt;\n      {posts.map((post) =&gt; (\n        &lt;div key={post.id}&gt;\n          &lt;h3&gt;{post.title}&lt;/h3&gt;\n          &lt;button onClick={() =&gt; deletePost(post.id)}&gt;Delete&lt;/button&gt;\n        &lt;/div&gt;\n      ))}\n    &lt;/div&gt;\n  );\n}\n</code></pre>"},{"location":"react/state_management/#2-slices-pattern-modular-store-architecture","title":"2. Slices Pattern (Modular Store Architecture)","text":"<p>Organize large stores by splitting into feature slices:</p> <pre><code>import { create } from \"zustand\";\n\n// User slice\nconst createUserSlice = (set, get) =&gt; ({\n  user: null,\n  isAuthenticated: false,\n\n  setUser: (user) =&gt;\n    set({\n      user,\n      isAuthenticated: !!user,\n    }),\n\n  updateUser: (updates) =&gt;\n    set((state) =&gt; ({\n      user: state.user ? { ...state.user, ...updates } : null,\n    })),\n\n  logout: () =&gt;\n    set({\n      user: null,\n      isAuthenticated: false,\n    }),\n\n  // Access other slices with get()\n  getUserCartTotal: () =&gt; {\n    const { user } = get();\n    const { items } = get();\n    return user ? items.reduce((sum, item) =&gt; sum + item.price, 0) : 0;\n  },\n});\n\n// Cart slice\nconst createCartSlice = (set, get) =&gt; ({\n  items: [],\n\n  addItem: (item) =&gt;\n    set((state) =&gt; {\n      const existing = state.items.find((i) =&gt; i.id === item.id);\n      if (existing) {\n        return {\n          items: state.items.map((i) =&gt;\n            i.id === item.id ? { ...i, quantity: i.quantity + 1 } : i\n          ),\n        };\n      }\n      return { items: [...state.items, { ...item, quantity: 1 }] };\n    }),\n\n  removeItem: (id) =&gt;\n    set((state) =&gt; ({\n      items: state.items.filter((i) =&gt; i.id !== id),\n    })),\n\n  clearCart: () =&gt; set({ items: [] }),\n\n  getTotalPrice: () =&gt; {\n    const { items } = get();\n    return items.reduce((sum, item) =&gt; sum + item.price * item.quantity, 0);\n  },\n\n  getTotalItems: () =&gt; {\n    const { items } = get();\n    return items.reduce((sum, item) =&gt; sum + item.quantity, 0);\n  },\n});\n\n// UI slice\nconst createUISlice = (set) =&gt; ({\n  theme: \"light\",\n  sidebarOpen: false,\n  notifications: [],\n\n  toggleTheme: () =&gt;\n    set((state) =&gt; ({\n      theme: state.theme === \"light\" ? \"dark\" : \"light\",\n    })),\n\n  toggleSidebar: () =&gt;\n    set((state) =&gt; ({\n      sidebarOpen: !state.sidebarOpen,\n    })),\n\n  addNotification: (message) =&gt;\n    set((state) =&gt; ({\n      notifications: [\n        ...state.notifications,\n        {\n          id: Date.now(),\n          message,\n          timestamp: new Date(),\n        },\n      ],\n    })),\n\n  removeNotification: (id) =&gt;\n    set((state) =&gt; ({\n      notifications: state.notifications.filter((n) =&gt; n.id !== id),\n    })),\n});\n\n// Combine all slices\nconst useStore = create((set, get) =&gt; ({\n  ...createUserSlice(set, get),\n  ...createCartSlice(set, get),\n  ...createUISlice(set, get),\n}));\n\n// Usage in components\nfunction CartSummary() {\n  const items = useStore((state) =&gt; state.items);\n  const getTotalPrice = useStore((state) =&gt; state.getTotalPrice);\n  const getTotalItems = useStore((state) =&gt; state.getTotalItems);\n\n  return (\n    &lt;div&gt;\n      &lt;p&gt;Items: {getTotalItems()}&lt;/p&gt;\n      &lt;p&gt;Total: ${getTotalPrice().toFixed(2)}&lt;/p&gt;\n    &lt;/div&gt;\n  );\n}\n\nfunction UserHeader() {\n  const user = useStore((state) =&gt; state.user);\n  const logout = useStore((state) =&gt; state.logout);\n  const getUserCartTotal = useStore((state) =&gt; state.getUserCartTotal);\n\n  return (\n    &lt;div&gt;\n      {user ? (\n        &lt;&gt;\n          &lt;span&gt;Welcome, {user.name}!&lt;/span&gt;\n          &lt;span&gt;Cart Total: ${getUserCartTotal().toFixed(2)}&lt;/span&gt;\n          &lt;button onClick={logout}&gt;Logout&lt;/button&gt;\n        &lt;/&gt;\n      ) : (\n        &lt;span&gt;Please login&lt;/span&gt;\n      )}\n    &lt;/div&gt;\n  );\n}\n</code></pre>"},{"location":"react/state_management/#3-persist-middleware-localstoragesessionstorage","title":"3. Persist Middleware (localStorage/sessionStorage)","text":"<p>Save state to browser storage automatically:</p> <pre><code>import { create } from \"zustand\";\nimport { persist, createJSONStorage } from \"zustand/middleware\";\n\n// Basic persistence\nconst useAuthStore = create(\n  persist(\n    (set) =&gt; ({\n      user: null,\n      token: null,\n\n      login: (user, token) =&gt; set({ user, token }),\n      logout: () =&gt; set({ user: null, token: null }),\n    }),\n    {\n      name: \"auth-storage\", // localStorage key\n    }\n  )\n);\n\n// Advanced persistence with custom storage\nconst useSettingsStore = create(\n  persist(\n    (set) =&gt; ({\n      theme: \"light\",\n      language: \"en\",\n      notifications: true,\n\n      setTheme: (theme) =&gt; set({ theme }),\n      setLanguage: (language) =&gt; set({ language }),\n      toggleNotifications: () =&gt;\n        set((state) =&gt; ({\n          notifications: !state.notifications,\n        })),\n    }),\n    {\n      name: \"app-settings\",\n      storage: createJSONStorage(() =&gt; sessionStorage), // Use sessionStorage\n\n      // Partial persistence - only save specific keys\n      partialize: (state) =&gt; ({\n        theme: state.theme,\n        language: state.language,\n        // notifications excluded - won't persist\n      }),\n\n      // Migrate old versions\n      version: 1,\n      migrate: (persistedState, version) =&gt; {\n        if (version === 0) {\n          // Migrate from v0 to v1\n          return {\n            ...persistedState,\n            language: persistedState.lang || \"en\",\n          };\n        }\n        return persistedState;\n      },\n\n      // Custom serialization\n      serialize: (state) =&gt; JSON.stringify(state),\n      deserialize: (str) =&gt; JSON.parse(str),\n\n      // Hydration callback\n      onRehydrateStorage: (state) =&gt; {\n        console.log(\"Hydration starts\");\n\n        return (state, error) =&gt; {\n          if (error) {\n            console.log(\"Error during hydration\", error);\n          } else {\n            console.log(\"Hydration finished\", state);\n          }\n        };\n      },\n    }\n  )\n);\n\n// Multi-tab sync example\nconst useCartStore = create(\n  persist(\n    (set, get) =&gt; ({\n      items: [],\n\n      addItem: (item) =&gt;\n        set((state) =&gt; ({\n          items: [...state.items, item],\n        })),\n\n      // Will sync across tabs automatically\n      clearCart: () =&gt; set({ items: [] }),\n    }),\n    {\n      name: \"shopping-cart\",\n      // Syncs across tabs/windows\n      storage: createJSONStorage(() =&gt; localStorage),\n    }\n  )\n);\n</code></pre>"},{"location":"react/state_management/#4-immer-middleware-mutable-syntax","title":"4. Immer Middleware (Mutable Syntax)","text":"<p>Write simpler updates with direct mutations (Immer handles immutability):</p> <pre><code>import { create } from \"zustand\";\nimport { immer } from \"zustand/middleware/immer\";\n\nconst useStore = create(\n  immer((set) =&gt; ({\n    user: {\n      name: \"Alice\",\n      profile: {\n        bio: \"Developer\",\n        settings: {\n          theme: \"dark\",\n          notifications: {\n            email: true,\n            push: false,\n          },\n        },\n      },\n    },\n\n    // Without Immer - complex immutable update\n    // updateThemeOldWay: (theme) =&gt; set((state) =&gt; ({\n    //   user: {\n    //     ...state.user,\n    //     profile: {\n    //       ...state.user.profile,\n    //       settings: {\n    //         ...state.user.profile.settings,\n    //         theme\n    //       }\n    //     }\n    //   }\n    // })),\n\n    // With Immer - simple mutation\n    updateTheme: (theme) =&gt;\n      set((state) =&gt; {\n        state.user.profile.settings.theme = theme;\n      }),\n\n    toggleEmailNotifications: () =&gt;\n      set((state) =&gt; {\n        state.user.profile.settings.notifications.email =\n          !state.user.profile.settings.notifications.email;\n      }),\n\n    updateBio: (bio) =&gt;\n      set((state) =&gt; {\n        state.user.profile.bio = bio;\n      }),\n\n    // Complex nested array mutations\n    posts: [],\n\n    addPost: (post) =&gt;\n      set((state) =&gt; {\n        state.posts.push(post);\n      }),\n\n    updatePost: (id, updates) =&gt;\n      set((state) =&gt; {\n        const post = state.posts.find((p) =&gt; p.id === id);\n        if (post) {\n          Object.assign(post, updates);\n        }\n      }),\n\n    deletePost: (id) =&gt;\n      set((state) =&gt; {\n        const index = state.posts.findIndex((p) =&gt; p.id === id);\n        if (index !== -1) {\n          state.posts.splice(index, 1);\n        }\n      }),\n  }))\n);\n\n// Combining Immer with Persist\nconst useTodoStore = create(\n  persist(\n    immer((set) =&gt; ({\n      todos: [],\n\n      addTodo: (text) =&gt;\n        set((state) =&gt; {\n          state.todos.push({\n            id: Date.now(),\n            text,\n            completed: false,\n          });\n        }),\n\n      toggleTodo: (id) =&gt;\n        set((state) =&gt; {\n          const todo = state.todos.find((t) =&gt; t.id === id);\n          if (todo) {\n            todo.completed = !todo.completed;\n          }\n        }),\n\n      deleteTodo: (id) =&gt;\n        set((state) =&gt; {\n          const index = state.todos.findIndex((t) =&gt; t.id === id);\n          if (index !== -1) {\n            state.todos.splice(index, 1);\n          }\n        }),\n    })),\n    {\n      name: \"todos-storage\",\n    }\n  )\n);\n</code></pre>"},{"location":"react/state_management/#5-subscriptions-external-updates","title":"5. Subscriptions &amp; External Updates","text":"<p>React to store changes outside components:</p> <pre><code>import { create } from \"zustand\";\nimport { subscribeWithSelector } from \"zustand/middleware\";\n\nconst useStore = create(\n  subscribeWithSelector((set) =&gt; ({\n    count: 0,\n    user: null,\n    theme: \"light\",\n\n    increment: () =&gt; set((state) =&gt; ({ count: state.count + 1 })),\n    setUser: (user) =&gt; set({ user }),\n    setTheme: (theme) =&gt; set({ theme }),\n  }))\n);\n\n// Subscribe to specific state changes\nconst unsubCount = useStore.subscribe(\n  (state) =&gt; state.count,\n  (count, prevCount) =&gt; {\n    console.log(\"Count changed from\", prevCount, \"to\", count);\n\n    // Trigger side effects\n    if (count &gt; 10) {\n      console.log(\"Count exceeded 10!\");\n    }\n  }\n);\n\n// Subscribe to user login/logout\nconst unsubUser = useStore.subscribe(\n  (state) =&gt; state.user,\n  (user, prevUser) =&gt; {\n    if (user &amp;&amp; !prevUser) {\n      console.log(\"User logged in:\", user.name);\n      // Track analytics\n      // analytics.track('user_login', { userId: user.id });\n    } else if (!user &amp;&amp; prevUser) {\n      console.log(\"User logged out\");\n      // Clear user data\n    }\n  }\n);\n\n// Subscribe to theme changes and update document\nuseStore.subscribe(\n  (state) =&gt; state.theme,\n  (theme) =&gt; {\n    document.documentElement.setAttribute(\"data-theme\", theme);\n  }\n);\n\n// Cleanup subscriptions\n// unsubCount();\n// unsubUser();\n\n// Subscribe to entire store\nconst unsubAll = useStore.subscribe((state) =&gt; {\n  console.log(\"State changed:\", state);\n});\n</code></pre>"},{"location":"react/state_management/#6-devtools-integration","title":"6. DevTools Integration","text":"<p>Debug your Zustand store like Redux:</p> <pre><code>import { create } from \"zustand\";\nimport { devtools } from \"zustand/middleware\";\n\nconst useStore = create(\n  devtools(\n    (set) =&gt; ({\n      count: 0,\n      user: null,\n\n      increment: () =&gt;\n        set(\n          (state) =&gt; ({ count: state.count + 1 }),\n          false, // don't replace state\n          \"increment\" // action name in devtools\n        ),\n\n      setUser: (user) =&gt;\n        set(\n          { user },\n          false,\n          { type: \"setUser\", user } // detailed action\n        ),\n    }),\n    {\n      name: \"MyAppStore\", // Store name in devtools\n      enabled: process.env.NODE_ENV === \"development\",\n    }\n  )\n);\n</code></pre>"},{"location":"react/state_management/#7-computed-values-selectors","title":"7. Computed Values (Selectors)","text":"<p>Create derived state efficiently:</p> <pre><code>import { create } from \"zustand\";\n\nconst useStore = create((set, get) =&gt; ({\n  items: [],\n  taxRate: 0.1,\n\n  addItem: (item) =&gt;\n    set((state) =&gt; ({\n      items: [...state.items, item],\n    })),\n\n  // Computed values as functions\n  getSubtotal: () =&gt; {\n    const { items } = get();\n    return items.reduce((sum, item) =&gt; sum + item.price * item.quantity, 0);\n  },\n\n  getTax: () =&gt; {\n    const { getSubtotal, taxRate } = get();\n    return getSubtotal() * taxRate;\n  },\n\n  getTotal: () =&gt; {\n    const { getSubtotal, getTax } = get();\n    return getSubtotal() + getTax();\n  },\n}));\n\n// Use in component\nfunction CartTotal() {\n  const getSubtotal = useStore((state) =&gt; state.getSubtotal);\n  const getTax = useStore((state) =&gt; state.getTax);\n  const getTotal = useStore((state) =&gt; state.getTotal);\n\n  return (\n    &lt;div&gt;\n      &lt;p&gt;Subtotal: ${getSubtotal().toFixed(2)}&lt;/p&gt;\n      &lt;p&gt;Tax: ${getTax().toFixed(2)}&lt;/p&gt;\n      &lt;p&gt;Total: ${getTotal().toFixed(2)}&lt;/p&gt;\n    &lt;/div&gt;\n  );\n}\n\n// Alternative: Selector pattern with useMemo in component\nfunction CartTotalOptimized() {\n  const items = useStore((state) =&gt; state.items);\n  const taxRate = useStore((state) =&gt; state.taxRate);\n\n  const subtotal = useMemo(\n    () =&gt; items.reduce((sum, item) =&gt; sum + item.price * item.quantity, 0),\n    [items]\n  );\n\n  const tax = subtotal * taxRate;\n  const total = subtotal + tax;\n\n  return (\n    &lt;div&gt;\n      &lt;p&gt;Subtotal: ${subtotal.toFixed(2)}&lt;/p&gt;\n      &lt;p&gt;Tax: ${tax.toFixed(2)}&lt;/p&gt;\n      &lt;p&gt;Total: ${total.toFixed(2)}&lt;/p&gt;\n    &lt;/div&gt;\n  );\n}\n</code></pre>"},{"location":"react/state_management/#8-combining-multiple-middlewares","title":"8. Combining Multiple Middlewares","text":"<p>Stack middlewares for maximum power:</p> <pre><code>import { create } from \"zustand\";\nimport { persist, createJSONStorage } from \"zustand/middleware\";\nimport { immer } from \"zustand/middleware/immer\";\nimport { devtools } from \"zustand/middleware\";\n\nconst useStore = create(\n  devtools(\n    persist(\n      immer((set) =&gt; ({\n        user: null,\n        items: [],\n\n        login: (user) =&gt;\n          set((state) =&gt; {\n            state.user = user;\n          }),\n\n        addItem: (item) =&gt;\n          set((state) =&gt; {\n            state.items.push(item);\n          }),\n      })),\n      {\n        name: \"app-storage\",\n        storage: createJSONStorage(() =&gt; localStorage),\n      }\n    ),\n    { name: \"MyStore\" }\n  )\n);\n</code></pre> <p>These advanced patterns make Zustand incredibly powerful while maintaining simplicity!</p>"},{"location":"react/state_management/#8-redux-toolkit","title":"8. Redux Toolkit","text":"<p>The official, opinionated toolset for Redux that simplifies setup and common patterns.</p> <p>Installation:</p> <pre><code>npm install @reduxjs/toolkit react-redux\n</code></pre> <p>Setup:</p> <pre><code>// store.js\nimport { configureStore } from \"@reduxjs/toolkit\";\nimport counterReducer from \"./counterSlice\";\nimport userReducer from \"./userSlice\";\n\nexport const store = configureStore({\n  reducer: {\n    counter: counterReducer,\n    user: userReducer,\n  },\n});\n\n// App.jsx\nimport { Provider } from \"react-redux\";\nimport { store } from \"./store\";\n\nfunction App() {\n  return (\n    &lt;Provider store={store}&gt;\n      &lt;YourApp /&gt;\n    &lt;/Provider&gt;\n  );\n}\n</code></pre> <p>Creating slices:</p> <pre><code>// counterSlice.js\nimport { createSlice } from \"@reduxjs/toolkit\";\n\nconst counterSlice = createSlice({\n  name: \"counter\",\n  initialState: {\n    value: 0,\n    history: [],\n  },\n  reducers: {\n    increment: (state) =&gt; {\n      // Redux Toolkit uses Immer, so we can \"mutate\" state\n      state.value += 1;\n      state.history.push(state.value);\n    },\n    decrement: (state) =&gt; {\n      state.value -= 1;\n      state.history.push(state.value);\n    },\n    incrementByAmount: (state, action) =&gt; {\n      state.value += action.payload;\n      state.history.push(state.value);\n    },\n    reset: (state) =&gt; {\n      state.value = 0;\n      state.history = [];\n    },\n  },\n});\n\nexport const { increment, decrement, incrementByAmount, reset } =\n  counterSlice.actions;\nexport default counterSlice.reducer;\n\n// Using in components\nimport { useSelector, useDispatch } from \"react-redux\";\nimport { increment, decrement, incrementByAmount } from \"./counterSlice\";\n\nfunction Counter() {\n  const count = useSelector((state) =&gt; state.counter.value);\n  const history = useSelector((state) =&gt; state.counter.history);\n  const dispatch = useDispatch();\n\n  return (\n    &lt;div&gt;\n      &lt;p&gt;Count: {count}&lt;/p&gt;\n      &lt;button onClick={() =&gt; dispatch(increment())}&gt;+&lt;/button&gt;\n      &lt;button onClick={() =&gt; dispatch(decrement())}&gt;-&lt;/button&gt;\n      &lt;button onClick={() =&gt; dispatch(incrementByAmount(5))}&gt;+5&lt;/button&gt;\n      &lt;p&gt;History: {history.join(\", \")}&lt;/p&gt;\n    &lt;/div&gt;\n  );\n}\n</code></pre> <p>Async thunks:</p> <pre><code>// userSlice.js\nimport { createSlice, createAsyncThunk } from \"@reduxjs/toolkit\";\n\n// Async action\nexport const fetchUser = createAsyncThunk(\"user/fetchUser\", async (userId) =&gt; {\n  const response = await fetch(`https://api.example.com/users/${userId}`);\n  return response.json();\n});\n\nconst userSlice = createSlice({\n  name: \"user\",\n  initialState: {\n    data: null,\n    status: \"idle\", // 'idle' | 'loading' | 'succeeded' | 'failed'\n    error: null,\n  },\n  reducers: {\n    clearUser: (state) =&gt; {\n      state.data = null;\n      state.status = \"idle\";\n    },\n  },\n  extraReducers: (builder) =&gt; {\n    builder\n      .addCase(fetchUser.pending, (state) =&gt; {\n        state.status = \"loading\";\n      })\n      .addCase(fetchUser.fulfilled, (state, action) =&gt; {\n        state.status = \"succeeded\";\n        state.data = action.payload;\n      })\n      .addCase(fetchUser.rejected, (state, action) =&gt; {\n        state.status = \"failed\";\n        state.error = action.error.message;\n      });\n  },\n});\n\nexport const { clearUser } = userSlice.actions;\nexport default userSlice.reducer;\n\n// Using in component\nfunction UserProfile({ userId }) {\n  const dispatch = useDispatch();\n  const { data, status, error } = useSelector((state) =&gt; state.user);\n\n  useEffect(() =&gt; {\n    dispatch(fetchUser(userId));\n  }, [userId, dispatch]);\n\n  if (status === \"loading\") return &lt;p&gt;Loading...&lt;/p&gt;;\n  if (status === \"failed\") return &lt;p&gt;Error: {error}&lt;/p&gt;;\n  if (!data) return null;\n\n  return (\n    &lt;div&gt;\n      &lt;h2&gt;{data.name}&lt;/h2&gt;\n      &lt;p&gt;{data.email}&lt;/p&gt;\n    &lt;/div&gt;\n  );\n}\n</code></pre>"},{"location":"react/state_management/#9-rtk-query","title":"9. RTK Query","text":"<p>Redux Toolkit's data fetching and caching solution, eliminating most data-fetching boilerplate.</p> <pre><code>// api.js\nimport { createApi, fetchBaseQuery } from \"@reduxjs/toolkit/query/react\";\n\nexport const api = createApi({\n  reducerPath: \"api\",\n  baseQuery: fetchBaseQuery({ baseUrl: \"https://api.example.com\" }),\n  tagTypes: [\"Post\", \"User\"],\n  endpoints: (builder) =&gt; ({\n    // GET request\n    getPosts: builder.query({\n      query: () =&gt; \"/posts\",\n      providesTags: [\"Post\"],\n    }),\n\n    // GET with params\n    getPost: builder.query({\n      query: (id) =&gt; `/posts/${id}`,\n      providesTags: (result, error, id) =&gt; [{ type: \"Post\", id }],\n    }),\n\n    // POST request\n    createPost: builder.mutation({\n      query: (newPost) =&gt; ({\n        url: \"/posts\",\n        method: \"POST\",\n        body: newPost,\n      }),\n      invalidatesTags: [\"Post\"],\n    }),\n\n    // PUT request\n    updatePost: builder.mutation({\n      query: ({ id, ...patch }) =&gt; ({\n        url: `/posts/${id}`,\n        method: \"PUT\",\n        body: patch,\n      }),\n      invalidatesTags: (result, error, { id }) =&gt; [{ type: \"Post\", id }],\n    }),\n\n    // DELETE request\n    deletePost: builder.mutation({\n      query: (id) =&gt; ({\n        url: `/posts/${id}`,\n        method: \"DELETE\",\n      }),\n      invalidatesTags: [\"Post\"],\n    }),\n  }),\n});\n\nexport const {\n  useGetPostsQuery,\n  useGetPostQuery,\n  useCreatePostMutation,\n  useUpdatePostMutation,\n  useDeletePostMutation,\n} = api;\n\n// store.js\nimport { configureStore } from \"@reduxjs/toolkit\";\nimport { api } from \"./api\";\n\nexport const store = configureStore({\n  reducer: {\n    [api.reducerPath]: api.reducer,\n  },\n  middleware: (getDefaultMiddleware) =&gt;\n    getDefaultMiddleware().concat(api.middleware),\n});\n\n// Using in components\nfunction PostsList() {\n  const { data: posts, error, isLoading, refetch } = useGetPostsQuery();\n\n  if (isLoading) return &lt;p&gt;Loading...&lt;/p&gt;;\n  if (error) return &lt;p&gt;Error: {error.message}&lt;/p&gt;;\n\n  return (\n    &lt;div&gt;\n      &lt;button onClick={refetch}&gt;Refresh&lt;/button&gt;\n      {posts.map((post) =&gt; (\n        &lt;PostItem key={post.id} post={post} /&gt;\n      ))}\n    &lt;/div&gt;\n  );\n}\n\nfunction PostItem({ post }) {\n  const [deletePost, { isLoading }] = useDeletePostMutation();\n\n  const handleDelete = async () =&gt; {\n    try {\n      await deletePost(post.id).unwrap();\n      alert(\"Post deleted!\");\n    } catch (err) {\n      alert(\"Failed to delete post\");\n    }\n  };\n\n  return (\n    &lt;div&gt;\n      &lt;h3&gt;{post.title}&lt;/h3&gt;\n      &lt;p&gt;{post.body}&lt;/p&gt;\n      &lt;button onClick={handleDelete} disabled={isLoading}&gt;\n        {isLoading ? \"Deleting...\" : \"Delete\"}\n      &lt;/button&gt;\n    &lt;/div&gt;\n  );\n}\n\nfunction CreatePost() {\n  const [title, setTitle] = useState(\"\");\n  const [body, setBody] = useState(\"\");\n  const [createPost, { isLoading }] = useCreatePostMutation();\n\n  const handleSubmit = async (e) =&gt; {\n    e.preventDefault();\n    try {\n      await createPost({ title, body }).unwrap();\n      setTitle(\"\");\n      setBody(\"\");\n      alert(\"Post created!\");\n    } catch (err) {\n      alert(\"Failed to create post\");\n    }\n  };\n\n  return (\n    &lt;form onSubmit={handleSubmit}&gt;\n      &lt;input\n        value={title}\n        onChange={(e) =&gt; setTitle(e.target.value)}\n        placeholder=\"Title\"\n      /&gt;\n      &lt;textarea\n        value={body}\n        onChange={(e) =&gt; setBody(e.target.value)}\n        placeholder=\"Body\"\n      /&gt;\n      &lt;button type=\"submit\" disabled={isLoading}&gt;\n        {isLoading ? \"Creating...\" : \"Create Post\"}\n      &lt;/button&gt;\n    &lt;/form&gt;\n  );\n}\n</code></pre> <p>Advanced features:</p> <pre><code>// Polling (auto-refetch)\nconst { data } = useGetPostsQuery(undefined, {\n  pollingInterval: 3000, // Refetch every 3 seconds\n});\n\n// Skip query conditionally\nconst { data } = useGetUserQuery(userId, {\n  skip: !userId, // Don't fetch if no userId\n});\n\n// Transforming responses\ngetPosts: builder.query({\n  query: () =&gt; \"/posts\",\n  transformResponse: (response) =&gt; {\n    return response.map((post) =&gt; ({\n      ...post,\n      formattedDate: new Date(post.createdAt).toLocaleDateString(),\n    }));\n  },\n});\n\n// Optimistic updates\nupdatePost: builder.mutation({\n  query: ({ id, ...patch }) =&gt; ({\n    url: `/posts/${id}`,\n    method: \"PUT\",\n    body: patch,\n  }),\n  async onQueryStarted({ id, ...patch }, { dispatch, queryFulfilled }) {\n    // Optimistically update cache\n    const patchResult = dispatch(\n      api.util.updateQueryData(\"getPost\", id, (draft) =&gt; {\n        Object.assign(draft, patch);\n      })\n    );\n    try {\n      await queryFulfilled;\n    } catch {\n      // Revert on error\n      patchResult.undo();\n    }\n  },\n});\n</code></pre>"},{"location":"react/state_management/#10-decision-making","title":"10. Decision Making","text":"<p>Choose the right state management solution for your needs:</p>"},{"location":"react/state_management/#usestate","title":"useState","text":"<p>Use when:</p> <ul> <li>State is local to a single component</li> <li>Simple form inputs</li> <li>Toggle states, modals</li> <li>Small component-level data</li> </ul> <p>Example: Button click counter, form validation, modal open/close state</p>"},{"location":"react/state_management/#lifting-state-up","title":"Lifting State Up","text":"<p>Use when:</p> <ul> <li>2-3 sibling components need shared state</li> <li>Parent-child communication is simple</li> <li>State logic is straightforward</li> </ul> <p>Example: Form with multiple inputs, filtering a list with controls in separate components</p>"},{"location":"react/state_management/#context-api","title":"Context API","text":"<p>Use when:</p> <ul> <li>State needs to be accessed by many components at different nesting levels</li> <li>You want to avoid prop drilling</li> <li>State changes infrequently (theme, auth, locale)</li> <li>Small to medium apps</li> </ul> <p>Pros: Built into React, simple API, no extra dependencies Cons: Can cause unnecessary re-renders, not optimized for frequent updates</p> <p>Example: User authentication, theme settings, language preferences</p>"},{"location":"react/state_management/#zustand","title":"Zustand","text":"<p>Use when:</p> <ul> <li>Medium-sized apps with moderate state complexity</li> <li>You want simplicity without Redux boilerplate</li> <li>Frequent state updates across multiple components</li> <li>You need middleware (persist, devtools)</li> </ul> <p>Pros: Minimal boilerplate, great DX, fast, built-in middleware Cons: Smaller ecosystem than Redux</p> <p>Example: Shopping cart, global UI state, user preferences, notifications</p>"},{"location":"react/state_management/#redux-toolkit","title":"Redux Toolkit","text":"<p>Use when:</p> <ul> <li>Large, complex applications</li> <li>Need time-travel debugging</li> <li>Complex state logic with many actions</li> <li>Team is familiar with Redux patterns</li> <li>Need robust middleware ecosystem</li> </ul> <p>Pros: Predictable state updates, excellent devtools, large ecosystem Cons: More boilerplate than Zustand, steeper learning curve</p> <p>Example: Enterprise apps, apps with complex business logic, apps needing undo/redo</p>"},{"location":"react/state_management/#rtk-query","title":"RTK Query","text":"<p>Use when:</p> <ul> <li>Your app heavily relies on server data</li> <li>You want automatic caching and synchronization</li> <li>Need optimistic updates</li> <li>Want to eliminate data-fetching boilerplate</li> </ul> <p>Pros: Automatic caching, refetching, garbage collection Cons: Tightly coupled with Redux Toolkit</p> <p>Example: CRUD applications, dashboards fetching API data, real-time data apps</p>"},{"location":"react/state_management/#quick-decision-tree","title":"Quick Decision Tree","text":"<pre><code>Does only one component need this state?\n\u251c\u2500 YES \u2192 useState\n\u2514\u2500 NO \u2193\n\nDo only 2-3 nearby components need it?\n\u251c\u2500 YES \u2192 Lift state up\n\u2514\u2500 NO \u2193\n\nIs it mostly server data (CRUD operations)?\n\u251c\u2500 YES \u2192 RTK Query\n\u2514\u2500 NO \u2193\n\nIs it infrequently changing global state?\n\u251c\u2500 YES \u2192 Context API\n\u2514\u2500 NO \u2193\n\nIs your app large and complex?\n\u251c\u2500 YES \u2192 Redux Toolkit\n\u2514\u2500 NO \u2192 Zustand\n</code></pre>"},{"location":"react/state_management/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Start simple: Always start with useState and lift state up. Only add complexity when needed.</p> </li> <li> <p>Collocate state: Keep state as close as possible to where it's used.</p> </li> <li> <p>Avoid premature optimization: Don't reach for global state management until prop drilling becomes painful.</p> </li> <li> <p>Separate server state from client state: Consider using RTK Query or React Query for server data, and Zustand/Context for UI state.</p> </li> <li> <p>Use TypeScript: All modern state management libraries have excellent TypeScript support.</p> </li> <li> <p>Keep actions simple: Each action should do one thing well.</p> </li> <li> <p>Normalize complex state: Store data in a flat structure with IDs rather than nested objects.</p> </li> <li> <p>Use selectors: Create reusable selectors to compute derived state.</p> </li> </ol>"},{"location":"react/state_management/#conclusion","title":"Conclusion","text":"<p>State management in React is a journey. Start with useState, embrace lifting state up, use Context for simple global state, and graduate to Zustand or Redux Toolkit as your application grows. Remember: the best state management solution is the simplest one that solves your problem.</p>"}]}